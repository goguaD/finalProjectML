{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Model_Experiment_LightGBM",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "source": [
        "import os\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "import mlflow\n",
        "import mlflow.lightgbm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "data_base_path = None\n",
        "\n",
        "# Option 1: Load data directly from Kaggle using kagglehub\n",
        "# try:\n",
        "#     import kagglehub\n",
        "#     data_base_path = kagglehub.competition_download('walmart-recruiting-store-sales-forecasting')\n",
        "#     print(f\"Data will be loaded from KaggleHub path: {data_base_path}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Warning: KaggleHub download failed. Error: {e}\")\n",
        "#     print(\"Falling back to checking other data loading options.\")\n",
        "\n",
        "# Option 2: Load data from Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# data_base_path = '/content/drive/MyDrive/path/to/your/walmart_sales_data/'\n",
        "# print(f\"Data will be loaded from Google Drive path: {data_base_path}\")\n",
        "\n",
        "# Option 3: Load data from Colab session storage\n",
        "data_base_path = '/content/'\n",
        "print(f\"Data will be loaded from Colab /content/ path: {data_base_path}\")\n",
        "\n",
        "if data_base_path is None or not os.path.exists(data_base_path):\n",
        "    print(\"\\nNo valid data path set or path does not exist from chosen options.\")\n",
        "    print(\"Please uncomment and configure one of the 'Option' blocks above to specify your data location.\")\n",
        "    if os.path.exists('/kaggle/input/walmart-recruiting-store-sales-forecasting/'):\n",
        "        data_base_path = '/kaggle/input/walmart-recruiting-store-sales-forecasting/'\n",
        "        print(f\"Attempting to load from default Kaggle input path: {data_base_path}\")\n",
        "    else:\n",
        "        print(\"Critical: No accessible data path found. Please ensure your data files are in one of the specified locations.\")\n",
        "        raise FileNotFoundError(\"Data files not found. Please check your data loading configuration.\")\n",
        "\n",
        "\n",
        "print(\"\\nLoading dataframes...\")\n",
        "try:\n",
        "    df_train = pd.read_csv(f'{data_base_path}train.csv', parse_dates=['Date'])\n",
        "    df_test = pd.read_csv(f'{data_base_path}test.csv', parse_dates=['Date'])\n",
        "    df_features = pd.read_csv(f'{data_base_path}features.csv', parse_dates=['Date'])\n",
        "    df_stores = pd.read_csv(f'{data_base_path}stores.csv')\n",
        "    print(\"All dataframes loaded successfully.\")\n",
        "    print(f\"\\n--- df_features head (check 'Store', 'Date', 'IsHoliday') ---\")\n",
        "    print(df_features[['Store', 'Date', 'IsHoliday']].head())\n",
        "    print(f\"Columns in df_features after loading: {df_features.columns.tolist()}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Critical Error: One or more data files not found in '{data_base_path}'. Error: {e}\")\n",
        "    print(\"Please ensure the CSV files (train.csv, test.csv, features.csv, stores.csv) are directly in the specified 'data_base_path'.\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"Critical Error: An unexpected error occurred during data loading. Error: {e}\")\n",
        "    raise\n",
        "\n",
        "\n",
        "print(\"\\nMerging dataframes...\")\n",
        "df = df_train.merge(df_stores, on='Store', how='left')\n",
        "\n",
        "print(\"\\n--- Pre-merge Debugging ---\")\n",
        "print(\"df (train + stores) info:\")\n",
        "df.info()\n",
        "print(\"\\ndf_features info:\")\n",
        "df_features.info()\n",
        "\n",
        "print(\"\\ndf (train + stores) head for merge keys:\")\n",
        "print(df[['Store', 'Date']].head())\n",
        "print(\"\\ndf_features head for merge keys:\")\n",
        "print(df_features[['Store', 'Date']].head())\n",
        "\n",
        "common_store_dates_df = df[['Store', 'Date']].drop_duplicates()\n",
        "common_store_dates_features = df_features[['Store', 'Date']].drop_duplicates()\n",
        "overlap_count = pd.merge(common_store_dates_df, common_store_dates_features, on=['Store', 'Date'], how='inner').shape[0]\n",
        "print(f\"\\nNumber of unique Store-Date pairs in df (train + stores): {common_store_dates_df.shape[0]}\")\n",
        "print(f\"Number of unique Store-Date pairs in df_features: {common_store_dates_features.shape[0]}\")\n",
        "print(f\"Number of overlapping Store-Date pairs (inner join): {overlap_count}\")\n",
        "if overlap_count == 0:\n",
        "    print(\"WARNING: No overlapping Store-Date pairs found between df and df_features. This will result in many NaNs after merge.\")\n",
        "    print(\"Please check the 'Store' and 'Date' columns for consistency (e.g., data types, ranges).\")\n",
        "\n",
        "\n",
        "common_cols_to_drop_from_df = [\n",
        "    'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "    'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5',\n",
        "    'IsHoliday'\n",
        "]\n",
        "cols_to_drop_existing = [col for col in common_cols_to_drop_from_df if col in df.columns]\n",
        "\n",
        "if cols_to_drop_existing:\n",
        "    print(f\"Dropping common columns from df (from train.csv/stores.csv) before merging with features.csv: {cols_to_drop_existing}\")\n",
        "    df = df.drop(columns=cols_to_drop_existing)\n",
        "else:\n",
        "    print(\"No common columns to drop from df before merging with features.csv (or they don't exist).\")\n",
        "\n",
        "\n",
        "df = df.merge(df_features, on=['Store', 'Date'], how='left')\n",
        "print(\"Dataframes merged.\")\n",
        "print(f\"\\n--- Merged DataFrame head (check 'Store', 'Date', 'IsHoliday') ---\")\n",
        "if 'IsHoliday' in df.columns:\n",
        "    print(df[['Store', 'Date', 'IsHoliday']].head())\n",
        "else:\n",
        "    print(\"Error: 'IsHoliday' column still not found after merge. This indicates a deeper issue with merge keys or data in features.csv.\")\n",
        "\n",
        "print(f\"Columns in merged DataFrame after final merge: {df.columns.tolist()}\")\n",
        "\n",
        "\n",
        "print(\"\\nHandling missing values...\")\n",
        "for col in ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].fillna(0)\n",
        "    else:\n",
        "        print(f\"Warning: Column '{col}' not found in DataFrame for NaN handling.\")\n",
        "print(\"Missing values filled.\")\n",
        "\n",
        "\n",
        "if 'IsHoliday' in df.columns:\n",
        "    df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "    print(\"'IsHoliday' column converted to integer type.\")\n",
        "else:\n",
        "    print(\"Critical Warning: 'IsHoliday' column still not found in DataFrame. Adding it with default value 0.\")\n",
        "    df['IsHoliday'] = 0\n",
        "\n",
        "\n",
        "df['Weekly_Sales'] = df['Weekly_Sales'].apply(lambda x: max(0, x))\n",
        "print(\"Negative 'Weekly_Sales' values handled.\")\n",
        "\n",
        "\n",
        "def create_features(df_input):\n",
        "    df_input['Year'] = df_input['Date'].dt.year\n",
        "    df_input['Month'] = df_input['Date'].dt.month\n",
        "    df_input['Week'] = df_input['Date'].dt.isocalendar().week.astype(int)\n",
        "    df_input['Day'] = df_input['Date'].dt.day\n",
        "    df_input['DayOfWeek'] = df_input['Date'].dt.dayofweek\n",
        "    df_input['DayOfYear'] = df_input['Date'].dt.dayofyear\n",
        "    return df_input\n",
        "\n",
        "print(\"\\nCreating time-based features...\")\n",
        "df = create_features(df.copy())\n",
        "print(\"Time-based features created.\")\n",
        "\n",
        "\n",
        "features = [\n",
        "    'Store', 'Dept', 'Year', 'Month', 'Week', 'Day', 'DayOfWeek', 'DayOfYear',\n",
        "    'IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "    'Size', 'Type',\n",
        "    'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'\n",
        "]\n",
        "target = 'Weekly_Sales'\n",
        "\n",
        "\n",
        "print(\"\\nApplying One-Hot Encoding to 'Type' column...\")\n",
        "df = pd.get_dummies(df, columns=['Type'], prefix='Type', drop_first=True)\n",
        "features.remove('Type')\n",
        "features.extend([col for col in df.columns if 'Type_' in col])\n",
        "print(\"'Type' column encoded.\")\n",
        "\n",
        "\n",
        "final_features = [f for f in features if f in df.columns]\n",
        "print(f\"\\nFinal features used for training: {final_features}\")\n",
        "\n",
        "\n",
        "df = df.sort_values('Date')\n",
        "split_date = pd.to_datetime('2011-12-31')\n",
        "\n",
        "X_train = df[df['Date'] <= split_date][final_features]\n",
        "y_train = df[df['Date'] <= split_date][target]\n",
        "X_valid = df[df['Date'] > split_date][final_features]\n",
        "y_valid = df[df['Date'] > split_date][target]\n",
        "\n",
        "print(f\"\\nTrain set size: {len(X_train)} (Dates: {df['Date'].min().strftime('%Y-%m-%d')} to {df[df['Date'] <= split_date]['Date'].max().strftime('%Y-%m-%d')})\")\n",
        "print(f\"Validation set size: {len(X_valid)} (Dates: {df[df['Date'] > split_date]['Date'].min().strftime('%Y-%m-%d')} to {df['Date'].max().strftime('%Y-%m-%d')})\")\n",
        "\n",
        "\n",
        "missing_in_valid = set(X_train.columns) - set(X_valid.columns)\n",
        "missing_in_train = set(X_valid.columns) - set(X_train.columns)\n",
        "if missing_in_valid or missing_in_train:\n",
        "    print(\"Warning: Mismatch in columns between train and validation sets!\")\n",
        "    if missing_in_valid:\n",
        "        print(f\"Missing in valid: {missing_in_valid}\")\n",
        "    if missing_in_train:\n",
        "        print(f\"Missing in train: {missing_in_train}\")\n",
        "    common_cols = list(set(X_train.columns) & set(X_valid.columns))\n",
        "    X_train = X_train[common_cols]\n",
        "    X_valid = X_valid[common_cols]\n",
        "    print(\"Columns aligned for train and validation sets.\")\n",
        "\n",
        "\n",
        "print(f\"\\nTarget variable ('Weekly_Sales') statistics (full dataset):\")\n",
        "print(f\"Mean: {df['Weekly_Sales'].mean():.2f}\")\n",
        "print(f\"Standard Deviation: {df['Weekly_Sales'].std():.2f}\")\n",
        "print(f\"Min: {df['Weekly_Sales'].min():.2f}\")\n",
        "print(f\"Max: {df['Weekly_Sales'].max():.2f}\")\n"
      ],
      "metadata": {
        "id": "1"
      },
      "execution_count": null,
      "cell_type": "code",
      "outputs": []
    },
    {
      "source": [
        "mlflow.set_tracking_uri(\"file:/content/mlruns\")\n",
        "\n",
        "experiment_name = \"LightGBM_Training\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "print(f\"MLflow Experiment '{experiment_name}' set up.\")\n",
        "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "\n",
        "with mlflow.start_run(run_name='LGBM_baseline_run'):\n",
        "    params = {\n",
        "        'objective': 'regression_l1',\n",
        "        'metric': 'rmse',\n",
        "        'n_estimators': 1500,\n",
        "        'learning_rate': 0.03,\n",
        "        'num_leaves': 31,\n",
        "        'max_depth': 8,\n",
        "        'min_child_samples': 20,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'reg_alpha': 0.1,\n",
        "        'reg_lambda': 0.1\n",
        "    }\n",
        "\n",
        "    mlflow.log_params(params)\n",
        "    print(\"\\nLightGBM model training in progress...\")\n",
        "\n",
        "    model = lgb.LGBMRegressor(**params)\n",
        "\n",
        "    callbacks = [\n",
        "        lgb.early_stopping(100, verbose=False),\n",
        "        lgb.log_evaluation(period=200)\n",
        "    ]\n",
        "\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_valid, y_valid)],\n",
        "              eval_metric='rmse',\n",
        "              callbacks=callbacks)\n",
        "\n",
        "    print(\"LightGBM model training finished.\")\n",
        "\n",
        "    preds_val = model.predict(X_valid)\n",
        "\n",
        "    val_rmse = np.sqrt(mean_squared_error(y_valid, preds_val))\n",
        "    val_mae = mean_absolute_error(y_valid, preds_val)\n",
        "\n",
        "    print(f\"\\nValidation RMSE: {val_rmse:.2f}\")\n",
        "    print(f\"Validation MAE: {val_mae:.2f}\")\n",
        "\n",
        "    mlflow.log_metric(\"val_rmse\", val_rmse)\n",
        "    mlflow.log_metric(\"val_mae\", val_mae)\n",
        "    mlflow.log_metric(\"best_iteration\", model.best_iteration_)\n",
        "\n",
        "    mlflow.lightgbm.log_model(model, artifact_path='model', registered_model_name=\"LightGBMSalesForecaster\")\n",
        "    print(\"\\nModel logged to MLflow.\")\n",
        "    print(f\"MLflow Run ID: {mlflow.active_run().info.run_id}\")\n",
        "\n",
        "    print(\"\\nPreparing test data for evaluation...\")\n",
        "\n",
        "    df_test_merged = df_test.merge(df_stores, on='Store', how='left')\n",
        "\n",
        "    common_cols_to_drop_from_test = [\n",
        "        'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "        'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5',\n",
        "        'IsHoliday'\n",
        "    ]\n",
        "    cols_to_drop_existing_test = [col for col in common_cols_to_drop_from_test if col in df_test_merged.columns]\n",
        "\n",
        "    if cols_to_drop_existing_test:\n",
        "        print(f\"Dropping common columns from df_test (from test.csv/stores.csv) before merging with features.csv: {cols_to_drop_existing_test}\")\n",
        "        df_test_merged = df_test_merged.drop(columns=cols_to_drop_existing_test)\n",
        "    else:\n",
        "        print(\"No common columns to drop from df_test_merged before merging with features.csv (or they don't exist).\")\n",
        "\n",
        "    df_test_final = df_test_merged.merge(df_features, on=['Store', 'Date'], how='left')\n",
        "\n",
        "    print(\"Handling missing values in test data...\")\n",
        "    for col in ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']:\n",
        "        if col in df_test_final.columns:\n",
        "            df_test_final[col] = df_test_final[col].fillna(0)\n",
        "        else:\n",
        "            print(f\"Warning: Column '{col}' not found in df_test_final for NaN handling.\")\n",
        "    print(\"Missing values in test data filled.\")\n",
        "\n",
        "    if 'IsHoliday' in df_test_final.columns:\n",
        "        df_test_final['IsHoliday'] = df_test_final['IsHoliday'].astype(int)\n",
        "    else:\n",
        "        print(\"Warning: 'IsHoliday' column not found in df_test_final. Adding it with default value 0.\")\n",
        "        df_test_final['IsHoliday'] = 0\n",
        "\n",
        "    print(\"Creating time-based features for test data...\")\n",
        "    df_test_final = create_features(df_test_final.copy())\n",
        "    print(\"Time-based features for test data created.\")\n",
        "\n",
        "    print(\"Applying One-Hot Encoding to 'Type' column in test data...\")\n",
        "    if 'Type' in df_test_final.columns:\n",
        "        df_test_final = pd.get_dummies(df_test_final, columns=['Type'], prefix='Type', drop_first=True)\n",
        "    else:\n",
        "        print(\"Warning: 'Type' column not found in df_test_final. Skipping One-Hot Encoding for 'Type'.\")\n",
        "    print(\"Test data preprocessing complete.\")\n",
        "\n",
        "    X_test = df_test_final.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "    if X_test.isnull().sum().sum() > 0:\n",
        "        print(\"Warning: NaNs found in X_test after alignment. Filling with 0.\")\n",
        "        X_test = X_test.fillna(0)\n",
        "\n",
        "    print(f\"Final Test set size for prediction: {len(X_test)}\")\n",
        "    print(f\"Test set features (first 5 rows):\\n{X_test.head()}\")\n",
        "\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    preds_test = model.predict(X_test)\n",
        "\n",
        "    submission_df = pd.DataFrame({'Id': df_test['Store'].astype(str) + '_' + df_test['Dept'].astype(str) + '_' + df_test['Date'].dt.strftime('%Y-%m-%d'),\n",
        "                                  'Weekly_Sales': preds_test})\n",
        "\n",
        "    submission_df['Weekly_Sales'] = submission_df['Weekly_Sales'].apply(lambda x: max(0, x))\n",
        "\n",
        "    submission_file_path = 'submission_lightgbm.csv'\n",
        "    submission_df.to_csv(submission_file_path, index=False)\n",
        "    print(f\"\\nSubmission file '{submission_file_path}' created successfully.\")\n",
        "\n",
        "    mlflow.log_artifact(submission_file_path)\n",
        "    print(f\"Submission file logged as MLflow artifact.\")\n",
        "\n",
        "print(\"\\nLightGBM experiment finished. Check MLflow UI for full results and submission file.\")\n"
      ],
      "metadata": {
        "id": "2"
      },
      "execution_count": null,
      "cell_type": "code",
      "outputs": []
    },
    {
      "source": [
        "import time\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"Terminated any existing ngrok tunnels.\")\n",
        "    time.sleep(2)\n",
        "except Exception as e:\n",
        "    print(f\"Could not terminate existing ngrok tunnels (might not exist): {e}\")\n",
        "\n",
        "try:\n",
        "    NGROK_AUTH_TOKEN = \"2zXieAERZUJhQWKxXhWwvjXc1fh_2CSAEsiYxxG6iSoupmRN9\"\n",
        "    print(f\"NGROK_AUTH_TOKEN value being used: '{NGROK_AUTH_TOKEN}'\")\n",
        "\n",
        "    if NGROK_AUTH_TOKEN == \"YOUR_NGROK_AUTH_TOKEN\":\n",
        "        print(\"CRITICAL: 'YOUR_NGROK_AUTH_TOKEN' placeholder still present. Please replace it with your actual token.\")\n",
        "        raise ValueError(\"ngrok authentication token not set correctly.\")\n",
        "    \n",
        "    conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "    print(\"ngrok authentication token set.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error setting ngrok auth token: {e}\")\n",
        "    print(\"Please ensure your ngrok token is correct and pasted without extra spaces or characters.\")\n",
        "    print(\"Also, check your ngrok dashboard (https://dashboard.ngrok.com/agents) to ensure no active sessions are running.\")\n",
        "\n",
        "\n",
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\")\n",
        "\n",
        "print(\"Creating ngrok tunnel for MLflow UI...\")\n",
        "try:\n",
        "    public_url = ngrok.connect(addr=\"5000\", proto=\"http\")\n",
        "    print(f\"MLflow UI is available at: {public_url}\")\n",
        "    print(\"Click the link above to access the MLflow UI in your browser.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating ngrok tunnel: {e}\")\n",
        "    print(\"Please ensure ngrok is installed and your auth token is correct.\")\n",
        "    print(\"If the error persists, check your ngrok dashboard (https://dashboard.ngrok.com/agents) and manually kill any active sessions.\")\n"
      ],
      "metadata": {
        "id": "3"
      },
      "execution_count": null,
      "cell_type": "code",
      "outputs": []
    }
  ]
}
