{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Model_Experiment_LightGBM",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "source": [
        "# --- Cell 1: Imports and Data Preparation ---\n",
        "\n",
        "# Standard library imports\n",
        "import os\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "# Data handling and numerical operations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Machine learning libraries\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import TimeSeriesSplit # Useful for more advanced CV, but simple split used here\n",
        "\n",
        "# MLflow for experiment tracking\n",
        "import mlflow\n",
        "import mlflow.lightgbm # Specific MLflow integration for LightGBM\n",
        "\n",
        "# Visualization (optional, but good for understanding data)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Data Loading Configuration ---\n",
        "# You MUST choose ONE of the following options for loading your data.\n",
        "# Uncomment and configure the option that matches how your data is available in Colab.\n",
        "\n",
        "data_base_path = None # Initialize the base path for your data files\n",
        "\n",
        "# Option 1: Load data directly from Kaggle using kagglehub (recommended if you have Kaggle API key set up)\n",
        "# Requires: !pip install kagglehub and !kagglehub login (run in a separate cell before this one)\n",
        "# try:\n",
        "#     import kagglehub\n",
        "#     # Ensure you have run `!kagglehub login` and entered your API key in a previous cell\n",
        "#     data_base_path = kagglehub.competition_download('walmart-recruiting-store-sales-forecasting')\n",
        "#     print(f\"Data will be loaded from KaggleHub path: {data_base_path}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Warning: KaggleHub download failed. Error: {e}\")\n",
        "#     print(\"Falling back to checking other data loading options.\")\n",
        "\n",
        "# Option 2: Load data from Google Drive (recommended for persistent storage)\n",
        "# Requires: Mounting Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# data_base_path = '/content/drive/MyDrive/path/to/your/walmart_sales_data/' # <--- IMPORTANT: ADJUST THIS PATH!\n",
        "# print(f\"Data will be loaded from Google Drive path: {data_base_path}\")\n",
        "\n",
        "# Option 3: Load data from Colab session storage (if you manually uploaded CSVs to /content/)\n",
        "# Files uploaded this way are deleted when the Colab runtime resets.\n",
        "# If you manually uploaded train.csv, test.csv, features.csv, stores.csv to /content/\n",
        "data_base_path = '/content/' # <--- THIS IS THE MOST LIKELY CORRECT PATH FOR YOUR SCENARIO\n",
        "print(f\"Data will be loaded from Colab /content/ path: {data_base_path}\")\n",
        "\n",
        "# --- Fallback if no specific option is chosen or successful ---\n",
        "if data_base_path is None or not os.path.exists(data_base_path):\n",
        "    print(\"\\nNo valid data path set or path does not exist from chosen options.\")\n",
        "    print(\"Please uncomment and configure one of the 'Option' blocks above to specify your data location.\")\n",
        "    # As a last resort, try default Kaggle input path if running directly on Kaggle,\n",
        "    # or if data is expected to be in a standard input directory.\n",
        "    if os.path.exists('/kaggle/input/walmart-recruiting-store-sales-forecasting/'):\n",
        "        data_base_path = '/kaggle/input/walmart-recruiting-store-sales-forecasting/'\n",
        "        print(f\"Attempting to load from default Kaggle input path: {data_base_path}\")\n",
        "    else:\n",
        "        print(\"Critical: No accessible data path found. Please ensure your data files are in one of the specified locations.\")\n",
        "        raise FileNotFoundError(\"Data files not found. Please check your data loading configuration.\")\n",
        "\n",
        "\n",
        "# --- Load DataFrames ---\n",
        "print(\"\\nLoading dataframes...\")\n",
        "try:\n",
        "    df_train = pd.read_csv(f'{data_base_path}train.csv', parse_dates=['Date'])\n",
        "    df_test = pd.read_csv(f'{data_base_path}test.csv', parse_dates=['Date'])\n",
        "    df_features = pd.read_csv(f'{data_base_path}features.csv', parse_dates=['Date'])\n",
        "    df_stores = pd.read_csv(f'{data_base_path}stores.csv')\n",
        "    print(\"All dataframes loaded successfully.\")\n",
        "    print(f\"\\n--- df_features head (check 'Store', 'Date', 'IsHoliday') ---\")\n",
        "    print(df_features[['Store', 'Date', 'IsHoliday']].head()) # Debug print\n",
        "    print(f\"Columns in df_features after loading: {df_features.columns.tolist()}\") # Debug print\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Critical Error: One or more data files not found in '{data_base_path}'. Error: {e}\")\n",
        "    print(\"Please ensure the CSV files (train.csv, test.csv, features.csv, stores.csv) are directly in the specified 'data_base_path'.\")\n",
        "    raise # Re-raise the error as data loading is critical\n",
        "except Exception as e:\n",
        "    print(f\"Critical Error: An unexpected error occurred during data loading. Error: {e}\")\n",
        "    raise # Re-raise any other unexpected errors\n",
        "\n",
        "\n",
        "# --- Merge DataFrames ---\n",
        "print(\"\\nMerging dataframes...\")\n",
        "# First merge train with stores\n",
        "df = df_train.merge(df_stores, on='Store', how='left')\n",
        "\n",
        "# --- Debugging Merge ---\n",
        "print(\"\\n--- Pre-merge Debugging ---\")\n",
        "print(\"df (train + stores) info:\")\n",
        "df.info()\n",
        "print(\"\\ndf_features info:\")\n",
        "df_features.info()\n",
        "\n",
        "print(\"\\ndf (train + stores) head for merge keys:\")\n",
        "print(df[['Store', 'Date']].head())\n",
        "print(\"\\ndf_features head for merge keys:\")\n",
        "print(df_features[['Store', 'Date']].head())\n",
        "\n",
        "# Check for common Store-Date pairs\n",
        "common_store_dates_df = df[['Store', 'Date']].drop_duplicates()\n",
        "common_store_dates_features = df_features[['Store', 'Date']].drop_duplicates()\n",
        "overlap_count = pd.merge(common_store_dates_df, common_store_dates_features, on=['Store', 'Date'], how='inner').shape[0]\n",
        "print(f\"\\nNumber of unique Store-Date pairs in df (train + stores): {common_store_dates_df.shape[0]}\")\n",
        "print(f\"Number of unique Store-Date pairs in df_features: {common_store_dates_features.shape[0]}\")\n",
        "print(f\"Number of overlapping Store-Date pairs (inner join): {overlap_count}\")\n",
        "if overlap_count == 0:\n",
        "    print(\"WARNING: No overlapping Store-Date pairs found between df and df_features. This will result in many NaNs after merge.\")\n",
        "    print(\"Please check the 'Store' and 'Date' columns for consistency (e.g., data types, ranges).\")\n",
        "\n",
        "\n",
        "# --- FIX: Identify and drop common columns from df before merging with df_features ---\n",
        "# This prevents _x and _y suffixes for features that exist in both train.csv/stores.csv and features.csv.\n",
        "# We want to keep the features.csv version of these columns.\n",
        "common_cols_to_drop_from_df = [\n",
        "    'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "    'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5',\n",
        "    'IsHoliday' # IsHoliday is also common, and we want features.csv's version\n",
        "]\n",
        "# Filter to only drop columns that actually exist in df\n",
        "cols_to_drop_existing = [col for col in common_cols_to_drop_from_df if col in df.columns]\n",
        "\n",
        "if cols_to_drop_existing:\n",
        "    print(f\"Dropping common columns from df (from train.csv/stores.csv) before merging with features.csv: {cols_to_drop_existing}\")\n",
        "    df = df.drop(columns=cols_to_drop_existing)\n",
        "else:\n",
        "    print(\"No common columns to drop from df before merging with features.csv (or they don't exist).\")\n",
        "\n",
        "\n",
        "# Then merge the result with features\n",
        "df = df.merge(df_features, on=['Store', 'Date'], how='left')\n",
        "print(\"Dataframes merged.\")\n",
        "print(f\"\\n--- Merged DataFrame head (check 'Store', 'Date', 'IsHoliday') ---\")\n",
        "# 'IsHoliday' and other common columns should now exist directly from df_features without suffixes\n",
        "if 'IsHoliday' in df.columns:\n",
        "    print(df[['Store', 'Date', 'IsHoliday']].head()) # Debug print\n",
        "else:\n",
        "    print(\"Error: 'IsHoliday' column still not found after merge. This indicates a deeper issue with merge keys or data in features.csv.\")\n",
        "\n",
        "print(f\"Columns in merged DataFrame after final merge: {df.columns.tolist()}\") # Debug print to check for all columns\n",
        "\n",
        "\n",
        "# --- Handle Missing Values ---\n",
        "print(\"\\nHandling missing values...\")\n",
        "# Fill NaNs in numerical columns with 0. Consider more sophisticated imputation strategies for production.\n",
        "# This loop now covers all relevant numerical columns that might have NaNs after the merge.\n",
        "for col in ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']:\n",
        "    if col in df.columns: # Check if column exists before filling NaNs\n",
        "        df[col] = df[col].fillna(0)\n",
        "    else:\n",
        "        print(f\"Warning: Column '{col}' not found in DataFrame for NaN handling.\")\n",
        "print(\"Missing values filled.\")\n",
        "\n",
        "\n",
        "# --- Convert IsHoliday to numerical ---\n",
        "# This step should now consistently find 'IsHoliday' from df_features\n",
        "if 'IsHoliday' in df.columns:\n",
        "    df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "    print(\"'IsHoliday' column converted to integer type.\")\n",
        "else:\n",
        "    # This else block should ideally not be hit if the merge and previous fix are successful\n",
        "    print(\"Critical Warning: 'IsHoliday' column still not found in DataFrame. Adding it with default value 0.\")\n",
        "    df['IsHoliday'] = 0 # Fallback to ensure column exists\n",
        "\n",
        "\n",
        "# --- Handle Negative Weekly_Sales values ---\n",
        "# Set negative sales to 0, as sales cannot be negative in reality.\n",
        "df['Weekly_Sales'] = df['Weekly_Sales'].apply(lambda x: max(0, x))\n",
        "print(\"Negative 'Weekly_Sales' values handled.\")\n",
        "\n",
        "\n",
        "# --- Feature Engineering ---\n",
        "# Function to create time-based features\n",
        "def create_features(df_input):\n",
        "    df_input['Year'] = df_input['Date'].dt.year\n",
        "    df_input['Month'] = df_input['Date'].dt.month\n",
        "    df_input['Week'] = df_input['Date'].dt.isocalendar().week.astype(int)\n",
        "    df_input['Day'] = df_input['Date'].dt.day\n",
        "    df_input['DayOfWeek'] = df_input['Date'].dt.dayofweek\n",
        "    df_input['DayOfYear'] = df_input['Date'].dt.dayofyear\n",
        "    return df_input\n",
        "\n",
        "print(\"\\nCreating time-based features...\")\n",
        "df = create_features(df.copy()) # Apply features to the main merged dataframe\n",
        "print(\"Time-based features created.\")\n",
        "\n",
        "\n",
        "# --- Define Features and Target ---\n",
        "# 'Type' will be one-hot encoded separately\n",
        "features = [\n",
        "    'Store', 'Dept', 'Year', 'Month', 'Week', 'Day', 'DayOfWeek', 'DayOfYear',\n",
        "    'IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "    'Size', 'Type', # 'Type' will be removed and replaced by one-hot encoded columns\n",
        "    'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'\n",
        "]\n",
        "target = 'Weekly_Sales'\n",
        "\n",
        "\n",
        "# --- Convert 'Type' to numerical using OneHotEncoder ---\n",
        "print(\"\\nApplying One-Hot Encoding to 'Type' column...\")\n",
        "df = pd.get_dummies(df, columns=['Type'], prefix='Type', drop_first=True)\n",
        "# Remove original 'Type' from features list and add new one-hot encoded columns\n",
        "features.remove('Type')\n",
        "features.extend([col for col in df.columns if 'Type_' in col])\n",
        "print(\"'Type' column encoded.\")\n",
        "\n",
        "\n",
        "# --- Final Feature Filtering ---\n",
        "# Ensure only existing features are used after all preprocessing steps\n",
        "final_features = [f for f in features if f in df.columns]\n",
        "print(f\"\\nFinal features used for training: {final_features}\")\n",
        "\n",
        "\n",
        "# --- Split Data into Training and Validation Sets (Time-Series Split) ---\n",
        "# Sort data by date for proper time-series splitting\n",
        "df = df.sort_values('Date')\n",
        "# Define a split date to separate training and validation data chronologically\n",
        "split_date = pd.to_datetime('2011-12-31') # Example split date, adjust as needed for your validation strategy\n",
        "\n",
        "X_train = df[df['Date'] <= split_date][final_features]\n",
        "y_train = df[df['Date'] <= split_date][target]\n",
        "X_valid = df[df['Date'] > split_date][final_features]\n",
        "y_valid = df[df['Date'] > split_date][target]\n",
        "\n",
        "print(f\"\\nTrain set size: {len(X_train)} (Dates: {df['Date'].min().strftime('%Y-%m-%d')} to {df[df['Date'] <= split_date]['Date'].max().strftime('%Y-%m-%d')})\")\n",
        "print(f\"Validation set size: {len(X_valid)} (Dates: {df[df['Date'] > split_date]['Date'].min().strftime('%Y-%m-%d')} to {df['Date'].max().strftime('%Y-%m-%d')})\")\n",
        "\n",
        "\n",
        "# --- Check for missing columns in X_train/X_valid after splitting and one-hot encoding ---\n",
        "# This is crucial to ensure consistency between feature sets for model training.\n",
        "missing_in_valid = set(X_train.columns) - set(X_valid.columns)\n",
        "missing_in_train = set(X_valid.columns) - set(X_train.columns)\n",
        "if missing_in_valid or missing_in_train:\n",
        "    print(\"Warning: Mismatch in columns between train and validation sets!\")\n",
        "    if missing_in_valid:\n",
        "        print(f\"Missing in valid: {missing_in_valid}\")\n",
        "    if missing_in_train:\n",
        "        print(f\"Missing in train: {missing_in_train}\")\n",
        "    # Align columns to ensure both have the same set\n",
        "    common_cols = list(set(X_train.columns) & set(X_valid.columns))\n",
        "    X_train = X_train[common_cols]\n",
        "    X_valid = X_valid[common_cols]\n",
        "    print(\"Columns aligned for train and validation sets.\")\n",
        "\n",
        "\n",
        "# --- Display basic statistics for the target variable ---\n",
        "# This helps in understanding the scale of the target and interpreting RMSE/MAE.\n",
        "print(f\"\\nTarget variable ('Weekly_Sales') statistics (full dataset):\")\n",
        "print(f\"Mean: {df['Weekly_Sales'].mean():.2f}\")\n",
        "print(f\"Standard Deviation: {df['Weekly_Sales'].std():.2f}\")\n",
        "print(f\"Min: {df['Weekly_Sales'].min():.2f}\")\n",
        "print(f\"Max: {df['Weekly_Sales'].max():.2f}\")\n"
      ],
      "metadata": {
        "id": "1"
      },
      "cell_type": "code",
      "outputs": []
    },
    {
      "source": [
        "# --- Cell 2: MLflow Setup and LightGBM Training and Test Evaluation ---\n",
        "\n",
        "# --- MLflow Configuration ---\n",
        "# Set MLflow tracking URI.\n",
        "# Use 'file:/content/mlruns' for local tracking in Colab.\n",
        "# If you are using a remote MLflow server, replace with its URI.\n",
        "mlflow.set_tracking_uri(\"file:/content/mlruns\")\n",
        "\n",
        "# Set MLflow experiment name.\n",
        "# As per project instructions, each model architecture should have a separate experiment.\n",
        "experiment_name = \"LightGBM_Training\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "print(f\"MLflow Experiment '{experiment_name}' set up.\")\n",
        "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "\n",
        "# --- Start MLflow Run and Train LightGBM Model ---\n",
        "# Start an MLflow run to log the training process.\n",
        "# This run will now encompass training, validation, and test evaluation.\n",
        "with mlflow.start_run(run_name='LGBM_baseline_run'):\n",
        "    # Define LightGBM hyperparameters\n",
        "    # You can tune these parameters for better performance.\n",
        "    params = {\n",
        "        'objective': 'regression_l1', # MAE objective: 'regression_l1', RMSE objective: 'regression_l2'\n",
        "        'metric': 'rmse',             # Metric to evaluate during training\n",
        "        'n_estimators': 1500,         # Number of boosting rounds (trees)\n",
        "        'learning_rate': 0.03,        # Step size shrinkage\n",
        "        'num_leaves': 31,             # Max number of leaves in one tree\n",
        "        'max_depth': 8,               # Max tree depth\n",
        "        'min_child_samples': 20,      # Minimum number of data needed in a child (leaf)\n",
        "        'subsample': 0.8,             # Subsample ratio of the training instance\n",
        "        'colsample_bytree': 0.8,      # Subsample ratio of columns when constructing each tree\n",
        "        'random_state': 42,           # Random seed for reproducibility\n",
        "        'n_jobs': -1,                 # Use all available CPU cores\n",
        "        'reg_alpha': 0.1,             # L1 regularization (alpha)\n",
        "        'reg_lambda': 0.1             # L2 regularization (lambda)\n",
        "    }\n",
        "\n",
        "    # Log hyperparameters to MLflow\n",
        "    mlflow.log_params(params)\n",
        "    print(\"\\nLightGBM model training in progress...\")\n",
        "\n",
        "    # Initialize and train LightGBM Regressor model\n",
        "    model = lgb.LGBMRegressor(**params)\n",
        "\n",
        "    # Use callbacks for early stopping and logging during training\n",
        "    callbacks = [\n",
        "        lgb.early_stopping(100, verbose=False), # Stop if validation metric doesn't improve for 100 rounds\n",
        "        lgb.log_evaluation(period=200) # Log evaluation results every 200 boosting rounds\n",
        "    ]\n",
        "\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_valid, y_valid)],\n",
        "              eval_metric='rmse', # Specify evaluation metric for early stopping\n",
        "              callbacks=callbacks)\n",
        "\n",
        "    print(\"LightGBM model training finished.\")\n",
        "\n",
        "    # --- Evaluation on Validation Set ---\n",
        "    # Make predictions on the validation set\n",
        "    preds_val = model.predict(X_valid)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    val_rmse = np.sqrt(mean_squared_error(y_valid, preds_val))\n",
        "    val_mae = mean_absolute_error(y_valid, preds_val)\n",
        "\n",
        "    print(f\"\\nValidation RMSE: {val_rmse:.2f}\")\n",
        "    print(f\"Validation MAE: {val_mae:.2f}\")\n",
        "\n",
        "    # Log metrics to MLflow\n",
        "    mlflow.log_metric(\"val_rmse\", val_rmse)\n",
        "    mlflow.log_metric(\"val_mae\", val_mae)\n",
        "    mlflow.log_metric(\"best_iteration\", model.best_iteration_) # Log the best iteration found by early stopping\n",
        "\n",
        "    # --- Save Model to MLflow ---\n",
        "    # Log the trained LightGBM model to MLflow.\n",
        "    # Register the model in MLflow Model Registry for easy retrieval later.\n",
        "    mlflow.lightgbm.log_model(model, artifact_path='model', registered_model_name=\"LightGBMSalesForecaster\")\n",
        "    print(\"\\nModel logged to MLflow.\")\n",
        "    print(f\"MLflow Run ID: {mlflow.active_run().info.run_id}\")\n",
        "\n",
        "    # --- Test Set Evaluation (Moved from Cell 3) ---\n",
        "    print(\"\\nPreparing test data for evaluation...\")\n",
        "\n",
        "    # Merge df_test with df_stores\n",
        "    df_test_merged = df_test.merge(df_stores, on='Store', how='left')\n",
        "\n",
        "    # Identify common columns in df_test_merged and df_features (excluding merge keys)\n",
        "    common_cols_to_drop_from_test = [\n",
        "        'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "        'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5',\n",
        "        'IsHoliday'\n",
        "    ]\n",
        "    cols_to_drop_existing_test = [col for col in common_cols_to_drop_from_test if col in df_test_merged.columns]\n",
        "\n",
        "    if cols_to_drop_existing_test:\n",
        "        print(f\"Dropping common columns from df_test (from test.csv/stores.csv) before merging with features.csv: {cols_to_drop_existing_test}\")\n",
        "        df_test_merged = df_test_merged.drop(columns=cols_to_drop_existing_test)\n",
        "    else:\n",
        "        print(\"No common columns to drop from df_test_merged before merging with features.csv (or they don't exist).\")\n",
        "\n",
        "    # Merge df_test_merged with df_features\n",
        "    df_test_final = df_test_merged.merge(df_features, on=['Store', 'Date'], how='left')\n",
        "\n",
        "    # Handle missing values in df_test_final (consistent with training)\n",
        "    print(\"Handling missing values in test data...\")\n",
        "    for col in ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']:\n",
        "        if col in df_test_final.columns:\n",
        "            df_test_final[col] = df_test_final[col].fillna(0)\n",
        "        else:\n",
        "            print(f\"Warning: Column '{col}' not found in df_test_final for NaN handling.\")\n",
        "    print(\"Missing values in test data filled.\")\n",
        "\n",
        "    # Convert IsHoliday to numerical in df_test_final\n",
        "    if 'IsHoliday' in df_test_final.columns:\n",
        "        df_test_final['IsHoliday'] = df_test_final['IsHoliday'].astype(int)\n",
        "    else:\n",
        "        print(\"Warning: 'IsHoliday' column not found in df_test_final. Adding it with default value 0.\")\n",
        "        df_test_final['IsHoliday'] = 0\n",
        "\n",
        "    # Apply feature engineering to df_test_final\n",
        "    print(\"Creating time-based features for test data...\")\n",
        "    df_test_final = create_features(df_test_final.copy())\n",
        "    print(\"Time-based features for test data created.\")\n",
        "\n",
        "    # Convert 'Type' to numerical using OneHotEncoder for df_test_final\n",
        "    print(\"Applying One-Hot Encoding to 'Type' column in test data...\")\n",
        "    if 'Type' in df_test_final.columns:\n",
        "        df_test_final = pd.get_dummies(df_test_final, columns=['Type'], prefix='Type', drop_first=True)\n",
        "    else:\n",
        "        print(\"Warning: 'Type' column not found in df_test_final. Skipping One-Hot Encoding for 'Type'.\")\n",
        "    print(\"Test data preprocessing complete.\")\n",
        "\n",
        "    # Align test set columns with training set columns (X_train)\n",
        "    X_test = df_test_final.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "    # Ensure no NaNs remain in X_test before prediction (after reindexing)\n",
        "    if X_test.isnull().sum().sum() > 0:\n",
        "        print(\"Warning: NaNs found in X_test after alignment. Filling with 0.\")\n",
        "        X_test = X_test.fillna(0)\n",
        "\n",
        "    print(f\"Final Test set size for prediction: {len(X_test)}\")\n",
        "    print(f\"Test set features (first 5 rows):\\n{X_test.head()}\")\n",
        "\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    preds_test = model.predict(X_test)\n",
        "\n",
        "    # Create submission DataFrame\n",
        "    submission_df = pd.DataFrame({'Id': df_test['Store'].astype(str) + '_' + df_test['Dept'].astype(str) + '_' + df_test['Date'].dt.strftime('%Y-%m-%d'),\n",
        "                                  'Weekly_Sales': preds_test})\n",
        "\n",
        "    # Ensure no negative sales in submission (as per competition rules)\n",
        "    submission_df['Weekly_Sales'] = submission_df['Weekly_Sales'].apply(lambda x: max(0, x))\n",
        "\n",
        "    # Save submission file\n",
        "    submission_file_path = 'submission_lightgbm.csv'\n",
        "    submission_df.to_csv(submission_file_path, index=False)\n",
        "    print(f\"\\nSubmission file '{submission_file_path}' created successfully.\")\n",
        "\n",
        "    # Log submission file as an MLflow artifact\n",
        "    mlflow.log_artifact(submission_file_path)\n",
        "    print(f\"Submission file logged as MLflow artifact.\")\n",
        "\n",
        "print(\"\\nLightGBM experiment finished. Check MLflow UI for full results and submission file.\")\n"
      ],
      "metadata": {
        "id": "2"
      },
      "cell_type": "code",
      "outputs": []
    },
    {
      "source": [
        "# --- Cell 3: MLflow UI with ngrok (Optional) ---\n",
        "# This cell was previously Cell 4.\n",
        "\n",
        "# IMPORTANT: Replace \"YOUR_NGROK_AUTH_TOKEN\" with your actual ngrok token.\n",
        "# Get your token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "# This cell needs to be run only once per Colab session if the token is set.\n",
        "\n",
        "import time # Import time for sleep\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# Kill any existing ngrok tunnels to ensure a clean start\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"Terminated any existing ngrok tunnels.\")\n",
        "    time.sleep(2) # Add a small delay to ensure processes are fully terminated\n",
        "except Exception as e:\n",
        "    print(f\"Could not terminate existing ngrok tunnels (might not exist): {e}\")\n",
        "\n",
        "# Set ngrok authentication token\n",
        "try:\n",
        "    # This is a placeholder. In a real scenario, you'd get this from a secure source\n",
        "    # or prompt the user. For a Colab notebook, direct setting is common.\n",
        "    NGROK_AUTH_TOKEN = \"2zXieAERZUJhQWKxXhKwvjXc1fh_2CSAEsiYxxG6iSoupmRN9\" # <--- REPLACE THIS WITH YOUR ACTUAL TOKEN\n",
        "    print(f\"NGROK_AUTH_TOKEN value being used: '{NGROK_AUTH_TOKEN}'\") # Debug print\n",
        "\n",
        "    if NGROK_AUTH_TOKEN == \"YOUR_NGROK_AUTH_TOKEN\":\n",
        "        print(\"CRITICAL: 'YOUR_NGROK_AUTH_TOKEN' placeholder still present. Please replace it with your actual token.\")\n",
        "        # You might want to raise an error or exit here if the token is critical\n",
        "        raise ValueError(\"ngrok authentication token not set correctly.\")\n",
        "    \n",
        "    conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "    print(\"ngrok authentication token set.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error setting ngrok auth token: {e}\")\n",
        "    print(\"Please ensure your ngrok token is correct and pasted without extra spaces or characters.\")\n",
        "    print(\"Also, check your ngrok dashboard (https://dashboard.ngrok.com/agents) to ensure no active sessions are running.\")\n",
        "\n",
        "\n",
        "# Run MLflow UI in the background\n",
        "# It typically runs on port 5000\n",
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\")\n",
        "\n",
        "# Create an ngrok tunnel to expose the MLflow UI\n",
        "print(\"Creating ngrok tunnel for MLflow UI...\")\n",
        "try:\n",
        "    public_url = ngrok.connect(addr=\"5000\", proto=\"http\")\n",
        "    print(f\"MLflow UI is available at: {public_url}\")\n",
        "    print(\"Click the link above to access the MLflow UI in your browser.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating ngrok tunnel: {e}\")\n",
        "    print(\"Please ensure ngrok is installed and your auth token is correct.\")\n",
        "    print(\"If the error persists, check your ngrok dashboard (https://dashboard.ngrok.com/agents) and manually kill any active sessions.\")\n"
      ],
      "metadata": {
        "id": "3"
      },
      "cell_type": "code",
      "outputs": []
    }
  ]
}
