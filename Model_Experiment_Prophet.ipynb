{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/abarb2022/Walmart-Recruiting---Store-Sales-Forecasting/blob/main/model_experiment_prophet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4520,
     "status": "ok",
     "timestamp": 1753967182711,
     "user": {
      "displayName": "ZipperNator",
      "userId": "13572371089708989227"
     },
     "user_tz": -240
    },
    "id": "KlwSaX9akGfG",
    "outputId": "6fe9c448-1f0d-4591-ec94-6f845d9c3263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.7.14)\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.5.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2211,
     "status": "ok",
     "timestamp": 1753967184944,
     "user": {
      "displayName": "ZipperNator",
      "userId": "13572371089708989227"
     },
     "user_tz": -240
    },
    "id": "NGineQt7dErh",
    "outputId": "24da8c81-1871-4400-f516-d5661b287dc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3776,
     "status": "ok",
     "timestamp": 1753967190449,
     "user": {
      "displayName": "ZipperNator",
      "userId": "13572371089708989227"
     },
     "user_tz": -240
    },
    "id": "ZTkKggcylXfa",
    "outputId": "09be0e5b-a83f-49d3-fea5-adab52890d70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
      "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
      "\r100% 2.70M/2.70M [00:00<00:00, 217MB/s]\n",
      "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
      "  inflating: features.csv.zip        \n",
      "  inflating: sampleSubmission.csv.zip  \n",
      "  inflating: stores.csv              \n",
      "  inflating: test.csv.zip            \n",
      "  inflating: train.csv.zip           \n"
     ]
    }
   ],
   "source": [
    "! mkdir ~/.kaggle\n",
    "!cp /content/drive/MyDrive/ML/kaggle.json ~/.kaggle/kaggle.json\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "! kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
    "! unzip walmart-recruiting-store-sales-forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAb9vK9B7YFb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import gc\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "255em5G65SWD"
   },
   "outputs": [],
   "source": [
    "\n",
    "stores = pd.read_csv('stores.csv')\n",
    "train = pd.read_csv(\"train.csv.zip\")\n",
    "features = pd.read_csv('features.csv.zip')\n",
    "sample = pd.read_csv('sampleSubmission.csv.zip')\n",
    "test = pd.read_csv('test.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 364,
     "status": "ok",
     "timestamp": 1753967198844,
     "user": {
      "displayName": "ZipperNator",
      "userId": "13572371089708989227"
     },
     "user_tz": -240
    },
    "id": "OFPJWG6V5nZ3",
    "outputId": "02aa43d2-9b7e-420f-aee2-c7ab02c33f80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Merged Train Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 421570 entries, 0 to 421569\n",
      "Data columns (total 16 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   Store         421570 non-null  int64         \n",
      " 1   Dept          421570 non-null  int64         \n",
      " 2   Date          421570 non-null  datetime64[ns]\n",
      " 3   Weekly_Sales  421570 non-null  float64       \n",
      " 4   IsHoliday     421570 non-null  bool          \n",
      " 5   Temperature   421570 non-null  float64       \n",
      " 6   Fuel_Price    421570 non-null  float64       \n",
      " 7   MarkDown1     150681 non-null  float64       \n",
      " 8   MarkDown2     111248 non-null  float64       \n",
      " 9   MarkDown3     137091 non-null  float64       \n",
      " 10  MarkDown4     134967 non-null  float64       \n",
      " 11  MarkDown5     151432 non-null  float64       \n",
      " 12  CPI           421570 non-null  float64       \n",
      " 13  Unemployment  421570 non-null  float64       \n",
      " 14  Type          421570 non-null  object        \n",
      " 15  Size          421570 non-null  int64         \n",
      "dtypes: bool(1), datetime64[ns](1), float64(10), int64(3), object(1)\n",
      "memory usage: 48.6+ MB\n",
      "None\n",
      "\n",
      "--- Merged Test Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 115064 entries, 0 to 115063\n",
      "Data columns (total 15 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   Store         115064 non-null  int64         \n",
      " 1   Dept          115064 non-null  int64         \n",
      " 2   Date          115064 non-null  datetime64[ns]\n",
      " 3   IsHoliday     115064 non-null  bool          \n",
      " 4   Temperature   115064 non-null  float64       \n",
      " 5   Fuel_Price    115064 non-null  float64       \n",
      " 6   MarkDown1     114915 non-null  float64       \n",
      " 7   MarkDown2     86437 non-null   float64       \n",
      " 8   MarkDown3     105235 non-null  float64       \n",
      " 9   MarkDown4     102176 non-null  float64       \n",
      " 10  MarkDown5     115064 non-null  float64       \n",
      " 11  CPI           76902 non-null   float64       \n",
      " 12  Unemployment  76902 non-null   float64       \n",
      " 13  Type          115064 non-null  object        \n",
      " 14  Size          115064 non-null  int64         \n",
      "dtypes: bool(1), datetime64[ns](1), float64(9), int64(3), object(1)\n",
      "memory usage: 12.4+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Date'] = pd.to_datetime(train['Date'])\n",
    "test['Date'] = pd.to_datetime(test['Date'])\n",
    "features['Date'] = pd.to_datetime(features['Date'])\n",
    "train_df = pd.merge(train, features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
    "test_df = pd.merge(test, features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
    "\n",
    "\n",
    "train_df = pd.merge(train_df, stores, on='Store', how='left')\n",
    "test_df = pd.merge(test_df, stores, on='Store', how='left')\n",
    "\n",
    "print(\"\\n--- Merged Train Data Info ---\")\n",
    "print(train_df.info())\n",
    "print(\"\\n--- Merged Test Data Info ---\")\n",
    "print(test_df.info())\n",
    "\n",
    "del train, test, features, stores\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFCOb354o73R"
   },
   "outputs": [],
   "source": [
    "class MissingValueImputer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, markdown_cols=None, numerical_cols_to_impute=None):\n",
    "        self.markdown_cols = markdown_cols if markdown_cols is not None else [f'MarkDown{i}' for i in range(1, 6)]\n",
    "        self.numerical_cols_to_impute = numerical_cols_to_impute if numerical_cols_to_impute is not None else ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
    "        self.means = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.numerical_cols_to_impute:\n",
    "            if col in X.columns:\n",
    "                self.means[col] = X[col].mean()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "\n",
    "\n",
    "        for col in self.markdown_cols:\n",
    "          if col in X_copy.columns:\n",
    "            X_copy[f\"{col}_was_missing\"] = X_copy[col].isna().astype(int)\n",
    "            X_copy[col] = X_copy[col].fillna(0)\n",
    "\n",
    "\n",
    "        for col in self.numerical_cols_to_impute:\n",
    "            if col in X_copy.columns:\n",
    "                X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n",
    "                if X_copy[col].isnull().any() and col in self.means:\n",
    "                    X_copy[col] = X_copy[col].fillna(self.means[col])\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okLoIoZSZlMU"
   },
   "outputs": [],
   "source": [
    "class AdvancedDateFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, date_column: str = 'Date', include_holidays: bool = True,\n",
    "                 include_seasonal: bool = True, include_lags: bool = False):\n",
    "        self.date_column = date_column\n",
    "        self.include_holidays = include_holidays\n",
    "        self.include_seasonal = include_seasonal\n",
    "        self.include_lags = include_lags\n",
    "\n",
    "    def _is_holiday_period(self, date):\n",
    "        month, day = date.month, date.day\n",
    "\n",
    "        if month == 11 and day >= 22:\n",
    "            return 1\n",
    "        elif month == 12:\n",
    "            return 1\n",
    "        elif month == 1 and day <= 7:\n",
    "            return 1\n",
    "        elif month == 9 and day <= 7:\n",
    "            return 1\n",
    "        elif month == 5 and day >= 25:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        if self.date_column not in X_copy.columns:\n",
    "            raise ValueError(f\"Date column '{self.date_column}' not found in DataFrame.\")\n",
    "\n",
    "        X_copy[self.date_column] = pd.to_datetime(X_copy[self.date_column])\n",
    "\n",
    "        X_copy['Year'] = X_copy[self.date_column].dt.year\n",
    "        X_copy['Month'] = X_copy[self.date_column].dt.month\n",
    "        X_copy['Day'] = X_copy[self.date_column].dt.day\n",
    "        X_copy['DayOfWeek'] = X_copy[self.date_column].dt.dayofweek\n",
    "        X_copy['Week'] = X_copy[self.date_column].dt.isocalendar().week.astype(int)\n",
    "        X_copy['Quarter'] = X_copy[self.date_column].dt.quarter\n",
    "        X_copy['DayOfYear'] = X_copy[self.date_column].dt.dayofyear\n",
    "\n",
    "        X_copy['Month_sin'] = np.sin(2 * np.pi * X_copy['Month'] / 12)\n",
    "        X_copy['Month_cos'] = np.cos(2 * np.pi * X_copy['Month'] / 12)\n",
    "        X_copy['Week_sin'] = np.sin(2 * np.pi * X_copy['Week'] / 52)\n",
    "        X_copy['Week_cos'] = np.cos(2 * np.pi * X_copy['Week'] / 52)\n",
    "        X_copy['DayOfWeek_sin'] = np.sin(2 * np.pi * X_copy['DayOfWeek'] / 7)\n",
    "        X_copy['DayOfWeek_cos'] = np.cos(2 * np.pi * X_copy['DayOfWeek'] / 7)\n",
    "\n",
    "        if self.include_seasonal:\n",
    "            X_copy['Season'] = X_copy['Month'].map({12: 0, 1: 0, 2: 0,\n",
    "                                                   3: 1, 4: 1, 5: 1,\n",
    "                                                   6: 2, 7: 2, 8: 2,\n",
    "                                                   9: 3, 10: 3, 11: 3})\n",
    "\n",
    "            X_copy['IsWeekend'] = (X_copy['DayOfWeek'] >= 5).astype(int)\n",
    "\n",
    "            X_copy['IsMonthEnd'] = (X_copy[self.date_column].dt.is_month_end).astype(int)\n",
    "            X_copy['IsMonthStart'] = (X_copy[self.date_column].dt.is_month_start).astype(int)\n",
    "\n",
    "        if self.include_holidays:\n",
    "            X_copy['IsHolidayPeriod'] = X_copy[self.date_column].apply(self._is_holiday_period)\n",
    "\n",
    "            if 'IsHoliday' in X_copy.columns:\n",
    "                if X_copy['IsHoliday'].dtype == bool:\n",
    "                    X_copy['IsHoliday'] = X_copy['IsHoliday'].astype(int)\n",
    "\n",
    "        markdown_cols = [col for col in X_copy.columns if col.startswith('MarkDown') and not col.endswith('_was_missing')]\n",
    "        if markdown_cols:\n",
    "            X_copy['Total_MarkDown'] = X_copy[markdown_cols].sum(axis=1)\n",
    "            X_copy['MarkDown_Intensity'] = X_copy['Total_MarkDown'] / (X_copy['Total_MarkDown'].mean() + 1e-8)\n",
    "            X_copy['HasMarkDown'] = (X_copy['Total_MarkDown'] > 0).astype(int)\n",
    "\n",
    "        econ_cols = ['Fuel_Price', 'CPI', 'Unemployment']\n",
    "        available_econ = [col for col in econ_cols if col in X_copy.columns]\n",
    "\n",
    "        if len(available_econ) >= 2:\n",
    "            if 'Fuel_Price' in X_copy.columns and 'CPI' in X_copy.columns:\n",
    "                X_copy['Fuel_CPI_Ratio'] = X_copy['Fuel_Price'] / (X_copy['CPI'] + 1e-8)\n",
    "\n",
    "            if 'CPI' in X_copy.columns and 'Unemployment' in X_copy.columns:\n",
    "                X_copy['Economic_Index'] = (X_copy['CPI'] * 0.4 + (100 - X_copy['Unemployment']) * 0.6) / 100\n",
    "\n",
    "        return X_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzQNQ5a8tRsi"
   },
   "outputs": [],
   "source": [
    "class DateFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, date_column='Date'):\n",
    "        self.date_column = date_column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        if self.date_column not in X_copy.columns:\n",
    "            raise ValueError(f\"Date column '{self.date_column}' not found in DataFrame.\")\n",
    "\n",
    "        X_copy[self.date_column] = pd.to_datetime(X_copy[self.date_column])\n",
    "\n",
    "        X_copy['Year'] = X_copy[self.date_column].dt.year\n",
    "        X_copy['Month'] = X_copy[self.date_column].dt.month\n",
    "        X_copy['Month_sin'] = np.sin(2 * np.pi * X_copy['Month'] / 12)\n",
    "        X_copy['Month_cos'] = np.cos(2 * np.pi * X_copy['Month'] / 12)\n",
    "\n",
    "        X_copy['Week'] = X_copy[self.date_column].dt.isocalendar().week.astype(int)\n",
    "        X_copy['Day'] = X_copy[self.date_column].dt.day\n",
    "        X_copy['DayOfWeek'] = X_copy[self.date_column].dt.dayofweek\n",
    "\n",
    "        if 'IsHoliday' in X_copy.columns and X_copy['IsHoliday'].dtype == bool:\n",
    "            X_copy['IsHoliday'] = X_copy['IsHoliday'].astype(int)\n",
    "\n",
    "        return X_copy.drop(columns=[ \"Month\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUuRfpD0toh1"
   },
   "outputs": [],
   "source": [
    "class CategoricalFeatureConverter(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, categorical_cols=None):\n",
    "        self.categorical_cols = categorical_cols if categorical_cols is not None else ['Store', 'Dept', 'Type']\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.categorical_cols:\n",
    "            if col in X_copy.columns:\n",
    "                X_copy[col] = X_copy[col].astype('category')\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1753967201568,
     "user": {
      "displayName": "ZipperNator",
      "userId": "13572371089708989227"
     },
     "user_tz": -240
    },
    "id": "zhSWnKEXaYyg",
    "outputId": "fe594b77-3e05-402a-f046-43a8e423b5e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-4289254921.py:7: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  holiday_dates = pd.concat([holiday_dates, pd.DataFrame({\n"
     ]
    }
   ],
   "source": [
    "def get_walmart_holidays():\n",
    "    holiday_dates = pd.DataFrame(columns=['ds', 'holiday'])\n",
    "\n",
    "    super_bowl = ['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08']\n",
    "    holiday_dates = pd.concat([holiday_dates, pd.DataFrame({\n",
    "        'ds': pd.to_datetime(super_bowl),\n",
    "        'holiday': 'Super Bowl'\n",
    "    })])\n",
    "\n",
    "    labor_day = ['2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06']\n",
    "    holiday_dates = pd.concat([holiday_dates, pd.DataFrame({\n",
    "        'ds': pd.to_datetime(labor_day),\n",
    "        'holiday': 'Labor Day'\n",
    "    })])\n",
    "\n",
    "    thanksgiving = ['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29']\n",
    "    holiday_dates = pd.concat([holiday_dates, pd.DataFrame({\n",
    "        'ds': pd.to_datetime(thanksgiving),\n",
    "        'holiday': 'Thanksgiving'\n",
    "    })])\n",
    "\n",
    "    christmas = ['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27']\n",
    "    holiday_dates = pd.concat([holiday_dates, pd.DataFrame({\n",
    "        'ds': pd.to_datetime(christmas),\n",
    "        'holiday': 'Christmas'\n",
    "    })])\n",
    "\n",
    "    return holiday_dates\n",
    "\n",
    "walmart_holidays = get_walmart_holidays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5883,
     "status": "ok",
     "timestamp": 1753967207729,
     "user": {
      "displayName": "ZipperNator",
      "userId": "13572371089708989227"
     },
     "user_tz": -240
    },
    "id": "Y6ftBKfUdosU",
    "outputId": "40bd7f2a-fdee-4190-d1a4-3be33744ba50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: prophet in /usr/local/lib/python3.11/dist-packages (1.1.7)\n",
      "Requirement already satisfied: holidays in /usr/local/lib/python3.11/dist-packages (0.77)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
      "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (1.2.5)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (2.0.2)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from prophet) (3.10.0)\n",
      "Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.11/dist-packages (from prophet) (4.67.1)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from prophet) (6.5.2)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from holidays) (2.9.0.post0)\n",
      "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (3.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.4->prophet) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.4->prophet) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->holidays) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install prophet holidays joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1753967283598,
     "user": {
      "displayName": "ZipperNator",
      "userId": "13572371089708989227"
     },
     "user_tz": -240
    },
    "id": "ZYY96Iz0cY0V",
    "outputId": "25f57feb-6185-49ca-e938-207aae5144d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1027183038.py:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n",
      "/tmp/ipython-input-1027183038.py:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_train = train_df['Weekly_Sales']\n",
    "X_train = train_df.drop(columns=['Weekly_Sales', 'Id'], errors='ignore')\n",
    "\n",
    "temp_train_df = X_train.copy()\n",
    "temp_train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
    "temp_train_df['Weekly_Sales'] = y_train\n",
    "\n",
    "temp_train_df = temp_train_df.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "validation_cutoff_date = pd.to_datetime('2012-07-01')\n",
    "\n",
    "X_train_split = temp_train_df[temp_train_df['Date'] < validation_cutoff_date]\n",
    "y_train_split = temp_train_df[temp_train_df['Date'] < validation_cutoff_date]['Weekly_Sales']\n",
    "\n",
    "X_val_split = temp_train_df[temp_train_df['Date'] >= validation_cutoff_date]\n",
    "y_val_split = temp_train_df[temp_train_df['Date'] >= validation_cutoff_date]['Weekly_Sales']\n",
    "\n",
    "preprocessing_pipeline = Pipeline([\n",
    " #   ('date_extractor', AdvancedDateFeatureExtractor()),\n",
    "    ('missing_imputer', MissingValueImputer())\n",
    "#    ('cat_converter', CategoricalFeatureConverter())\n",
    "])\n",
    "\n",
    "X_train_processed = preprocessing_pipeline.fit_transform(X_train_split)\n",
    "X_val_processed = preprocessing_pipeline.transform(X_val_split)\n",
    "#X_train_processed = X_train_split.copy()\n",
    "#X_val_processed = X_val_split.copy()\n",
    "X_train_processed['Weekly_Sales'] = y_train_split.values\n",
    "X_val_processed['Weekly_Sales'] = y_val_split.values\n",
    "\n",
    "\n",
    "\n",
    "def weighted_mean_absolute_error(y_true, y_pred, weights):\n",
    "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
    "\n",
    "val_weights = np.where(X_val_split['IsHoliday'] == 1, 5, 1)\n",
    "\n",
    "log_data = {\n",
    "\n",
    "    'total_combinations': 0,\n",
    "    'models_trained': 0,\n",
    "    'models_skipped': 0,\n",
    "    'validation_metrics': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Qw3ytJbmcfG"
   },
   "outputs": [],
   "source": [
    "def get_nearest_dept(store, dept, X_train_processed):\n",
    "    \"\"\"Find department in same store with closest average sales\"\"\"\n",
    "    store_data = X_train_processed[X_train_processed['Store'] == store]\n",
    "    if len(store_data) == 0:\n",
    "        return None\n",
    "\n",
    "    dept_means = store_data.copy()\n",
    "    dept_means['Dept'] = pd.to_numeric(dept_means['Dept'], errors='coerce')\n",
    "    dept = pd.to_numeric(dept, errors='coerce')\n",
    "\n",
    "    dept_means = dept_means.groupby('Dept')['Weekly_Sales'].mean().reset_index()\n",
    "    dept_means['abs_diff'] = np.abs(dept_means['Dept'] - dept)\n",
    "\n",
    "    nearest = dept_means.sort_values(['abs_diff', 'Weekly_Sales']).iloc[0]\n",
    "    return nearest['Dept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8V4nBufmhA1"
   },
   "outputs": [],
   "source": [
    "def train_models(X_train_processed, X_val_processed):\n",
    "    log_data = {\n",
    "        'total_combinations': 0,\n",
    "        'models_trained': 0,\n",
    "        'models_skipped': 0,\n",
    "        'fallback_used': 0,\n",
    "        'validation_metrics': {}\n",
    "    }\n",
    "\n",
    "    combinations = X_train_processed[['Store', 'Dept']].drop_duplicates()\n",
    "    log_data['total_combinations'] = len(combinations)\n",
    "    val_results = []\n",
    "    models = {}\n",
    "\n",
    "    # es washale\n",
    "    extra_regressors = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
    "                      'IsHoliday', 'Size', 'MarkDown1', 'MarkDown2',\n",
    "                      'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
    "\n",
    "    print(\"\\nTraining global fallback model...\")\n",
    "    prophet_train_global = X_train_processed[['Date', 'Weekly_Sales'] + extra_regressors].rename(columns={\n",
    "        'Date': 'ds',\n",
    "        'Weekly_Sales': 'y'\n",
    "    })\n",
    "\n",
    "    model_global = Prophet(\n",
    "        holidays=walmart_holidays,\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=False,\n",
    "        seasonality_mode='multiplicative'\n",
    "    )\n",
    "\n",
    "    for regressor in extra_regressors:\n",
    "        if regressor in prophet_train_global.columns:\n",
    "            model_global.add_regressor(regressor)\n",
    "\n",
    "    model_global.fit(prophet_train_global)\n",
    "    print(\"Global fallback model trained.\")\n",
    "\n",
    "    print(f\"\\nStarting training on {len(X_train_processed):,} samples\")\n",
    "    print(f\"Found {len(combinations)} unique store-dept combinations\")\n",
    "\n",
    "    for idx, (store, dept) in enumerate(combinations.itertuples(index=False), 1):\n",
    "        print(f\"\\nProcessing {idx}/{len(combinations)} - Store {store}, Dept {dept}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        train_data = X_train_processed[(X_train_processed['Store'] == store) &\n",
    "                                     (X_train_processed['Dept'] == dept)]\n",
    "        val_data = X_val_processed[(X_val_processed['Store'] == store) &\n",
    "                                 (X_val_processed['Dept'] == dept)]\n",
    "\n",
    "        if len(train_data) < 10:\n",
    "            print(f\"  Skipping - only {len(train_data)} training samples\")\n",
    "            log_data['models_skipped'] += 1\n",
    "            continue\n",
    "\n",
    "        prophet_train = train_data[['Date', 'Weekly_Sales'] + extra_regressors].rename(columns={\n",
    "            'Date': 'ds',\n",
    "            'Weekly_Sales': 'y'\n",
    "        })\n",
    "\n",
    "        model = Prophet(\n",
    "            holidays=walmart_holidays,\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=False,\n",
    "            seasonality_mode='multiplicative'\n",
    "        )\n",
    "\n",
    "        for regressor in extra_regressors:\n",
    "            if regressor in prophet_train.columns:\n",
    "                model.add_regressor(regressor)\n",
    "\n",
    "        print(f\"  Training on {len(prophet_train)} samples...\")\n",
    "        model.fit(prophet_train)\n",
    "        models[(store, dept)] = model\n",
    "        log_data['models_trained'] += 1\n",
    "\n",
    "        if len(val_data) > 0:\n",
    "            prophet_val = val_data[['Date'] + extra_regressors].rename(columns={'Date': 'ds'})\n",
    "            forecast = model.predict(prophet_val)\n",
    "\n",
    "            val_actual = val_data['Weekly_Sales'].values\n",
    "            val_pred = forecast['yhat'].values\n",
    "            val_weights = np.where(val_data['IsHoliday'] == 1, 5, 1)\n",
    "\n",
    "            mae = mean_absolute_error(val_actual, val_pred)\n",
    "            wmae = weighted_mean_absolute_error(val_actual, val_pred, val_weights)\n",
    "\n",
    "            val_results.append({\n",
    "                'Store': store,\n",
    "                'Dept': dept,\n",
    "                'Date': val_data['Date'].values,\n",
    "                'Actual': val_actual,\n",
    "                'Predicted': val_pred,\n",
    "                'MAE': mae,\n",
    "                'WMAE': wmae,\n",
    "                'IsHoliday': val_data['IsHoliday'].values,\n",
    "                'UsedFallback': False\n",
    "            })\n",
    "\n",
    "            print(f\"  Validation MAE: {mae:.2f}, WMAE: {wmae:.2f}\")\n",
    "\n",
    "    '''\n",
    "    skipped_combinations = set(combinations.itertuples(index=False)) - set(models.keys())\n",
    "    for store, dept in skipped_combinations:\n",
    "\n",
    "        print(f\"\\nProcessing skipped combination - Store {store}, Dept {dept}\")\n",
    "        val_data = X_val_processed[(X_val_processed['Store'] == store) &\n",
    "                                 (X_val_processed['Dept'] == dept)]\n",
    "\n",
    "        if len(val_data) == 0:\n",
    "            continue\n",
    "\n",
    "        # Try nearest neighbor fallback first\n",
    "        nearest_dept = get_nearest_dept(store, dept, X_train_processed)\n",
    "        if nearest_dept is not None and (store, nearest_dept) in models:\n",
    "            print(f\"  Using nearest neighbor: Store {store}, Dept {nearest_dept}\")\n",
    "            model = models[(store, nearest_dept)]\n",
    "            fallback_type = \"nearest_neighbor\"\n",
    "        else:\n",
    "            print(\"  Using global fallback model\")\n",
    "            model = model_global\n",
    "            fallback_type = \"global\"\n",
    "\n",
    "        prophet_val = val_data[['Date'] + extra_regressors].rename(columns={'Date': 'ds'})\n",
    "        forecast = model.predict(prophet_val)\n",
    "\n",
    "        val_actual = val_data['Weekly_Sales'].values\n",
    "        val_pred = forecast['yhat'].values\n",
    "        val_weights = np.where(val_data['IsHoliday'] == 1, 5, 1)\n",
    "\n",
    "        mae = mean_absolute_error(val_actual, val_pred)\n",
    "        wmae = weighted_mean_absolute_error(val_actual, val_pred, val_weights)\n",
    "\n",
    "        val_results.append({\n",
    "            'Store': store,\n",
    "            'Dept': dept,\n",
    "            'Date': val_data['Date'].values,\n",
    "            'Actual': val_actual,\n",
    "            'Predicted': val_pred,\n",
    "            'MAE': mae,\n",
    "            'WMAE': wmae,\n",
    "            'IsHoliday': val_data['IsHoliday'].values,\n",
    "            'UsedFallback': True,\n",
    "            'FallbackType': fallback_type\n",
    "        })\n",
    "\n",
    "        log_data['fallback_used'] += 1\n",
    "        print(f\"  Fallback Validation MAE: {mae:.2f}, WMAE: {wmae:.2f}\")\n",
    "    '''\n",
    "    return models, model_global, val_results, log_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "135wD7tbkgb4NaHw6A97insntGce33Zvf"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4181211,
     "status": "ok",
     "timestamp": 1753976234128,
     "user": {
      "displayName": "ZipperNator",
      "userId": "13572371089708989227"
     },
     "user_tz": -240
    },
    "id": "5z-71ZHSmuCq",
    "outputId": "1e5f6ef7-7e99-4565-ee41-7781a9e36a3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models, model_global, val_results, log_data = train_models(X_train_processed, X_val_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1753976668678,
     "user": {
      "displayName": "ZipperNator",
      "userId": "13572371089708989227"
     },
     "user_tz": -240
    },
    "id": "hZyAh-gmAwwt",
    "outputId": "b3aa4e54-d31f-48a9-fd2d-f59f3ca193f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Validation Metrics:\n",
      "- Weighted MAE (WMAE): 3040.82\n",
      "- Average MAE: 2967.36\n",
      "- Average WMAE: 3043.97\n",
      "- Coverage: 99.72%\n",
      "- Fallback Usage: 0.00%\n",
      "\n",
      "Saved validation results to validation_results.csv\n",
      "\n",
      "Saved training logs to training_log.json\n",
      "\n",
      "Training and validation complete!\n",
      "- Models trained: 3152\n",
      "- Models skipped: 165\n",
      "- Fallbacks used: 0\n",
      "- Validation coverage: 99.72%\n"
     ]
    }
   ],
   "source": [
    "if val_results:\n",
    "    all_rows = []\n",
    "    for x in val_results:\n",
    "        dates = np.array(x['Date'])\n",
    "        actuals = np.array(x['Actual'])\n",
    "        preds = np.array(x['Predicted'])\n",
    "        is_holidays = np.array(x['IsHoliday'])\n",
    "\n",
    "        min_length = min(len(dates), len(actuals), len(preds), len(is_holidays))\n",
    "\n",
    "        for i in range(min_length):\n",
    "            all_rows.append({\n",
    "                'Store': x['Store'],\n",
    "                'Dept': x['Dept'],\n",
    "                'Date': dates[i],\n",
    "                'Actual': float(actuals[i]),\n",
    "                'Predicted': float(preds[i]),\n",
    "                'IsHoliday': bool(is_holidays[i]),\n",
    "                'UsedFallback': x.get('UsedFallback', False),\n",
    "                'FallbackType': x.get('FallbackType', None)\n",
    "            })\n",
    "\n",
    "    all_val = pd.DataFrame(all_rows)\n",
    "\n",
    "    all_val['Weight'] = np.where(all_val['IsHoliday'], 5, 1)\n",
    "\n",
    "    try:\n",
    "        overall_wmae = weighted_mean_absolute_error(\n",
    "            all_val['Actual'].astype(float),\n",
    "            all_val['Predicted'].astype(float),\n",
    "            all_val['Weight'].astype(float))\n",
    "\n",
    "        avg_mae = np.mean([float(x['MAE']) for x in val_results])\n",
    "        avg_wmae = np.mean([float(x['WMAE']) for x in val_results])\n",
    "\n",
    "        log_data['validation_metrics'] = {\n",
    "            'overall_wmae': float(overall_wmae),\n",
    "            'average_mae': float(avg_mae),\n",
    "            'average_wmae': float(avg_wmae),\n",
    "            'num_validated': int(len(all_val)),\n",
    "            'coverage': float(len(all_val) / len(X_val_processed)),\n",
    "            'fallback_usage': float(len(all_val[all_val['UsedFallback']]) / len(all_val))\n",
    "        }\n",
    "\n",
    "        print(\"\\nOverall Validation Metrics:\")\n",
    "        print(f\"- Weighted MAE (WMAE): {overall_wmae:.2f}\")\n",
    "        print(f\"- Average MAE: {avg_mae:.2f}\")\n",
    "        print(f\"- Average WMAE: {avg_wmae:.2f}\")\n",
    "        print(f\"- Coverage: {len(all_val)/len(X_val_processed):.2%}\")\n",
    "        print(f\"- Fallback Usage: {len(all_val[all_val['UsedFallback']])/len(all_val):.2%}\")\n",
    "\n",
    "        all_val.to_csv('validation_results.csv', index=False)\n",
    "        print(\"\\nSaved validation results to validation_results.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics: {str(e)}\")\n",
    "        log_data['validation_metrics'] = {\n",
    "            'error': str(e),\n",
    "            'num_validated': len(all_val)\n",
    "        }\n",
    "\n",
    "try:\n",
    "    with open('training_log.json', 'w') as f:\n",
    "        json.dump(log_data, f, indent=2, default=str)\n",
    "    print(\"\\nSaved training logs to training_log.json\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving logs: {str(e)}\")\n",
    "\n",
    "print(\"\\nTraining and validation complete!\")\n",
    "print(f\"- Models trained: {log_data.get('models_trained', 'N/A')}\")\n",
    "print(f\"- Models skipped: {log_data.get('models_skipped', 'N/A')}\")\n",
    "print(f\"- Fallbacks used: {log_data.get('fallback_used', 'N/A')}\")\n",
    "print(f\"- Validation coverage: {len(all_val)/len(X_val_processed):.2%}\" if 'all_val' in locals() else \"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 13297,
     "status": "ok",
     "timestamp": 1753971532024,
     "user": {
      "displayName": "ZipperNator",
      "userId": "13572371089708989227"
     },
     "user_tz": -240
    },
    "id": "VGI8vYgqsIOy",
    "outputId": "7d34f223-dcc9-4b65-bd16-4ef35bc4a1ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Downloading mlflow-3.1.4-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting mlflow-skinny==3.1.4 (from mlflow)\n",
      "  Downloading mlflow_skinny-3.1.4-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
      "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting gunicorn<24 (from mlflow)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.2)\n",
      "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
      "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
      "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.16.0)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (8.2.1)\n",
      "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (3.1.1)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.1.4->mlflow)\n",
      "  Downloading databricks_sdk-0.61.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.116.1)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (3.1.45)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (8.7.0)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.1.4->mlflow)\n",
      "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.1.4->mlflow)\n",
      "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (25.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (2.11.7)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (2.32.3)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (4.14.1)\n",
      "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.35.0)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
      "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (2.38.0)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.4->mlflow) (0.47.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.4->mlflow) (4.0.12)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.4->mlflow) (3.23.0)\n",
      "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.4->mlflow)\n",
      "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.4->mlflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.4->mlflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.4->mlflow) (2025.7.14)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==3.1.4->mlflow) (0.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.4->mlflow) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (4.9.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (4.9.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.6.1)\n",
      "Downloading mlflow-3.1.4-py3-none-any.whl (24.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_skinny-3.1.4-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading databricks_sdk-0.61.0-py3-none-any.whl (680 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m680.6/680.6 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gunicorn, graphql-core, opentelemetry-api, graphql-relay, docker, alembic, opentelemetry-semantic-conventions, graphene, databricks-sdk, opentelemetry-sdk, mlflow-skinny, mlflow\n",
      "Successfully installed alembic-1.16.4 databricks-sdk-0.61.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-3.1.4 mlflow-skinny-3.1.4 opentelemetry-api-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11694,
     "status": "ok",
     "timestamp": 1753971577969,
     "user": {
      "displayName": "ZipperNator",
      "userId": "13572371089708989227"
     },
     "user_tz": -240
    },
    "id": "J8bONcNf2gx0",
    "outputId": "445a3ca8-4897-4e56-9e0c-9a99f08490be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dagshub\n",
      "  Downloading dagshub-0.6.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: PyYAML>=5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (6.0.2)\n",
      "Collecting appdirs>=1.4.4 (from dagshub)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: click>=8.0.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.2.1)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.28.1)\n",
      "Requirement already satisfied: GitPython>=3.1.29 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.1.45)\n",
      "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (13.9.4)\n",
      "Collecting dacite~=1.6.0 (from dagshub)\n",
      "  Downloading dacite-1.6.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.5.0)\n",
      "Collecting gql[requests] (from dagshub)\n",
      "  Downloading gql-3.5.3-py2.py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting dataclasses-json (from dagshub)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.2.2)\n",
      "Collecting treelib>=1.6.4 (from dagshub)\n",
      "  Downloading treelib-1.8.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pathvalidate>=3.0.0 (from dagshub)\n",
      "  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.9.0.post0)\n",
      "Collecting boto3 (from dagshub)\n",
      "  Downloading boto3-1.39.17-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting semver (from dagshub)\n",
      "  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting dagshub-annotation-converter>=0.1.5 (from dagshub)\n",
      "  Downloading dagshub_annotation_converter-0.1.11-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (5.4.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (11.3.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (4.14.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython>=3.1.29->dagshub) (4.0.12)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (2.19.2)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from treelib>=1.6.4->dagshub) (1.17.0)\n",
      "Collecting botocore<1.40.0,>=1.39.17 (from boto3->dagshub)\n",
      "  Downloading botocore-1.39.17-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->dagshub)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3->dagshub)\n",
      "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->dagshub)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->dagshub)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: graphql-core<3.2.7,>=3.2 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (3.2.6)\n",
      "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.20.1)\n",
      "Collecting backoff<3.0,>=1.11.1 (from gql[requests]->dagshub)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: requests<3,>=2.26 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.40.0,>=1.39.17->boto3->dagshub) (2.5.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.29->dagshub) (5.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->dagshub) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.26->gql[requests]->dagshub) (3.4.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.2)\n",
      "Downloading dagshub-0.6.2-py3-none-any.whl (261 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m261.2/261.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading dacite-1.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading dagshub_annotation_converter-0.1.11-py3-none-any.whl (35 kB)\n",
      "Downloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\n",
      "Downloading treelib-1.8.0-py3-none-any.whl (30 kB)\n",
      "Downloading boto3-1.39.17-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading semver-3.0.4-py3-none-any.whl (17 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading botocore-1.39.17-py3-none-any.whl (13.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading gql-3.5.3-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: appdirs, treelib, semver, pathvalidate, mypy-extensions, marshmallow, jmespath, dacite, backoff, typing-inspect, gql, botocore, s3transfer, dataclasses-json, dagshub-annotation-converter, boto3, dagshub\n",
      "Successfully installed appdirs-1.4.4 backoff-2.2.1 boto3-1.39.17 botocore-1.39.17 dacite-1.6.0 dagshub-0.6.2 dagshub-annotation-converter-0.1.11 dataclasses-json-0.6.7 gql-3.5.3 jmespath-1.0.1 marshmallow-3.26.1 mypy-extensions-1.1.0 pathvalidate-3.3.1 s3transfer-0.13.1 semver-3.0.4 treelib-1.8.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install dagshub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8149,
     "status": "ok",
     "timestamp": 1753976703561,
     "user": {
      "displayName": "ZipperNator",
      "userId": "13572371089708989227"
     },
     "user_tz": -240
    },
    "id": "ntKQddoE28ZV",
    "outputId": "e4a01509-d827-4269-d8ba-e7e85b8935a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MLflow logging complete: Run ID = 755047df06bb4a049f6682f2c59078c2\n",
      " View run Prophet_Model_wo_Fallback at: https://dagshub.com/goguaD/finalProjectML.mlflow/#/experiments/0/runs/755047df06bb4a049f6682f2c59078c2\n",
      " View experiment at: https://dagshub.com/goguaD/finalProjectML.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"goguaD\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"685c4f5b2a0c555f9136c60a8666661d952de9be\"\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/goguaD/finalProjectML.mlflow\")\n",
    "mlflow.set_experiment(\"walmart-sales\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"Prophet_Model_wo_Fallback\") as run:\n",
    "    mlflow.log_param(\"total_combinations\", log_data.get('total_combinations'))\n",
    "    mlflow.log_param(\"models_trained\", log_data.get('models_trained'))\n",
    "    mlflow.log_param(\"models_skipped\", log_data.get('models_skipped'))\n",
    "\n",
    "    metrics = log_data.get('validation_metrics', {})\n",
    "    if metrics:\n",
    "        mlflow.log_metric(\"overall_wmae\", metrics.get(\"overall_wmae\", 0.0))\n",
    "        mlflow.log_metric(\"average_mae\", metrics.get(\"average_mae\", 0.0))\n",
    "        mlflow.log_metric(\"average_wmae\", metrics.get(\"average_wmae\", 0.0))\n",
    "        mlflow.log_metric(\"validation_coverage\", metrics.get(\"coverage\", 0.0))\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"\\n MLflow logging complete: Run ID = {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElwXllxX4qAv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
