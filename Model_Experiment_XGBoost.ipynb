{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 3816,
          "databundleVersionId": 32105,
          "sourceType": "competition"
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Model_Experiment_XGBoost",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": []
        },
        "id": "XJByXrgaXy9Q",
        "outputId": "5d0d6623-57a1-4992-8e27-2b98a14d64c2"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggleâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19d6d4d6ec884318bfbae6780dcea161"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle credentials set.\n",
            "Kaggle credentials successfully validated.\n"
          ]
        }
      ],
      "execution_count": 51
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "walmart_recruiting_store_sales_forecasting_path = kagglehub.competition_download('walmart-recruiting-store-sales-forecasting')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihwCWOvdXy9R",
        "outputId": "d7926886-b4e4-4f3c-f236-403499e2b7aa"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 52
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "_KcTPTbuXy9S"
      },
      "outputs": [],
      "execution_count": 53
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load data\n",
        "path = '/kaggle/input/walmart-recruiting-store-sales-forecasting/'\n",
        "df_train = pd.read_csv(f'{path}train.csv', parse_dates=['Date'])\n",
        "df_test = pd.read_csv(f'{path}test.csv', parse_dates=['Date'])\n",
        "df_features = pd.read_csv(f'{path}features.csv', parse_dates=['Date'])\n",
        "df_stores = pd.read_csv(f'{path}stores.csv')\n",
        "\n",
        "# Merge dataframes\n",
        "df = df_train.merge(df_stores, on='Store', how='left')\n",
        "df = df.merge(df_features, on=['Store', 'Date'], how='left')\n",
        "\n",
        "# Handle missing values (example: fill with 0 or interpolate)\n",
        "# For simplicity, let's fill NaNs in numerical columns with 0 for now.\n",
        "# In a real project, you'd use more sophisticated imputation.\n",
        "for col in ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']:\n",
        "    df[col] = df[col].fillna(0) # Or df[col].fillna(df[col].mean())\n",
        "\n",
        "# Convert IsHoliday to numerical\n",
        "df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "\n",
        "# Handle negative Weekly_Sales values (common in this dataset)\n",
        "df['Weekly_Sales'] = df['Weekly_Sales'].apply(lambda x: max(0, x)) # Set negative sales to 0\n",
        "\n",
        "# Feature Engineering (as per previous instructions)\n",
        "def create_features(df):\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Week'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "    # df['Is_Holiday'] is already handled by df['IsHoliday'].astype(int) above\n",
        "    return df\n",
        "\n",
        "df = create_features(df)\n",
        "\n",
        "# Define features and target\n",
        "features = [\n",
        "    'Store', 'Dept', 'Year', 'Month', 'Week', 'Day', 'DayOfWeek', 'DayOfYear',\n",
        "    'IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "    'Size', 'Type', # Add 'Size' and 'Type' from stores.csv\n",
        "    'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5' # MarkDown features\n",
        "]\n",
        "target = 'Weekly_Sales'\n",
        "\n",
        "# Convert 'Type' to numerical using OneHotEncoder or LabelEncoder\n",
        "df = pd.get_dummies(df, columns=['Type'], prefix='Type', drop_first=True)\n",
        "features.remove('Type') # Remove original 'Type' from features list\n",
        "features.extend([col for col in df.columns if 'Type_' in col]) # Add new one-hot encoded columns\n",
        "\n",
        "# Filter features to ensure they exist after all preprocessing\n",
        "final_features = [f for f in features if f in df.columns]\n",
        "print(f\"Final features used for training: {final_features}\")\n",
        "\n",
        "# Split data into training and validation sets (time-series split)\n",
        "df = df.sort_values('Date')\n",
        "split_date = pd.to_datetime('2011-12-31') # Example split date, adjust as needed\n",
        "\n",
        "X_train = df[df['Date'] <= split_date][final_features]\n",
        "y_train = df[df['Date'] <= split_date][target]\n",
        "X_valid = df[df['Date'] > split_date][final_features]\n",
        "y_valid = df[df['Date'] > split_date][target]\n",
        "\n",
        "print(f\"Train set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_valid)}\")\n",
        "\n",
        "# Check for missing columns in X_train/X_valid after splitting and one-hot encoding\n",
        "missing_in_valid = set(X_train.columns) - set(X_valid.columns)\n",
        "missing_in_train = set(X_valid.columns) - set(X_train.columns)\n",
        "if missing_in_valid or missing_in_train:\n",
        "    print(\"Warning: Mismatch in columns between train and validation sets!\")\n",
        "    print(f\"Missing in valid: {missing_in_valid}\")\n",
        "    print(f\"Missing in train: {missing_in_train}\")\n",
        "    # Align columns to ensure both have the same set\n",
        "    common_cols = list(set(X_train.columns) & set(X_valid.columns))\n",
        "    X_train = X_train[common_cols]\n",
        "    X_valid = X_valid[common_cols]\n",
        "    print(\"Columns aligned for train and validation sets.\")\n",
        "\n",
        "\n",
        "USE_SMALL = True\n",
        "\n",
        "if USE_SMALL:\n",
        "    df_work = df.query(\"Store == 1 and Dept == 1\").copy()\n",
        "else:\n",
        "    df_work = df.copy()\n",
        "\n",
        "print(f\"ave: {df_work['Weekly_Sales'].mean():.2f}\")\n",
        "print(f\"stdev: {df_work['Weekly_Sales'].std():.2f}\")\n",
        "print(f\"min: {df_work['Weekly_Sales'].min():.2f}\")\n",
        "print(f\"max: {df_work['Weekly_Sales'].max():.2f}\")\n"
      ],
      "metadata": {
        "id": "mkEh8-0zYVdx"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final features used for training: ['Store', 'Dept', 'Year', 'Month', 'Week', 'Day', 'DayOfWeek', 'DayOfYear', 'IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Type_B', 'Type_C']\n",
            "Train set size: 382439\n",
            "Validation set size: 48946\n",
            "ave: 17968.43\n",
            "stdev: 24524.41\n",
            "min: -370.12\n",
            "max: 630999.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ngrok\n",
        "\n",
        "ngrok.set_auth_token(\"2g3h4j5k6l7m8n9o0p1q2r3s4t5u6v7w\") # Replace with your actual ngrok token\n",
        "\n",
        "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
        "\n",
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\")\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\")\n",
        "print(f\"MLflow UI: {ngrok_tunnel.public_url} -> {ngrok_tunnel.addr}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvFc0_P7bax5",
        "outputId": "dd226a00-2791-4b0b-c3d8-182c7ac16dee"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok  MLflow UI\n",
            "MLflow UI: NgrokTunnel: \"https://1bd1-34-61-217-101.ngrok-free.app\" -> \"http://localhost:5000\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"ave: {df_work['Weekly_Sales'].mean():.2f}\")\n",
        "print(f\"stdev: {df_work['Weekly_Sales'].std():.2f}\")\n",
        "print(f\"min: {df_work['Weekly_Sales'].min():.2f}\")\n",
        "print(f\"max: {df_work['Weekly_Sales'].max():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkEh8-0zYVdx",
        "outputId": "dd226a00-2791-4b0b-c3d8-182c7ac16dee"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave: 17968.43\n",
            "stdev: 24524.41\n",
            "min: -370.12\n",
            "max: 630999.19\n"
          ]
        }
      ]
    }
  ]
}
