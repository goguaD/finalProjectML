{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4bf7b4ef-d889-411d-8f7c-10e83df73ab1",
      "metadata": {
        "id": "4bf7b4ef-d889-411d-8f7c-10e83df73ab1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
        "from xgboost import XGBRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import scipy.stats as stats\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "xsyaKzcyEeN9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "00b4de364dfa46b69c281636f3dc3df1",
            "da27840483f94509808db615bedf01be",
            "1ce4de57a80e4d5b809dce94110c9c44",
            "b14a3b47865046cdadb51117e93a878f",
            "3fb3e72990d9458c9bf7486368ef4673",
            "2b894296f7e649de882728081268e823",
            "93573d2ea25f4f92a4ff081683305f28",
            "e80cfc72cf414a8c82aebae4a0183bef",
            "c27f16f8db764c44bc8f0c52ec1da3e9",
            "57f2f219d0814c72956995414e4363bb",
            "a8168c03578a4d7f8d412742af111253",
            "67541f2010e7495c9873db2a3e74a475",
            "b6d32b9c0c9d45918eac3d2bd2380b3c",
            "4a0ee787cc0c407cbf671eaa9d53d303",
            "59831f17489f4807a0156d2ffcc9cd77",
            "0ba84137a751482da2478318c7ad8483",
            "f197d0ecd27743068a562bd702643ebf",
            "48a129a90f744f88a614fe685c58943c",
            "9a0ba0d122b342aea64cb4f66c595a29",
            "02df3688ada840f7bfe54618fcb60bb4",
            "af0069c059544f538cd010d00facb306",
            "fe375d546874456e8ae18de38ba027e4",
            "5276b7fba64f42e8ba9336608a56ccaa"
          ]
        },
        "id": "xsyaKzcyEeN9",
        "outputId": "4fd36b46-d82e-4f0e-a0c0-7dd4c188cb35"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00b4de364dfa46b69c281636f3dc3df1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle credentials set.\n",
            "Kaggle credentials successfully validated.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2eCsoj83EhIa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eCsoj83EhIa",
        "outputId": "5f67f4a2-ab8d-4bcc-ba19-41a417c27926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/competitions/data/download-all/walmart-recruiting-store-sales-forecasting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.70M/2.70M [00:00<00:00, 147MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Data source import complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "walmart_recruiting_store_sales_forecasting_path = kagglehub.competition_download('walmart-recruiting-store-sales-forecasting')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4-rrOS1rEjmt",
      "metadata": {
        "id": "4-rrOS1rEjmt"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "NJIjgw7FEuL0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJIjgw7FEuL0",
        "outputId": "f48f67e9-c606-45f5-afd4-20da2849c150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "_kpU61yFE2j-",
      "metadata": {
        "id": "_kpU61yFE2j-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c8a6636-270c-480e-964a-a43d6de85682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 911MB/s]\n",
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ],
      "source": [
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ML/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "! unzip walmart-recruiting-store-sales-forecasting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "JrXJTS29FDHZ",
      "metadata": {
        "id": "JrXJTS29FDHZ"
      },
      "outputs": [],
      "source": [
        "stores = pd.read_csv('stores.csv')\n",
        "train = pd.read_csv(\"train.csv.zip\")\n",
        "features = pd.read_csv('features.csv.zip')\n",
        "sample = pd.read_csv('sampleSubmission.csv.zip')\n",
        "test = pd.read_csv('test.csv.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "pNzCDRbzFSPt",
      "metadata": {
        "id": "pNzCDRbzFSPt"
      },
      "outputs": [],
      "source": [
        "train_ = pd.merge(train, features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "test_df = pd.merge(test, features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "train_ = pd.merge(train_, stores, on='Store', how='left')\n",
        "test_df = pd.merge(test_df, stores, on='Store', how='left')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "C8G1BuBCFd__",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8G1BuBCFd__",
        "outputId": "885314d1-f099-4735-f39a-e57f6bfb8966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Merged Train Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 421570 entries, 0 to 421569\n",
            "Data columns (total 16 columns):\n",
            " #   Column        Non-Null Count   Dtype  \n",
            "---  ------        --------------   -----  \n",
            " 0   Store         421570 non-null  int64  \n",
            " 1   Dept          421570 non-null  int64  \n",
            " 2   Date          421570 non-null  object \n",
            " 3   Weekly_Sales  421570 non-null  float64\n",
            " 4   IsHoliday     421570 non-null  bool   \n",
            " 5   Temperature   421570 non-null  float64\n",
            " 6   Fuel_Price    421570 non-null  float64\n",
            " 7   MarkDown1     150681 non-null  float64\n",
            " 8   MarkDown2     111248 non-null  float64\n",
            " 9   MarkDown3     137091 non-null  float64\n",
            " 10  MarkDown4     134967 non-null  float64\n",
            " 11  MarkDown5     151432 non-null  float64\n",
            " 12  CPI           421570 non-null  float64\n",
            " 13  Unemployment  421570 non-null  float64\n",
            " 14  Type          421570 non-null  object \n",
            " 15  Size          421570 non-null  int64  \n",
            "dtypes: bool(1), float64(10), int64(3), object(2)\n",
            "memory usage: 48.6+ MB\n",
            "None\n",
            "\n",
            "--- Merged Test Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 115064 entries, 0 to 115063\n",
            "Data columns (total 15 columns):\n",
            " #   Column        Non-Null Count   Dtype  \n",
            "---  ------        --------------   -----  \n",
            " 0   Store         115064 non-null  int64  \n",
            " 1   Dept          115064 non-null  int64  \n",
            " 2   Date          115064 non-null  object \n",
            " 3   IsHoliday     115064 non-null  bool   \n",
            " 4   Temperature   115064 non-null  float64\n",
            " 5   Fuel_Price    115064 non-null  float64\n",
            " 6   MarkDown1     114915 non-null  float64\n",
            " 7   MarkDown2     86437 non-null   float64\n",
            " 8   MarkDown3     105235 non-null  float64\n",
            " 9   MarkDown4     102176 non-null  float64\n",
            " 10  MarkDown5     115064 non-null  float64\n",
            " 11  CPI           76902 non-null   float64\n",
            " 12  Unemployment  76902 non-null   float64\n",
            " 13  Type          115064 non-null  object \n",
            " 14  Size          115064 non-null  int64  \n",
            "dtypes: bool(1), float64(9), int64(3), object(2)\n",
            "memory usage: 12.4+ MB\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "print(\"\\n--- Merged Train Data Info ---\")\n",
        "print(train_.info())\n",
        "print(\"\\n--- Merged Test Data Info ---\")\n",
        "print(test_df.info())\n",
        "\n",
        "del train, test, features, stores\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MissingMarkdownHandler:\n",
        "    \"\"\"\n",
        "    Handles missing markdown values by replacing NaNs with 0s and creating\n",
        "    boolean flags to distinguish between actual 0s and missing information.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.markdown_cols = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Identify markdown columns\n",
        "        self.markdown_cols = [col for col in X.columns if 'markdown' in col.lower()]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        for col in self.markdown_cols:\n",
        "            if col in X_copy.columns:\n",
        "                # Create boolean flag for missing information\n",
        "                X_copy[f'{col}_was_missing'] = X_copy[col].isna().astype(int)\n",
        "\n",
        "                # Replace NaNs with 0s (better than median for independence assumption)\n",
        "                X_copy[col] = X_copy[col].fillna(0)\n",
        "\n",
        "        return X_copy\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.fit(X).transform(X)\n",
        "\n"
      ],
      "metadata": {
        "id": "QHCWFckreVhz"
      },
      "id": "QHCWFckreVhz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b19a3c3d",
      "metadata": {
        "id": "b19a3c3d"
      },
      "outputs": [],
      "source": [
        "class MissingValueImputer:\n",
        "    \"\"\"\n",
        "    Handles missing values in non-markdown features using forward-fill and backward-fill\n",
        "    strategy, which is more relevant for time-series data than mean/median.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.non_markdown_cols = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Identify non-markdown columns that might have missing values\n",
        "        self.non_markdown_cols = [col for col in X.columns\n",
        "                                 if 'markdown' not in col.lower() and X[col].dtype in ['float64', 'int64']]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        # Sort by date for proper forward/backward fill\n",
        "        if 'Date' in X_copy.columns:\n",
        "            X_copy = X_copy.sort_values('Date')\n",
        "\n",
        "        for col in self.non_markdown_cols:\n",
        "            if col in X_copy.columns and X_copy[col].isna().any():\n",
        "                # Forward fill first (use previous week's information)\n",
        "                X_copy[col] = X_copy[col].fillna(method='ffill')\n",
        "                # Backward fill for any remaining NaNs\n",
        "                X_copy[col] = X_copy[col].fillna(method='bfill')\n",
        "\n",
        "        return X_copy\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.fit(X).transform(X)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DateFeatureExtractor:\n",
        "    \"\"\"\n",
        "    Extracts comprehensive date features including seasonal patterns,\n",
        "    cyclic encoding, and holiday proximity features.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.holidays = {\n",
        "            'Christmas': '12-25',\n",
        "            'Thanksgiving': '11-22',\n",
        "            'New_Year': '01-01',\n",
        "            'Independence_Day': '07-04',\n",
        "            'Labor_Day': '09-01',\n",
        "        }\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        # Ensure Date column is datetime\n",
        "        if 'Date' in X_copy.columns:\n",
        "            X_copy['Date'] = pd.to_datetime(X_copy['Date'])\n",
        "\n",
        "            # Basic date features\n",
        "            X_copy['Year'] = X_copy['Date'].dt.year\n",
        "            X_copy['Month'] = X_copy['Date'].dt.month\n",
        "            X_copy['Week'] = X_copy['Date'].dt.isocalendar().week\n",
        "            X_copy['Day'] = X_copy['Date'].dt.day\n",
        "            X_copy['DayOfWeek'] = X_copy['Date'].dt.dayofweek\n",
        "\n",
        "            # Cyclic encoding for seasonal patterns\n",
        "            X_copy['Month_sin'] = np.sin(2 * np.pi * X_copy['Month'] / 12)\n",
        "            X_copy['Month_cos'] = np.cos(2 * np.pi * X_copy['Month'] / 12)\n",
        "            X_copy['Week_sin'] = np.sin(2 * np.pi * X_copy['Week'] / 52)\n",
        "            X_copy['Week_cos'] = np.cos(2 * np.pi * X_copy['Week'] / 52)\n",
        "\n",
        "            # Holiday proximity features\n",
        "            for holiday, date_str in self.holidays.items():\n",
        "                month, day = map(int, date_str.split('-'))\n",
        "                holiday_date = pd.to_datetime(f\"{X_copy['Year'].iloc[0]}-{month:02d}-{day:02d}\")\n",
        "\n",
        "                days_until = (holiday_date - X_copy['Date']).dt.days\n",
        "                X_copy[f'days_until_{holiday.lower()}'] = days_until\n",
        "                X_copy[f'{holiday.lower()}_week'] = (np.abs(days_until) <= 7).astype(int)\n",
        "\n",
        "        return X_copy\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.fit(X).transform(X)\n",
        "\n"
      ],
      "metadata": {
        "id": "Z7c7fyGmenkt"
      },
      "id": "Z7c7fyGmenkt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedDateFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Enhanced date feature extractor with holiday detection and seasonal patterns\n",
        "    \"\"\"\n",
        "    def __init__(self, date_column: str = 'Date', include_holidays: bool = True,\n",
        "                 include_seasonal: bool = True, include_lags: bool = False):\n",
        "        self.date_column = date_column\n",
        "        self.include_holidays = include_holidays\n",
        "        self.include_seasonal = include_seasonal\n",
        "        self.include_lags = include_lags\n",
        "\n",
        "    def _is_holiday_period(self, date):\n",
        "        \"\"\"Detect major US retail holiday periods\"\"\"\n",
        "        month, day = date.month, date.day\n",
        "\n",
        "        # Black Friday week (last week of November)\n",
        "        if month == 11 and day >= 22:\n",
        "            return 1\n",
        "        # Christmas season (December)\n",
        "        elif month == 12:\n",
        "            return 1\n",
        "        # New Year\n",
        "        elif month == 1 and day <= 7:\n",
        "            return 1\n",
        "        # Labor Day (first Monday in September) - approximate\n",
        "        elif month == 9 and day <= 7:\n",
        "            return 1\n",
        "        # Memorial Day (last Monday in May) - approximate\n",
        "        elif month == 5 and day >= 25:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        if self.date_column not in X_copy.columns:\n",
        "            raise ValueError(f\"Date column '{self.date_column}' not found in DataFrame.\")\n",
        "\n",
        "        # Ensure datetime format\n",
        "        X_copy[self.date_column] = pd.to_datetime(X_copy[self.date_column])\n",
        "\n",
        "        # Basic temporal features\n",
        "        X_copy['Year'] = X_copy[self.date_column].dt.year\n",
        "        X_copy['Month'] = X_copy[self.date_column].dt.month\n",
        "        X_copy['Day'] = X_copy[self.date_column].dt.day\n",
        "        X_copy['DayOfWeek'] = X_copy[self.date_column].dt.dayofweek\n",
        "        X_copy['Week'] = X_copy[self.date_column].dt.isocalendar().week.astype(int)\n",
        "        X_copy['Quarter'] = X_copy[self.date_column].dt.quarter\n",
        "        X_copy['DayOfYear'] = X_copy[self.date_column].dt.dayofyear\n",
        "\n",
        "        # Cyclical encoding\n",
        "        X_copy['Month_sin'] = np.sin(2 * np.pi * X_copy['Month'] / 12)\n",
        "        X_copy['Month_cos'] = np.cos(2 * np.pi * X_copy['Month'] / 12)\n",
        "        X_copy['Week_sin'] = np.sin(2 * np.pi * X_copy['Week'] / 52)\n",
        "        X_copy['Week_cos'] = np.cos(2 * np.pi * X_copy['Week'] / 52)\n",
        "        X_copy['DayOfWeek_sin'] = np.sin(2 * np.pi * X_copy['DayOfWeek'] / 7)\n",
        "        X_copy['DayOfWeek_cos'] = np.cos(2 * np.pi * X_copy['DayOfWeek'] / 7)\n",
        "\n",
        "        # Seasonal features\n",
        "        if self.include_seasonal:\n",
        "            X_copy['Season'] = X_copy['Month'].map({12: 0, 1: 0, 2: 0,  # Winter\n",
        "                                                   3: 1, 4: 1, 5: 1,    # Spring\n",
        "                                                   6: 2, 7: 2, 8: 2,    # Summer\n",
        "                                                   9: 3, 10: 3, 11: 3}) # Fall\n",
        "\n",
        "            # Weekend indicator\n",
        "            X_copy['IsWeekend'] = (X_copy['DayOfWeek'] >= 5).astype(int)\n",
        "\n",
        "            # Month-end indicator\n",
        "            X_copy['IsMonthEnd'] = (X_copy[self.date_column].dt.is_month_end).astype(int)\n",
        "            X_copy['IsMonthStart'] = (X_copy[self.date_column].dt.is_month_start).astype(int)\n",
        "\n",
        "        # Holiday features\n",
        "        if self.include_holidays:\n",
        "            X_copy['IsHolidayPeriod'] = X_copy[self.date_column].apply(self._is_holiday_period)\n",
        "\n",
        "            # Convert existing IsHoliday to int if it exists\n",
        "            if 'IsHoliday' in X_copy.columns:\n",
        "                if X_copy['IsHoliday'].dtype == bool:\n",
        "                    X_copy['IsHoliday'] = X_copy['IsHoliday'].astype(int)\n",
        "\n",
        "        # Enhanced markdown features if markdown columns exist\n",
        "        markdown_cols = [col for col in X_copy.columns if col.startswith('MarkDown') and not col.endswith('_was_missing')]\n",
        "        if markdown_cols:\n",
        "            X_copy['Total_MarkDown'] = X_copy[markdown_cols].sum(axis=1)\n",
        "            X_copy['MarkDown_Intensity'] = X_copy['Total_MarkDown'] / (X_copy['Total_MarkDown'].mean() + 1e-8)\n",
        "            X_copy['HasMarkDown'] = (X_copy['Total_MarkDown'] > 0).astype(int)\n",
        "\n",
        "        # Economic features if available\n",
        "        econ_cols = ['Fuel_Price', 'CPI', 'Unemployment']\n",
        "        available_econ = [col for col in econ_cols if col in X_copy.columns]\n",
        "\n",
        "        if len(available_econ) >= 2:\n",
        "            if 'Fuel_Price' in X_copy.columns and 'CPI' in X_copy.columns:\n",
        "                X_copy['Fuel_CPI_Ratio'] = X_copy['Fuel_Price'] / (X_copy['CPI'] + 1e-8)\n",
        "\n",
        "            if 'CPI' in X_copy.columns and 'Unemployment' in X_copy.columns:\n",
        "                X_copy['Economic_Index'] = (X_copy['CPI'] * 0.4 + (100 - X_copy['Unemployment']) * 0.6) / 100\n",
        "\n",
        "        return X_copy\n"
      ],
      "metadata": {
        "id": "Gh9rckaugphK"
      },
      "id": "Gh9rckaugphK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MarkdownFeatureEngineer:\n",
        "    \"\"\"\n",
        "    Creates markdown-related features including total markdown and intensity.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.markdown_cols = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.markdown_cols = [col for col in X.columns if 'markdown' in col.lower()\n",
        "                             and col not in [c for c in X.columns if 'was_missing' in c]]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        if self.markdown_cols:\n",
        "            # Total markdown amount\n",
        "            X_copy['Total_MarkDown'] = X_copy[self.markdown_cols].sum(axis=1)\n",
        "\n",
        "            # Markdown intensity (normalized by store size if available)\n",
        "            if 'Size' in X_copy.columns:\n",
        "                X_copy['MarkDown_Intensity'] = X_copy['Total_MarkDown'] / (X_copy['Size'] + 1)\n",
        "            else:\n",
        "                # Normalize by mean markdown if Size not available\n",
        "                mean_markdown = X_copy['Total_MarkDown'].mean()\n",
        "                X_copy['MarkDown_Intensity'] = X_copy['Total_MarkDown'] / (mean_markdown + 1)\n",
        "\n",
        "        return X_copy\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.fit(X).transform(X)\n",
        "\n"
      ],
      "metadata": {
        "id": "lzaaqOdzerb6"
      },
      "id": "lzaaqOdzerb6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EconomicFeatureEngineer:\n",
        "    \"\"\"\n",
        "    Creates economic index by combining CPI and Unemployment into unified score.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        if 'CPI' in X_copy.columns and 'Unemployment' in X_copy.columns:\n",
        "            # Normalize both features to 0-1 scale for combination\n",
        "            cpi_norm = (X_copy['CPI'] - X_copy['CPI'].min()) / (X_copy['CPI'].max() - X_copy['CPI'].min())\n",
        "            unemployment_norm = (X_copy['Unemployment'] - X_copy['Unemployment'].min()) / (X_copy['Unemployment'].max() - X_copy['Unemployment'].min())\n",
        "\n",
        "            # Economic index (higher CPI and lower unemployment = better economic conditions)\n",
        "            X_copy['Economic_Index'] = cpi_norm - unemployment_norm\n",
        "\n",
        "        return X_copy\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.fit(X).transform(X)\n",
        "\n"
      ],
      "metadata": {
        "id": "y8xX0HOXeuqC"
      },
      "id": "y8xX0HOXeuqC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LagFeatureCreator:\n",
        "    \"\"\"\n",
        "    Creates lag features for each (Store, Dept) combination.\n",
        "    \"\"\"\n",
        "    def __init__(self, target_col='Weekly_Sales', lags=[1, 4, 52]):\n",
        "        self.target_col = target_col\n",
        "        self.lags = lags\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        if self.target_col in X_copy.columns and 'Store' in X_copy.columns and 'Dept' in X_copy.columns:\n",
        "            # Sort by Store, Dept, and Date for proper lag calculation\n",
        "            if 'Date' in X_copy.columns:\n",
        "                X_copy = X_copy.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "            for lag in self.lags:\n",
        "                lag_col_name = f'{self.target_col}_lag_{lag}'\n",
        "                X_copy[lag_col_name] = X_copy.groupby(['Store', 'Dept'])[self.target_col].shift(lag)\n",
        "\n",
        "        return X_copy\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.fit(X).transform(X)\n",
        "\n"
      ],
      "metadata": {
        "id": "ReGmNy9ReySL"
      },
      "id": "ReGmNy9ReySL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "class XGBoostLabelEncoder:\n",
        "    \"\"\"\n",
        "    Encodes categorical features using LabelEncoder for XGBoost compatibility.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.encoders = {}\n",
        "        self.categorical_cols = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Identify categorical columns\n",
        "        self.categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "        for col in self.categorical_cols:\n",
        "            if col != 'Date':  # Don't encode Date column\n",
        "                self.encoders[col] = LabelEncoder()\n",
        "                self.encoders[col].fit(X[col].astype(str))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        for col in self.categorical_cols:\n",
        "            if col in self.encoders and col in X_copy.columns and col != 'Date':\n",
        "                X_copy[col] = self.encoders[col].transform(X_copy[col].astype(str))\n",
        "\n",
        "        return X_copy\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.fit(X).transform(X)"
      ],
      "metadata": {
        "id": "PlEnorIme1PK"
      },
      "id": "PlEnorIme1PK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, List\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class SmartLabelEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Enhanced label encoder with better handling of unseen categories and validation sets\n",
        "    \"\"\"\n",
        "    def __init__(self, categorical_cols: Optional[List[str]] = None,\n",
        "                 handle_unknown: str = 'ignore', unknown_value: int = -1):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        - handle_unknown: 'ignore', 'error', or 'use_encoded_value'\n",
        "        - unknown_value: value to use for unknown categories\n",
        "        \"\"\"\n",
        "        self.categorical_cols = categorical_cols or ['Store', 'Dept', 'Type']\n",
        "        self.handle_unknown = handle_unknown\n",
        "        self.unknown_value = unknown_value\n",
        "        self.label_encoders = {}\n",
        "        self.category_mappings = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        for col in self.categorical_cols:\n",
        "            if col in X.columns:\n",
        "                # Create robust encoder\n",
        "                unique_vals = X[col].astype(str).unique()\n",
        "                unique_vals = unique_vals[pd.notna(unique_vals)]  # Remove NaN values\n",
        "\n",
        "                # Sort for consistent encoding\n",
        "                unique_vals = sorted(unique_vals)\n",
        "\n",
        "                # Create mapping\n",
        "                self.category_mappings[col] = {val: idx for idx, val in enumerate(unique_vals)}\n",
        "\n",
        "                # Add unknown category mapping if needed\n",
        "                if self.handle_unknown == 'use_encoded_value':\n",
        "                    max_idx = len(unique_vals)\n",
        "                    self.category_mappings[col]['__UNKNOWN__'] = max_idx\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        for col in self.categorical_cols:\n",
        "            if col in X_copy.columns and col in self.category_mappings:\n",
        "                # Convert to string and handle NaN\n",
        "                str_col = X_copy[col].astype(str)\n",
        "                str_col = str_col.replace('nan', '__UNKNOWN__')\n",
        "\n",
        "                # Map values\n",
        "                mapping = self.category_mappings[col]\n",
        "\n",
        "                if self.handle_unknown == 'ignore':\n",
        "                    # Map known values, keep unknown as is\n",
        "                    X_copy[col] = str_col.map(mapping).fillna(self.unknown_value)\n",
        "                elif self.handle_unknown == 'use_encoded_value':\n",
        "                    # Map all values, use special encoding for unknown\n",
        "                    X_copy[col] = str_col.apply(lambda x: mapping.get(x, mapping['__UNKNOWN__']))\n",
        "                else:  # error\n",
        "                    unknown_vals = set(str_col.unique()) - set(mapping.keys())\n",
        "                    if unknown_vals:\n",
        "                        raise ValueError(f\"Unknown categories found in column {col}: {unknown_vals}\")\n",
        "                    X_copy[col] = str_col.map(mapping)\n",
        "\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "paP0KOVqg046"
      },
      "id": "paP0KOVqg046",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wmae_score(y_true, y_pred, weights):\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n"
      ],
      "metadata": {
        "id": "Xe6QbD7ie4Kf"
      },
      "id": "Xe6QbD7ie4Kf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WMAEScorer:\n",
        "    \"\"\"\n",
        "    Custom scorer for WMAE metric compatible with sklearn's GridSearchCV.\n",
        "    \"\"\"\n",
        "    def __init__(self, weights_col='IsHoliday'):\n",
        "        self.weights_col = weights_col\n",
        "\n",
        "    def __call__(self, estimator, X, y):\n",
        "        y_pred = estimator.predict(X)\n",
        "\n",
        "        # Create weights (holiday weeks get 5x weight)\n",
        "        if isinstance(X, pd.DataFrame) and self.weights_col in X.columns:\n",
        "            weights = X[self.weights_col].apply(lambda x: 5 if x else 1)\n",
        "        else:\n",
        "            weights = np.ones(len(y))  # Equal weights if no holiday info\n",
        "\n",
        "        return -wmae_score(y, y_pred, weights)  # Negative for sklearn (higher is better)\n",
        "\n"
      ],
      "metadata": {
        "id": "MJSI3Pdve6Ws"
      },
      "id": "MJSI3Pdve6Ws",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreprocessingPipeline:\n",
        "    \"\"\"\n",
        "    Complete preprocessing pipeline that applies all transformations in sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.missing_markdown_handler = MissingMarkdownHandler()\n",
        "        self.missing_value_imputer = MissingValueImputer()\n",
        "        self.date_feature_extractor = DateFeatureExtractor()\n",
        "        self.markdown_feature_engineer = MarkdownFeatureEngineer()\n",
        "        self.economic_feature_engineer = EconomicFeatureEngineer()\n",
        "        self.lag_feature_creator = LagFeatureCreator()\n",
        "        self.label_encoder = XGBoostLabelEncoder()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Fit all transformers in sequence\n",
        "        X_temp = self.missing_markdown_handler.fit_transform(X)\n",
        "        X_temp = self.missing_value_imputer.fit_transform(X_temp)\n",
        "        X_temp = self.date_feature_extractor.fit_transform(X_temp)\n",
        "        X_temp = self.markdown_feature_engineer.fit_transform(X_temp)\n",
        "        X_temp = self.economic_feature_engineer.fit_transform(X_temp)\n",
        "        X_temp = self.lag_feature_creator.fit_transform(X_temp)\n",
        "        self.label_encoder.fit(X_temp)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_temp = self.missing_markdown_handler.transform(X)\n",
        "        X_temp = self.missing_value_imputer.transform(X_temp)\n",
        "        X_temp = self.date_feature_extractor.transform(X_temp)\n",
        "        X_temp = self.markdown_feature_engineer.transform(X_temp)\n",
        "        X_temp = self.economic_feature_engineer.transform(X_temp)\n",
        "        X_temp = self.lag_feature_creator.transform(X_temp)\n",
        "        X_temp = self.label_encoder.transform(X_temp)\n",
        "\n",
        "        return X_temp\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.fit(X, y).transform(X)\n",
        "\n"
      ],
      "metadata": {
        "id": "vWXd_58de8hJ"
      },
      "id": "vWXd_58de8hJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_validation_split(df, split_date='2012-09-01', date_col='Date'):\n",
        "    \"\"\"\n",
        "    Split data into train and validation sets based on date.\n",
        "\n",
        "    Parameters:\n",
        "    df: DataFrame with date column\n",
        "    split_date: Date to split on\n",
        "    date_col: Name of date column\n",
        "\n",
        "    Returns:\n",
        "    train_df, val_df\n",
        "    \"\"\"\n",
        "    df[date_col] = pd.to_datetime(df[date_col])\n",
        "    split_date = pd.to_datetime(split_date)\n",
        "\n",
        "    train_df = df[df[date_col] < split_date].copy()\n",
        "    val_df = df[df[date_col] >= split_date].copy()\n",
        "\n",
        "    print(f\"Train set: {len(train_df)} samples\")\n",
        "    print(f\"Validation set: {len(val_df)} samples\")\n",
        "    print(f\"Train date range: {train_df[date_col].min()} to {train_df[date_col].max()}\")\n",
        "    print(f\"Validation date range: {val_df[date_col].min()} to {val_df[date_col].max()}\")\n",
        "\n",
        "    return train_df, val_df\n",
        "\n"
      ],
      "metadata": {
        "id": "r_oSEyELe_b6"
      },
      "id": "r_oSEyELe_b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XGBoostTrainer:\n",
        "    \"\"\"\n",
        "    XGBoost model trainer with grid search and WMAE optimization.\n",
        "    \"\"\"\n",
        "    def __init__(self, random_state=42):\n",
        "        self.random_state = random_state\n",
        "        self.best_model = None\n",
        "        self.best_params = None\n",
        "        self.best_score = None\n",
        "\n",
        "    def grid_search_basic_params(self, X_train, y_train, weights_train=None):\n",
        "        \"\"\"\n",
        "        Grid search for basic XGBoost parameters: learning_rate and max_depth.\n",
        "        \"\"\"\n",
        "        print(\"Starting grid search for basic parameters...\")\n",
        "\n",
        "        # Parameter grid\n",
        "        param_grid = {\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'max_depth': [3, 6, 9],\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'random_state': [self.random_state]\n",
        "        }\n",
        "\n",
        "        # Base model\n",
        "        xgb_model = xgb.XGBRegressor(\n",
        "            objective='reg:squarederror',\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "\n",
        "        # Time series cross-validation\n",
        "        tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "        # Custom WMAE scorer\n",
        "        wmae_scorer = WMAEScorer()\n",
        "\n",
        "        # Grid search\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=xgb_model,\n",
        "            param_grid=param_grid,\n",
        "            cv=tscv,\n",
        "            scoring=wmae_scorer,\n",
        "            n_jobs=-1,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        self.best_params = grid_search.best_params_\n",
        "        self.best_score = -grid_search.best_score_  # Convert back to positive WMAE\n",
        "        self.best_model = grid_search.best_estimator_\n",
        "\n",
        "        print(f\"Best parameters: {self.best_params}\")\n",
        "        print(f\"Best WMAE score: {self.best_score:.4f}\")\n",
        "\n",
        "        return self.best_params, self.best_score\n",
        "\n",
        "    def train_with_custom_params(self, X_train, y_train, **custom_params):\n",
        "        \"\"\"\n",
        "        Train XGBoost with custom parameters.\n",
        "        \"\"\"\n",
        "        print(\"Training XGBoost with custom parameters...\")\n",
        "\n",
        "        # Default parameters\n",
        "        default_params = {\n",
        "            'objective': 'reg:squarederror',\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 6,\n",
        "            'n_estimators': 200,\n",
        "            'random_state': self.random_state\n",
        "        }\n",
        "\n",
        "        # Update with custom parameters\n",
        "        default_params.update(custom_params)\n",
        "\n",
        "        # Train model\n",
        "        self.best_model = xgb.XGBRegressor(**default_params)\n",
        "        self.best_model.fit(X_train, y_train)\n",
        "\n",
        "        print(f\"Model trained with parameters: {default_params}\")\n",
        "\n",
        "        return self.best_model\n",
        "\n",
        "    def evaluate_model(self, X_val, y_val, weights_val=None):\n",
        "        \"\"\"\n",
        "        Evaluate the trained model on validation set.\n",
        "        \"\"\"\n",
        "        if self.best_model is None:\n",
        "            raise ValueError(\"No model has been trained yet!\")\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = self.best_model.predict(X_val)\n",
        "\n",
        "        # Calculate WMAE\n",
        "        if weights_val is None:\n",
        "            # Create weights based on IsHoliday if available\n",
        "            if isinstance(X_val, pd.DataFrame) and 'IsHoliday' in X_val.columns:\n",
        "                weights_val = X_val['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
        "            else:\n",
        "                weights_val = np.ones(len(y_val))\n",
        "\n",
        "        wmae = wmae_score(y_val, y_pred, weights_val)\n",
        "        mae = mean_absolute_error(y_val, y_pred)\n",
        "\n",
        "        print(f\"Validation WMAE: {wmae:.4f}\")\n",
        "        print(f\"Validation MAE: {mae:.4f}\")\n",
        "\n",
        "        return wmae, mae, y_pred\n",
        "\n",
        "    def get_feature_importance(self, feature_names=None, top_n=20):\n",
        "        \"\"\"\n",
        "        Get feature importance from trained model.\n",
        "        \"\"\"\n",
        "        if self.best_model is None:\n",
        "            raise ValueError(\"No model has been trained yet!\")\n",
        "\n",
        "        importance = self.best_model.feature_importances_\n",
        "\n",
        "        if feature_names is not None:\n",
        "            importance_df = pd.DataFrame({\n",
        "                'feature': feature_names,\n",
        "                'importance': importance\n",
        "            }).sort_values('importance', ascending=False)\n",
        "\n",
        "            return importance_df.head(top_n)\n",
        "\n",
        "        return importance\n",
        "\n"
      ],
      "metadata": {
        "id": "pE6dDsO0fEiu"
      },
      "id": "pE6dDsO0fEiu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_training_pipeline(df, target_col='Weekly_Sales'):\n",
        "    \"\"\"\n",
        "    Complete training pipeline example.\n",
        "    \"\"\"\n",
        "    print(\"=== Walmart Sales Forecasting Pipeline ===\\n\")\n",
        "\n",
        "    # 1. Train-Validation Split\n",
        "    print(\"1. Creating train-validation split...\")\n",
        "    train_df, val_df = create_train_validation_split(df, split_date='2012-09-01')\n",
        "\n",
        "    # 2. Preprocessing\n",
        "    print(\"\\n2. Preprocessing data...\")\n",
        "    preprocessor = DataPreprocessingPipeline()\n",
        "\n",
        "    # Fit on training data\n",
        "    train_processed = preprocessor.fit_transform(train_df)\n",
        "    val_processed = preprocessor.transform(val_df)\n",
        "\n",
        "    # Prepare features and target\n",
        "    feature_cols = [col for col in train_processed.columns if col not in [target_col, 'Date']]\n",
        "    X_train = train_processed[feature_cols]\n",
        "    y_train = train_processed[target_col]\n",
        "    X_val = val_processed[feature_cols]\n",
        "    y_val = val_processed[target_col]\n",
        "\n",
        "    print(f\"Training features shape: {X_train.shape}\")\n",
        "    print(f\"Validation features shape: {X_val.shape}\")\n",
        "\n",
        "    # 3. XGBoost Training\n",
        "    print(\"\\n3. Training XGBoost model...\")\n",
        "    trainer = XGBoostTrainer()\n",
        "\n",
        "    # Option A: Grid search for best parameters\n",
        "    best_params, best_score = trainer.grid_search_basic_params(X_train, y_train)\n",
        "\n",
        "    custom_params = {\n",
        "         'learning_rate': 0.1,\n",
        "         'max_depth': 6,\n",
        "         'n_estimators': 300,\n",
        "         'subsample': 0.8,\n",
        "         'colsample_bytree': 0.8\n",
        "     }\n",
        "    trainer.train_with_custom_params(X_train, y_train, **custom_params)\n",
        "\n",
        "    # 4. Model Evaluation\n",
        "    print(\"\\n4. Evaluating model...\")\n",
        "    wmae, mae, y_pred = trainer.evaluate_model(X_val, y_val)\n",
        "\n",
        "    # 5. Feature Importance\n",
        "    print(\"\\n5. Top 20 Feature Importances:\")\n",
        "    importance_df = trainer.get_feature_importance(feature_cols, top_n=20)\n",
        "    print(importance_df)\n",
        "\n",
        "    return trainer, preprocessor, importance_df\n"
      ],
      "metadata": {
        "id": "fYYVMhsqfLL-"
      },
      "id": "fYYVMhsqfLL-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer, preprocessor, importance_df = main_training_pipeline(train_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGSJGuHWfeTq",
        "outputId": "83db8339-4ebe-4479-cb72-dbcfcb377edd"
      },
      "id": "HGSJGuHWfeTq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Walmart Sales Forecasting Pipeline ===\n",
            "\n",
            "1. Creating train-validation split...\n",
            "Train set: 397841 samples\n",
            "Validation set: 23729 samples\n",
            "Train date range: 2010-02-05 00:00:00 to 2012-08-31 00:00:00\n",
            "Validation date range: 2012-09-07 00:00:00 to 2012-10-26 00:00:00\n",
            "\n",
            "2. Preprocessing data...\n",
            "Training features shape: (397841, 44)\n",
            "Validation features shape: (23729, 44)\n",
            "\n",
            "3. Training XGBoost model...\n",
            "Starting grid search for basic parameters...\n",
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
            "Best parameters: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 300, 'random_state': 42}\n",
            "Best WMAE score: 1797.7908\n",
            "Training XGBoost with custom parameters...\n",
            "Model trained with parameters: {'objective': 'reg:squarederror', 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 300, 'random_state': 42, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
            "\n",
            "4. Evaluating model...\n",
            "Validation WMAE: 6918.3100\n",
            "Validation MAE: 3337.0404\n",
            "\n",
            "5. Top 20 Feature Importances:\n",
            "                        feature  importance\n",
            "41           Weekly_Sales_lag_1    0.348639\n",
            "42           Weekly_Sales_lag_4    0.335641\n",
            "43          Weekly_Sales_lag_52    0.083808\n",
            "2                     IsHoliday    0.026520\n",
            "21                         Week    0.023078\n",
            "7                     MarkDown3    0.016585\n",
            "13                         Size    0.015666\n",
            "1                          Dept    0.014937\n",
            "31            thanksgiving_week    0.014081\n",
            "20                        Month    0.013509\n",
            "32          days_until_new_year    0.009077\n",
            "27                     Week_cos    0.008307\n",
            "28         days_until_christmas    0.007705\n",
            "29               christmas_week    0.007448\n",
            "39           MarkDown_Intensity    0.006976\n",
            "30      days_until_thanksgiving    0.006418\n",
            "19                         Year    0.004821\n",
            "22                          Day    0.004537\n",
            "34  days_until_independence_day    0.004282\n",
            "18        MarkDown5_was_missing    0.004245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECOND TRY"
      ],
      "metadata": {
        "id": "CVGOTP-6LOcV"
      },
      "id": "CVGOTP-6LOcV"
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "train['Date'] = pd.to_datetime(train['Date'])\n",
        "test['Date'] = pd.to_datetime(test['Date'])\n",
        "features['Date'] = pd.to_datetime(features\n",
        " ['Date'])\n",
        "\n",
        "train_df = pd.merge(train, features, on=['Store', 'Date'], how='left')\n",
        "test_df = pd.merge(test, features, on=['Store', 'Date'], how='left')\n",
        "\n",
        "# Merge store information\n",
        "train_df = pd.merge(train_df, stores, on='Store', how='left')\n",
        "test_df = pd.merge(test_df, stores, on='Store', how='left')\n",
        "\n",
        "print(\"\\n--- Merged Train Data Head ---\")\n",
        "print(train_df.head())\n",
        "print(\"\\n--- Merged Test Data Head ---\")\n",
        "print(test_df.head())\n",
        "\n",
        "print(\"\\n--- Merged Train Data Info ---\")\n",
        "print(train_df.info())\n",
        "print(\"\\n--- Merged Test Data Info ---\")\n",
        "print(test_df.info())\n",
        "\n",
        "# After merging but before preprocessing, add this:\n",
        "train_df['IsHoliday'] = train_df['IsHoliday_x'] | train_df['IsHoliday_y']\n",
        "test_df['IsHoliday'] = test_df['IsHoliday_x'] | test_df['IsHoliday_y']\n",
        "\n",
        "# Then drop the redundant columns\n",
        "train_df = train_df.drop(['IsHoliday_x', 'IsHoliday_y'], axis=1)\n",
        "test_df = test_df.drop(['IsHoliday_x', 'IsHoliday_y'], axis=1)\n",
        "X_train = train_df.drop(['Weekly_Sales'], axis=1)\n",
        "y_train = train_df['Weekly_Sales']\n",
        "\n",
        "# Free up memory\n",
        "del train, test, features, stores\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koS1lJKkL2lM",
        "outputId": "b2defce8-12ee-44be-c2aa-0ef048795f88"
      },
      "id": "koS1lJKkL2lM",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Merged Train Data Head ---\n",
            "   Store  Dept       Date  Weekly_Sales  IsHoliday_x  Temperature  Fuel_Price  \\\n",
            "0      1     1 2010-02-05      24924.50        False        42.31       2.572   \n",
            "1      1     1 2010-02-12      46039.49         True        38.51       2.548   \n",
            "2      1     1 2010-02-19      41595.55        False        39.93       2.514   \n",
            "3      1     1 2010-02-26      19403.54        False        46.63       2.561   \n",
            "4      1     1 2010-03-05      21827.90        False        46.50       2.625   \n",
            "\n",
            "   MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  \\\n",
            "0        NaN        NaN        NaN        NaN        NaN  211.096358   \n",
            "1        NaN        NaN        NaN        NaN        NaN  211.242170   \n",
            "2        NaN        NaN        NaN        NaN        NaN  211.289143   \n",
            "3        NaN        NaN        NaN        NaN        NaN  211.319643   \n",
            "4        NaN        NaN        NaN        NaN        NaN  211.350143   \n",
            "\n",
            "   Unemployment  IsHoliday_y Type    Size  \n",
            "0         8.106        False    A  151315  \n",
            "1         8.106         True    A  151315  \n",
            "2         8.106        False    A  151315  \n",
            "3         8.106        False    A  151315  \n",
            "4         8.106        False    A  151315  \n",
            "\n",
            "--- Merged Test Data Head ---\n",
            "   Store  Dept       Date  IsHoliday_x  Temperature  Fuel_Price  MarkDown1  \\\n",
            "0      1     1 2012-11-02        False        55.32       3.386    6766.44   \n",
            "1      1     1 2012-11-09        False        61.24       3.314   11421.32   \n",
            "2      1     1 2012-11-16        False        52.92       3.252    9696.28   \n",
            "3      1     1 2012-11-23         True        56.23       3.211     883.59   \n",
            "4      1     1 2012-11-30        False        52.34       3.207    2460.03   \n",
            "\n",
            "   MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment  \\\n",
            "0    5147.70      50.82    3639.90    2737.42  223.462779         6.573   \n",
            "1    3370.89      40.28    4646.79    6154.16  223.481307         6.573   \n",
            "2     292.10     103.78    1133.15    6612.69  223.512911         6.573   \n",
            "3       4.17   74910.32     209.91     303.32  223.561947         6.573   \n",
            "4        NaN    3838.35     150.57    6966.34  223.610984         6.573   \n",
            "\n",
            "   IsHoliday_y Type    Size  \n",
            "0        False    A  151315  \n",
            "1        False    A  151315  \n",
            "2        False    A  151315  \n",
            "3         True    A  151315  \n",
            "4        False    A  151315  \n",
            "\n",
            "--- Merged Train Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 421570 entries, 0 to 421569\n",
            "Data columns (total 17 columns):\n",
            " #   Column        Non-Null Count   Dtype         \n",
            "---  ------        --------------   -----         \n",
            " 0   Store         421570 non-null  int64         \n",
            " 1   Dept          421570 non-null  int64         \n",
            " 2   Date          421570 non-null  datetime64[ns]\n",
            " 3   Weekly_Sales  421570 non-null  float64       \n",
            " 4   IsHoliday_x   421570 non-null  bool          \n",
            " 5   Temperature   421570 non-null  float64       \n",
            " 6   Fuel_Price    421570 non-null  float64       \n",
            " 7   MarkDown1     150681 non-null  float64       \n",
            " 8   MarkDown2     111248 non-null  float64       \n",
            " 9   MarkDown3     137091 non-null  float64       \n",
            " 10  MarkDown4     134967 non-null  float64       \n",
            " 11  MarkDown5     151432 non-null  float64       \n",
            " 12  CPI           421570 non-null  float64       \n",
            " 13  Unemployment  421570 non-null  float64       \n",
            " 14  IsHoliday_y   421570 non-null  bool          \n",
            " 15  Type          421570 non-null  object        \n",
            " 16  Size          421570 non-null  int64         \n",
            "dtypes: bool(2), datetime64[ns](1), float64(10), int64(3), object(1)\n",
            "memory usage: 49.0+ MB\n",
            "None\n",
            "\n",
            "--- Merged Test Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 115064 entries, 0 to 115063\n",
            "Data columns (total 16 columns):\n",
            " #   Column        Non-Null Count   Dtype         \n",
            "---  ------        --------------   -----         \n",
            " 0   Store         115064 non-null  int64         \n",
            " 1   Dept          115064 non-null  int64         \n",
            " 2   Date          115064 non-null  datetime64[ns]\n",
            " 3   IsHoliday_x   115064 non-null  bool          \n",
            " 4   Temperature   115064 non-null  float64       \n",
            " 5   Fuel_Price    115064 non-null  float64       \n",
            " 6   MarkDown1     114915 non-null  float64       \n",
            " 7   MarkDown2     86437 non-null   float64       \n",
            " 8   MarkDown3     105235 non-null  float64       \n",
            " 9   MarkDown4     102176 non-null  float64       \n",
            " 10  MarkDown5     115064 non-null  float64       \n",
            " 11  CPI           76902 non-null   float64       \n",
            " 12  Unemployment  76902 non-null   float64       \n",
            " 13  IsHoliday_y   115064 non-null  bool          \n",
            " 14  Type          115064 non-null  object        \n",
            " 15  Size          115064 non-null  int64         \n",
            "dtypes: bool(2), datetime64[ns](1), float64(9), int64(3), object(1)\n",
            "memory usage: 12.5+ MB\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "class MissingMarkdownHandler(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.markdown_cols = [f'MarkDown{i}' for i in range(1, 6)]\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        for col in self.markdown_cols:\n",
        "            if col in X_copy.columns:\n",
        "                X_copy[f\"{col}_was_missing\"] = X_copy[col].isna().astype(int)\n",
        "                X_copy[col] = X_copy[col].fillna(0)\n",
        "\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "f7W_ZhN8MUgO"
      },
      "id": "f7W_ZhN8MUgO",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MissingValueImputer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom Transformer to handle missing values for specific columns.\n",
        "    - MarkDown columns: fill with 0.\n",
        "    - Other specified numerical columns: fill with ffill then bfill, fallback to mean.\n",
        "    \"\"\"\n",
        "    def __init__(self, numerical_cols_to_impute=None):\n",
        "        self.numerical_cols_to_impute = numerical_cols_to_impute if numerical_cols_to_impute is not None else ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "        self.means = {} # To store means for fallback imputation during transform\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Calculate means for fallback imputation from the training data\n",
        "        for col in self.numerical_cols_to_impute:\n",
        "            if col in X.columns:\n",
        "                self.means[col] = X[col].mean()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        # Impute other numerical columns with ffill then bfill, fallback to mean\n",
        "        for col in self.numerical_cols_to_impute:\n",
        "            if col in X_copy.columns:\n",
        "                X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n",
        "                # Fallback to mean if NaNs still exist (e.g., if all values were NaN in a column)\n",
        "                if X_copy[col].isnull().any() and col in self.means:\n",
        "                    X_copy[col] = X_copy[col].fillna(self.means[col])\n",
        "        return X_copy\n"
      ],
      "metadata": {
        "id": "lSNflVOpMkIB"
      },
      "id": "lSNflVOpMkIB",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AdvancedDateFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Enhanced date feature extractor with holiday detection and seasonal patterns\n",
        "    \"\"\"\n",
        "    def __init__(self, date_column: str = 'Date', include_holidays: bool = True,\n",
        "                 include_seasonal: bool = True, include_lags: bool = False):\n",
        "        self.date_column = date_column\n",
        "        self.include_holidays = include_holidays\n",
        "        self.include_seasonal = include_seasonal\n",
        "        self.include_lags = include_lags\n",
        "\n",
        "    def _is_holiday_period(self, date):\n",
        "        \"\"\"Detect major US retail holiday periods\"\"\"\n",
        "        month, day = date.month, date.day\n",
        "\n",
        "        # Black Friday week (last week of November)\n",
        "        if month == 11 and day >= 22:\n",
        "            return 1\n",
        "        # Christmas season (December)\n",
        "        elif month == 12:\n",
        "            return 1\n",
        "        # New Year\n",
        "        elif month == 1 and day <= 7:\n",
        "            return 1\n",
        "        # Labor Day (first Monday in September) - approximate\n",
        "        elif month == 9 and day <= 7:\n",
        "            return 1\n",
        "        # Memorial Day (last Monday in May) - approximate\n",
        "        elif month == 5 and day >= 25:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        if self.date_column not in X_copy.columns:\n",
        "            raise ValueError(f\"Date column '{self.date_column}' not found in DataFrame.\")\n",
        "\n",
        "        # Ensure datetime format\n",
        "        X_copy[self.date_column] = pd.to_datetime(X_copy[self.date_column])\n",
        "\n",
        "        # Basic temporal features\n",
        "        X_copy['Year'] = X_copy[self.date_column].dt.year\n",
        "        X_copy['Month'] = X_copy[self.date_column].dt.month\n",
        "        X_copy['Day'] = X_copy[self.date_column].dt.day\n",
        "        X_copy['DayOfWeek'] = X_copy[self.date_column].dt.dayofweek\n",
        "        X_copy['Week'] = X_copy[self.date_column].dt.isocalendar().week.astype(int)\n",
        "        X_copy['Quarter'] = X_copy[self.date_column].dt.quarter\n",
        "        X_copy['DayOfYear'] = X_copy[self.date_column].dt.dayofyear\n",
        "\n",
        "        # Cyclical encoding\n",
        "        X_copy['Month_sin'] = np.sin(2 * np.pi * X_copy['Month'] / 12)\n",
        "        X_copy['Month_cos'] = np.cos(2 * np.pi * X_copy['Month'] / 12)\n",
        "        X_copy['Week_sin'] = np.sin(2 * np.pi * X_copy['Week'] / 52)\n",
        "        X_copy['Week_cos'] = np.cos(2 * np.pi * X_copy['Week'] / 52)\n",
        "        X_copy['DayOfWeek_sin'] = np.sin(2 * np.pi * X_copy['DayOfWeek'] / 7)\n",
        "        X_copy['DayOfWeek_cos'] = np.cos(2 * np.pi * X_copy['DayOfWeek'] / 7)\n",
        "\n",
        "        # Seasonal features\n",
        "        if self.include_seasonal:\n",
        "            X_copy['Season'] = X_copy['Month'].map({12: 0, 1: 0, 2: 0,  # Winter\n",
        "                                                   3: 1, 4: 1, 5: 1,    # Spring\n",
        "                                                   6: 2, 7: 2, 8: 2,    # Summer\n",
        "                                                   9: 3, 10: 3, 11: 3}) # Fall\n",
        "\n",
        "            # Weekend indicator\n",
        "            X_copy['IsWeekend'] = (X_copy['DayOfWeek'] >= 5).astype(int)\n",
        "\n",
        "            # Month-end indicator\n",
        "            X_copy['IsMonthEnd'] = (X_copy[self.date_column].dt.is_month_end).astype(int)\n",
        "            X_copy['IsMonthStart'] = (X_copy[self.date_column].dt.is_month_start).astype(int)\n",
        "\n",
        "        # Holiday features\n",
        "        if self.include_holidays:\n",
        "            X_copy['IsHolidayPeriod'] = X_copy[self.date_column].apply(self._is_holiday_period)\n",
        "\n",
        "            # Convert existing IsHoliday to int if it exists\n",
        "            if 'IsHoliday' in X_copy.columns:\n",
        "                if X_copy['IsHoliday'].dtype == bool:\n",
        "                    X_copy['IsHoliday'] = X_copy['IsHoliday'].astype(int)\n",
        "\n",
        "        # Enhanced markdown features if markdown columns exist\n",
        "        markdown_cols = [col for col in X_copy.columns if col.startswith('MarkDown') and not col.endswith('_was_missing')]\n",
        "        if markdown_cols:\n",
        "            X_copy['Total_MarkDown'] = X_copy[markdown_cols].sum(axis=1)\n",
        "            X_copy['MarkDown_Intensity'] = X_copy['Total_MarkDown'] / (X_copy['Total_MarkDown'].mean() + 1e-8)\n",
        "            X_copy['HasMarkDown'] = (X_copy['Total_MarkDown'] > 0).astype(int)\n",
        "\n",
        "        # Economic features if available\n",
        "        econ_cols = ['Fuel_Price', 'CPI', 'Unemployment']\n",
        "        available_econ = [col for col in econ_cols if col in X_copy.columns]\n",
        "\n",
        "        if len(available_econ) >= 2:\n",
        "            if 'Fuel_Price' in X_copy.columns and 'CPI' in X_copy.columns:\n",
        "                X_copy['Fuel_CPI_Ratio'] = X_copy['Fuel_Price'] / (X_copy['CPI'] + 1e-8)\n",
        "\n",
        "            if 'CPI' in X_copy.columns and 'Unemployment' in X_copy.columns:\n",
        "                X_copy['Economic_Index'] = (X_copy['CPI'] * 0.4 + (100 - X_copy['Unemployment']) * 0.6) / 100\n",
        "\n",
        "        return X_copy\n"
      ],
      "metadata": {
        "id": "NkycDfRyUrTE"
      },
      "id": "NkycDfRyUrTE",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DateFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom Transformer to extract temporal features from the 'Date' column.\n",
        "    \"\"\"\n",
        "    def __init__(self, date_column='Date'):\n",
        "        self.date_column = date_column\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        if self.date_column not in X_copy.columns:\n",
        "            raise ValueError(f\"Date column '{self.date_column}' not found in DataFrame.\")\n",
        "\n",
        "        X_copy[self.date_column] = pd.to_datetime(X_copy[self.date_column])\n",
        "\n",
        "        X_copy['Year'] = X_copy[self.date_column].dt.year\n",
        "        X_copy['Month'] = X_copy[self.date_column].dt.month\n",
        "        X_copy['Month_sin'] = np.sin(2 * np.pi * X_copy['Month'] / 12)\n",
        "        X_copy['Month_cos'] = np.cos(2 * np.pi * X_copy['Month'] / 12)\n",
        "\n",
        "        # Using .dt.isocalendar().week for consistent week numbering across years\n",
        "        X_copy['Week'] = X_copy[self.date_column].dt.isocalendar().week.astype(int)\n",
        "        X_copy['Day'] = X_copy[self.date_column].dt.day\n",
        "        X_copy['DayOfWeek'] = X_copy[self.date_column].dt.dayofweek\n",
        "\n",
        "        X_copy['Week_sin'] = np.sin(2 * np.pi * X_copy['Week'] / 52)\n",
        "        X_copy['Week_cos'] = np.cos(2 * np.pi * X_copy['Week'] / 52)\n",
        "\n",
        "        # Markdown aggregation\n",
        "        X_copy['Total_MarkDown'] = X_copy[[f'MarkDown{i}' for i in range(1, 6)]].sum(axis=1)\n",
        "        X_copy['MarkDown_Intensity'] = X_copy['Total_MarkDown'] / (X_copy['Total_MarkDown'].mean() + 1)\n",
        "\n",
        "        # Economic indicators\n",
        "        X_copy['Fuel_CPI_Ratio'] = X_copy['Fuel_Price'] / X_copy['CPI']\n",
        "        X_copy['Economic_Index'] = (X_copy['CPI'] * 0.4 + (100 - X_copy['Unemployment']) * 0.6) / 100\n",
        "\n",
        "\n",
        "        # Convert IsHoliday to integer if it exists and is boolean\n",
        "        if 'IsHoliday' in X_copy.columns and X_copy['IsHoliday'].dtype == bool:\n",
        "            X_copy['IsHoliday'] = X_copy['IsHoliday'].astype(int)\n",
        "\n",
        "        # Keep the 'Date' column for ARIMA\n",
        "        return X_copy # Removed .drop(columns=[self.date_column, \"Month\", \"Week\"])"
      ],
      "metadata": {
        "id": "pc9FGLgGMrA3"
      },
      "id": "pc9FGLgGMrA3",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XGBoostLabelEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom Transformer to encode categorical features for XGBoost.\n",
        "    XGBoost works better with label-encoded categoricals than pandas categories.\n",
        "    \"\"\"\n",
        "    def __init__(self, categorical_cols=None):\n",
        "        self.categorical_cols = categorical_cols if categorical_cols is not None else ['Store', 'Dept', 'Type']\n",
        "        self.label_encoders = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        for col in self.categorical_cols:\n",
        "            if col in X.columns:\n",
        "                self.label_encoders[col] = LabelEncoder()\n",
        "                self.label_encoders[col].fit(X[col].astype(str))\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        for col in self.categorical_cols:\n",
        "            if col in X_copy.columns and col in self.label_encoders:\n",
        "                # Handle unseen categories by using a default value\n",
        "                X_copy[col] = X_copy[col].astype(str)\n",
        "                known_categories = set(self.label_encoders[col].classes_)\n",
        "                X_copy[col] = X_copy[col].apply(lambda x: x if x in known_categories else 'unknown')\n",
        "\n",
        "                # Add 'unknown' to encoder if needed\n",
        "                if 'unknown' not in self.label_encoders[col].classes_:\n",
        "                    current_classes = list(self.label_encoders[col].classes_)\n",
        "                    current_classes.append('unknown')\n",
        "                    self.label_encoders[col].classes_ = np.array(current_classes)\n",
        "\n",
        "                X_copy[col] = self.label_encoders[col].transform(X_copy[col])\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "_jVyo9DRMuNU"
      },
      "id": "_jVyo9DRMuNU",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XGBoostTargetEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, categorical_cols=None, smoothing=1.0):\n",
        "        self.categorical_cols = categorical_cols if categorical_cols is not None else ['Store', 'Dept', 'Type']\n",
        "        self.smoothing = smoothing\n",
        "        self.target_encoders = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for col in self.categorical_cols:\n",
        "            if col in X.columns:\n",
        "                self.target_encoders[col] = TargetEncoder(smoothing=self.smoothing)\n",
        "                self.target_encoders[col].fit(X[col], y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        for col in self.categorical_cols:\n",
        "            if col in X_copy.columns and col in self.target_encoders:\n",
        "                X_copy[col] = self.target_encoders[col].transform(X_copy[col])\n",
        "        return X_copy\n"
      ],
      "metadata": {
        "id": "tvClzLwDMxHL"
      },
      "id": "tvClzLwDMxHL",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from typing import Optional, List\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class SmartLabelEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Enhanced label encoder with better handling of unseen categories and validation sets\n",
        "    \"\"\"\n",
        "    def __init__(self, categorical_cols: Optional[List[str]] = None,\n",
        "                 handle_unknown: str = 'ignore', unknown_value: int = -1):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        - handle_unknown: 'ignore', 'error', or 'use_encoded_value'\n",
        "        - unknown_value: value to use for unknown categories\n",
        "        \"\"\"\n",
        "        self.categorical_cols = categorical_cols or ['Store', 'Dept', 'Type']\n",
        "        self.handle_unknown = handle_unknown\n",
        "        self.unknown_value = unknown_value\n",
        "        self.label_encoders = {}\n",
        "        self.category_mappings = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        for col in self.categorical_cols:\n",
        "            if col in X.columns:\n",
        "                # Create robust encoder\n",
        "                unique_vals = X[col].astype(str).unique()\n",
        "                unique_vals = unique_vals[pd.notna(unique_vals)]  # Remove NaN values\n",
        "\n",
        "                # Sort for consistent encoding\n",
        "                unique_vals = sorted(unique_vals)\n",
        "\n",
        "                # Create mapping\n",
        "                self.category_mappings[col] = {val: idx for idx, val in enumerate(unique_vals)}\n",
        "\n",
        "                # Add unknown category mapping if needed\n",
        "                if self.handle_unknown == 'use_encoded_value':\n",
        "                    max_idx = len(unique_vals)\n",
        "                    self.category_mappings[col]['__UNKNOWN__'] = max_idx\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        for col in self.categorical_cols:\n",
        "            if col in X_copy.columns and col in self.category_mappings:\n",
        "                # Convert to string and handle NaN\n",
        "                str_col = X_copy[col].astype(str)\n",
        "                str_col = str_col.replace('nan', '__UNKNOWN__')\n",
        "\n",
        "                # Map values\n",
        "                mapping = self.category_mappings[col]\n",
        "\n",
        "                if self.handle_unknown == 'ignore':\n",
        "                    # Map known values, keep unknown as is\n",
        "                    X_copy[col] = str_col.map(mapping).fillna(self.unknown_value)\n",
        "                elif self.handle_unknown == 'use_encoded_value':\n",
        "                    # Map all values, use special encoding for unknown\n",
        "                    X_copy[col] = str_col.apply(lambda x: mapping.get(x, mapping['__UNKNOWN__']))\n",
        "                else:  # error\n",
        "                    unknown_vals = set(str_col.unique()) - set(mapping.keys())\n",
        "                    if unknown_vals:\n",
        "                        raise ValueError(f\"Unknown categories found in column {col}: {unknown_vals}\")\n",
        "                    X_copy[col] = str_col.map(mapping)\n",
        "\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "C8C5qo0IT6Me"
      },
      "id": "C8C5qo0IT6Me",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Build the preprocessing pipeline\n",
        "preprocessing_pipeline = Pipeline([\n",
        "    ('missing_markdown', MissingMarkdownHandler()),\n",
        "    ('missing_imputer', MissingValueImputer()),\n",
        "    ('date_features', AdvancedDateFeatureExtractor()),\n",
        "    ('label_encoder', SmartLabelEncoder())  # or XGBoostTargetEncoder if you want target encoding\n",
        "])"
      ],
      "metadata": {
        "id": "aBpw84HiMzNF"
      },
      "id": "aBpw84HiMzNF",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_processed = preprocessing_pipeline.fit_transform(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "kQOOMHuyM___",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d93ebe-1631-4de1-903e-d310d5cdca10"
      },
      "id": "kQOOMHuyM___",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1507361449.py:24: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'Date' in X_train_processed.columns:\n",
        "    dates = X_train_processed['Date']  # Keep for potential time-based validation\n",
        "    X_train_processed = X_train_processed.drop(['Date'], axis=1)\n",
        "\n",
        "print(f\"Training data shape: {X_train_processed.shape}\")\n",
        "print(f\"Features: {list(X_train_processed.columns)}\")\n",
        "\n",
        "print(\"\\n--- Applying Preprocessing Pipeline to Test Data ---\")\n",
        "# For the test set, we only call transform, as fit was done on the training data.\n",
        "X_test_processed = preprocessing_pipeline.transform(test_df.drop(columns=['Id'], errors='ignore'))\n",
        "\n",
        "if 'Date' in X_test_processed.columns:\n",
        "    dates = X_test_processed['Date']  # Keep for potential time-based validation\n",
        "    X_test_processed = X_test_processed.drop(['Date'], axis=1)\n",
        "\n",
        "print(\"\\nProcessed X_train_processed info:\")\n",
        "print(X_train_processed.info())\n",
        "print(\"\\nProcessed X_test_processed info:\")\n",
        "print(X_test_processed.info())\n",
        "\n",
        "# Verify no missing values in processed data\n",
        "print(\"\\nMissing values in processed X_train_processed:\\n\", X_train_processed.isnull().sum().sum())\n",
        "print(\"Missing values in processed X_test_processed:\\n\", X_test_processed.isnull().sum().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GPoqlv_OKLx",
        "outputId": "fd9011de-d1f7-4587-deba-c1d84c859885"
      },
      "id": "7GPoqlv_OKLx",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (421570, 42)\n",
            "Features: ['Store', 'Dept', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size', 'IsHoliday', 'MarkDown1_was_missing', 'MarkDown2_was_missing', 'MarkDown3_was_missing', 'MarkDown4_was_missing', 'MarkDown5_was_missing', 'Year', 'Month', 'Day', 'DayOfWeek', 'Week', 'Quarter', 'DayOfYear', 'Month_sin', 'Month_cos', 'Week_sin', 'Week_cos', 'DayOfWeek_sin', 'DayOfWeek_cos', 'Season', 'IsWeekend', 'IsMonthEnd', 'IsMonthStart', 'IsHolidayPeriod', 'Total_MarkDown', 'MarkDown_Intensity', 'HasMarkDown', 'Fuel_CPI_Ratio', 'Economic_Index']\n",
            "\n",
            "--- Applying Preprocessing Pipeline to Test Data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1507361449.py:24: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed X_train_processed info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 421570 entries, 0 to 421569\n",
            "Data columns (total 42 columns):\n",
            " #   Column                 Non-Null Count   Dtype  \n",
            "---  ------                 --------------   -----  \n",
            " 0   Store                  421570 non-null  int64  \n",
            " 1   Dept                   421570 non-null  int64  \n",
            " 2   Temperature            421570 non-null  float64\n",
            " 3   Fuel_Price             421570 non-null  float64\n",
            " 4   MarkDown1              421570 non-null  float64\n",
            " 5   MarkDown2              421570 non-null  float64\n",
            " 6   MarkDown3              421570 non-null  float64\n",
            " 7   MarkDown4              421570 non-null  float64\n",
            " 8   MarkDown5              421570 non-null  float64\n",
            " 9   CPI                    421570 non-null  float64\n",
            " 10  Unemployment           421570 non-null  float64\n",
            " 11  Type                   421570 non-null  int64  \n",
            " 12  Size                   421570 non-null  int64  \n",
            " 13  IsHoliday              421570 non-null  int64  \n",
            " 14  MarkDown1_was_missing  421570 non-null  int64  \n",
            " 15  MarkDown2_was_missing  421570 non-null  int64  \n",
            " 16  MarkDown3_was_missing  421570 non-null  int64  \n",
            " 17  MarkDown4_was_missing  421570 non-null  int64  \n",
            " 18  MarkDown5_was_missing  421570 non-null  int64  \n",
            " 19  Year                   421570 non-null  int32  \n",
            " 20  Month                  421570 non-null  int32  \n",
            " 21  Day                    421570 non-null  int32  \n",
            " 22  DayOfWeek              421570 non-null  int32  \n",
            " 23  Week                   421570 non-null  int64  \n",
            " 24  Quarter                421570 non-null  int32  \n",
            " 25  DayOfYear              421570 non-null  int32  \n",
            " 26  Month_sin              421570 non-null  float64\n",
            " 27  Month_cos              421570 non-null  float64\n",
            " 28  Week_sin               421570 non-null  float64\n",
            " 29  Week_cos               421570 non-null  float64\n",
            " 30  DayOfWeek_sin          421570 non-null  float64\n",
            " 31  DayOfWeek_cos          421570 non-null  float64\n",
            " 32  Season                 421570 non-null  int64  \n",
            " 33  IsWeekend              421570 non-null  int64  \n",
            " 34  IsMonthEnd             421570 non-null  int64  \n",
            " 35  IsMonthStart           421570 non-null  int64  \n",
            " 36  IsHolidayPeriod        421570 non-null  int64  \n",
            " 37  Total_MarkDown         421570 non-null  float64\n",
            " 38  MarkDown_Intensity     421570 non-null  float64\n",
            " 39  HasMarkDown            421570 non-null  int64  \n",
            " 40  Fuel_CPI_Ratio         421570 non-null  float64\n",
            " 41  Economic_Index         421570 non-null  float64\n",
            "dtypes: float64(19), int32(6), int64(17)\n",
            "memory usage: 125.4 MB\n",
            "None\n",
            "\n",
            "Processed X_test_processed info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 115064 entries, 0 to 115063\n",
            "Data columns (total 42 columns):\n",
            " #   Column                 Non-Null Count   Dtype  \n",
            "---  ------                 --------------   -----  \n",
            " 0   Store                  115064 non-null  int64  \n",
            " 1   Dept                   115064 non-null  int64  \n",
            " 2   Temperature            115064 non-null  float64\n",
            " 3   Fuel_Price             115064 non-null  float64\n",
            " 4   MarkDown1              115064 non-null  float64\n",
            " 5   MarkDown2              115064 non-null  float64\n",
            " 6   MarkDown3              115064 non-null  float64\n",
            " 7   MarkDown4              115064 non-null  float64\n",
            " 8   MarkDown5              115064 non-null  float64\n",
            " 9   CPI                    115064 non-null  float64\n",
            " 10  Unemployment           115064 non-null  float64\n",
            " 11  Type                   115064 non-null  int64  \n",
            " 12  Size                   115064 non-null  int64  \n",
            " 13  IsHoliday              115064 non-null  int64  \n",
            " 14  MarkDown1_was_missing  115064 non-null  int64  \n",
            " 15  MarkDown2_was_missing  115064 non-null  int64  \n",
            " 16  MarkDown3_was_missing  115064 non-null  int64  \n",
            " 17  MarkDown4_was_missing  115064 non-null  int64  \n",
            " 18  MarkDown5_was_missing  115064 non-null  int64  \n",
            " 19  Year                   115064 non-null  int32  \n",
            " 20  Month                  115064 non-null  int32  \n",
            " 21  Day                    115064 non-null  int32  \n",
            " 22  DayOfWeek              115064 non-null  int32  \n",
            " 23  Week                   115064 non-null  int64  \n",
            " 24  Quarter                115064 non-null  int32  \n",
            " 25  DayOfYear              115064 non-null  int32  \n",
            " 26  Month_sin              115064 non-null  float64\n",
            " 27  Month_cos              115064 non-null  float64\n",
            " 28  Week_sin               115064 non-null  float64\n",
            " 29  Week_cos               115064 non-null  float64\n",
            " 30  DayOfWeek_sin          115064 non-null  float64\n",
            " 31  DayOfWeek_cos          115064 non-null  float64\n",
            " 32  Season                 115064 non-null  int64  \n",
            " 33  IsWeekend              115064 non-null  int64  \n",
            " 34  IsMonthEnd             115064 non-null  int64  \n",
            " 35  IsMonthStart           115064 non-null  int64  \n",
            " 36  IsHolidayPeriod        115064 non-null  int64  \n",
            " 37  Total_MarkDown         115064 non-null  float64\n",
            " 38  MarkDown_Intensity     115064 non-null  float64\n",
            " 39  HasMarkDown            115064 non-null  int64  \n",
            " 40  Fuel_CPI_Ratio         115064 non-null  float64\n",
            " 41  Economic_Index         115064 non-null  float64\n",
            "dtypes: float64(19), int32(6), int64(17)\n",
            "memory usage: 34.2 MB\n",
            "None\n",
            "\n",
            "Missing values in processed X_train_processed:\n",
            " 0\n",
            "Missing values in processed X_test_processed:\n",
            " 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_processed.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6tlrsvGuUNn",
        "outputId": "b22649cb-482e-467d-fd89-7cd00b3ef9df"
      },
      "id": "N6tlrsvGuUNn",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 421570 entries, 0 to 421569\n",
            "Data columns (total 42 columns):\n",
            " #   Column                 Non-Null Count   Dtype  \n",
            "---  ------                 --------------   -----  \n",
            " 0   Store                  421570 non-null  int64  \n",
            " 1   Dept                   421570 non-null  int64  \n",
            " 2   Temperature            421570 non-null  float64\n",
            " 3   Fuel_Price             421570 non-null  float64\n",
            " 4   MarkDown1              421570 non-null  float64\n",
            " 5   MarkDown2              421570 non-null  float64\n",
            " 6   MarkDown3              421570 non-null  float64\n",
            " 7   MarkDown4              421570 non-null  float64\n",
            " 8   MarkDown5              421570 non-null  float64\n",
            " 9   CPI                    421570 non-null  float64\n",
            " 10  Unemployment           421570 non-null  float64\n",
            " 11  Type                   421570 non-null  int64  \n",
            " 12  Size                   421570 non-null  int64  \n",
            " 13  IsHoliday              421570 non-null  int64  \n",
            " 14  MarkDown1_was_missing  421570 non-null  int64  \n",
            " 15  MarkDown2_was_missing  421570 non-null  int64  \n",
            " 16  MarkDown3_was_missing  421570 non-null  int64  \n",
            " 17  MarkDown4_was_missing  421570 non-null  int64  \n",
            " 18  MarkDown5_was_missing  421570 non-null  int64  \n",
            " 19  Year                   421570 non-null  int32  \n",
            " 20  Month                  421570 non-null  int32  \n",
            " 21  Day                    421570 non-null  int32  \n",
            " 22  DayOfWeek              421570 non-null  int32  \n",
            " 23  Week                   421570 non-null  int64  \n",
            " 24  Quarter                421570 non-null  int32  \n",
            " 25  DayOfYear              421570 non-null  int32  \n",
            " 26  Month_sin              421570 non-null  float64\n",
            " 27  Month_cos              421570 non-null  float64\n",
            " 28  Week_sin               421570 non-null  float64\n",
            " 29  Week_cos               421570 non-null  float64\n",
            " 30  DayOfWeek_sin          421570 non-null  float64\n",
            " 31  DayOfWeek_cos          421570 non-null  float64\n",
            " 32  Season                 421570 non-null  int64  \n",
            " 33  IsWeekend              421570 non-null  int64  \n",
            " 34  IsMonthEnd             421570 non-null  int64  \n",
            " 35  IsMonthStart           421570 non-null  int64  \n",
            " 36  IsHolidayPeriod        421570 non-null  int64  \n",
            " 37  Total_MarkDown         421570 non-null  float64\n",
            " 38  MarkDown_Intensity     421570 non-null  float64\n",
            " 39  HasMarkDown            421570 non-null  int64  \n",
            " 40  Fuel_CPI_Ratio         421570 non-null  float64\n",
            " 41  Economic_Index         421570 non-null  float64\n",
            "dtypes: float64(19), int32(6), int64(17)\n",
            "memory usage: 125.4 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prepare weights for training (Walmart competition uses WMAE - holiday weeks get 5x weight)\n",
        "# We need the 'IsHoliday' column which is now an integer from DateFeatureExtractor\n",
        "train_weights = np.where(X_train_processed['IsHoliday'] == 1, 5, 1)\n",
        "\n",
        "# Store test IDs for submission\n",
        "test_ids = test_df['Store'].astype(str) + '_' + test_df['Dept'].astype(str) + '_' + test_df['Date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "# Sort processed data by date for proper time-series splitting\n",
        "# We need to re-attach Date for splitting\n",
        "temp_train_df = X_train_processed.copy()\n",
        "temp_train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "\n",
        "# Get original dates back for sorting\n",
        "temp_train_df['Weekly_Sales'] = y_train\n",
        "temp_train_df = temp_train_df.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "# Define a cutoff date for validation (avoid random splits in time series)\n",
        "validation_cutoff_date = pd.to_datetime('2012-07-01')\n",
        "\n",
        "# Remove Date column from features list for training\n",
        "features_for_training = [col for col in features_after_pipeline if col != 'Date']\n",
        "\n",
        "# # Split data based on time\n",
        "X_train_split = temp_train_df[temp_train_df['Date'] < validation_cutoff_date][features_for_training]\n",
        "y_train_split = temp_train_df[temp_train_df['Date'] < validation_cutoff_date]['Weekly_Sales']\n",
        "X_val_split = temp_train_df[temp_train_df['Date'] >= validation_cutoff_date][features_for_training]\n",
        "y_val_split = temp_train_df[temp_train_df['Date'] >= validation_cutoff_date]['Weekly_Sales']\n",
        "\n",
        "# Calculate weights for validation split\n",
        "def weighted_mean_absolute_error(y_true, y_pred, weights):\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "val_weights = np.where(X_val_split['IsHoliday'] == 1, 5, 1)\n",
        "train_weights_split = np.where(X_train_split['IsHoliday'] == 1, 5, 1)"
      ],
      "metadata": {
        "id": "L9UGNBu8Ofn9"
      },
      "id": "L9UGNBu8Ofn9",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_base_params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'n_estimators': 200,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'min_child_weight': 1,\n",
        "    'tree_method': 'hist',\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'eval_metric': 'mae',\n",
        "    'early_stopping_rounds': 50\n",
        "}"
      ],
      "metadata": {
        "id": "8IAzPZZOOj9_"
      },
      "id": "8IAzPZZOOj9_",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'max_depth': [6, 8],\n",
        "    'learning_rate': [0.05, 0.1]\n",
        "}"
      ],
      "metadata": {
        "id": "HZAqcp6tOneC"
      },
      "id": "HZAqcp6tOneC",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_score = float('inf')\n",
        "best_params = None\n",
        "best_model = None\n"
      ],
      "metadata": {
        "id": "CuoU-4GxOpju"
      },
      "id": "CuoU-4GxOpju",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "        subrun_name = f\"depth={params['max_depth']}_lr={params['learning_rate']}\"\n",
        "        model_params = {**xgb_base_params, **params}\n",
        "        model = xgb.XGBRegressor(**model_params)\n",
        "\n",
        "        model.fit(\n",
        "            X_train_split,\n",
        "            y_train_split,\n",
        "            sample_weight=train_weights_split,\n",
        "            eval_set=[(X_val_split, y_val_split)],\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        val_preds = model.predict(X_val_split)\n",
        "        train_preds = model.predict(X_train_split)\n",
        "\n",
        "        val_wmae = weighted_mean_absolute_error(y_val_split, val_preds, val_weights)\n",
        "        train_wmae = weighted_mean_absolute_error(y_train_split, train_preds, train_weights_split)\n",
        "\n",
        "\n",
        "        print(f\"WMAE - Train: {train_wmae:.4f} | Val: {val_wmae:.4f} | Params: {params}\")\n",
        "\n",
        "        if val_wmae < best_score:\n",
        "            best_score = val_wmae\n",
        "            best_params = model_params\n",
        "            best_model = model\n",
        "\n",
        "print(\"\\nFinal Results:\")\n",
        "print(f\"Best WMAE: {best_score:.4f}\")\n",
        "print(\"Best parameters:\", {k: v for k, v in best_params.items() if k in param_grid})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0XQI6tCOu_J",
        "outputId": "c219641e-aec4-4a39-cb38-bcddd6d81668"
      },
      "id": "z0XQI6tCOu_J",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WMAE - Train: 4231.7323 | Val: 4066.0996 | Params: {'learning_rate': 0.05, 'max_depth': 6}\n",
            "WMAE - Train: 2878.5930 | Val: 2946.1302 | Params: {'learning_rate': 0.05, 'max_depth': 8}\n",
            "WMAE - Train: 3535.2761 | Val: 3493.8232 | Params: {'learning_rate': 0.1, 'max_depth': 6}\n",
            "WMAE - Train: 2313.7670 | Val: 2586.0551 | Params: {'learning_rate': 0.1, 'max_depth': 8}\n",
            "\n",
            "Final Results:\n",
            "Best WMAE: 2586.0551\n",
            "Best parameters: {'learning_rate': 0.1, 'max_depth': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kbbeHhpGkt_5",
        "outputId": "349d73ca-4095-4107-8e86-b476baa00ec9"
      },
      "id": "kbbeHhpGkt_5",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-3.1.4-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting mlflow-skinny==3.1.4 (from mlflow)\n",
            "  Downloading mlflow_skinny-3.1.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.16.0)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.1.4->mlflow)\n",
            "  Downloading databricks_sdk-0.60.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.116.1)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (8.7.0)\n",
            "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.1.4->mlflow)\n",
            "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.1.4->mlflow)\n",
            "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (2.11.7)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (4.14.1)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.35.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.4->mlflow) (0.47.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.4->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.4->mlflow) (3.23.0)\n",
            "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.4->mlflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.4->mlflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.4->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.4->mlflow) (2025.7.14)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==3.1.4->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.4->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.6.1)\n",
            "Downloading mlflow-3.1.4-py3-none-any.whl (24.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.1.4-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.60.0-py3-none-any.whl (677 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.0/677.0 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gunicorn, graphql-core, opentelemetry-api, graphql-relay, docker, alembic, opentelemetry-semantic-conventions, graphene, databricks-sdk, opentelemetry-sdk, mlflow-skinny, mlflow\n",
            "Successfully installed alembic-1.16.4 databricks-sdk-0.60.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-3.1.4 mlflow-skinny-3.1.4 opentelemetry-api-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Setup MLflow tracking\"\"\"\n",
        "import mlflow\n",
        "import os\n",
        "\n",
        "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"goguaD\"\n",
        "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"685c4f5b2a0c555f9136c60a8666661d952de9be\"\n",
        "mlflow.set_tracking_uri(\"https://dagshub.com/goguaD/finalProjectML.mlflow\")\n",
        "mlflow.set_experiment(\"walmart-sales\")"
      ],
      "metadata": {
        "id": "nKAoe2azPUlX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b506d909-0aa5-4722-e443-430025af27a3"
      },
      "id": "nKAoe2azPUlX",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='mlflow-artifacts:/0df2743fae1f428cbe1ef62bd56be3ce', creation_time=1751821538135, experiment_id='0', last_update_time=1751821538135, lifecycle_stage='active', name='walmart-sales', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import os\n",
        "\n",
        "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"goguaD\"\n",
        "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"685c4f5b2a0c555f9136c60a8666661d952de9be\"\n",
        "mlflow.set_tracking_uri(\"https://dagshub.com/goguaD/finalProjectML.mlflow\")\n",
        "mlflow.set_experiment(\"walmart-sales\")\n",
        "\n",
        "wmae = 2586.0551\n",
        "\n",
        "with mlflow.start_run(run_name = \"XGBoost_best\"):\n",
        "    mlflow.log_param(\"model\", \"XGBoost\")\n",
        "    mlflow.log_param(\"n_estimators\", 200)\n",
        "    mlflow.log_param(\"learning_rate\", 0.1)\n",
        "    mlflow.log_param(\"max_depth\", 8)\n",
        "    mlflow.log_param(\"subsample\", 0.8)\n",
        "    mlflow.log_param(\"colsample_bytree\", 0.8)\n",
        "    mlflow.log_param(\"tree_method\", \"hist\")\n",
        "    mlflow.log_param(\"objective\", \"reg:squarederror\")\n",
        "\n",
        "    mlflow.log_metric(\"final_wmae\", wmae)\n",
        "\n",
        "    print(\"Logged successfully!\")\n",
        "\n",
        "with mlflow.start_run(run_name = \"XGBoost_0.05_6\"):\n",
        "    mlflow.log_param(\"model\", \"XGBoost\")\n",
        "    mlflow.log_param(\"n_estimators\", 200)\n",
        "    mlflow.log_param(\"learning_rate\", 0.05)\n",
        "    mlflow.log_param(\"max_depth\", 6)\n",
        "    mlflow.log_param(\"subsample\", 0.8)\n",
        "    mlflow.log_param(\"colsample_bytree\", 0.8)\n",
        "    mlflow.log_param(\"tree_method\", \"hist\")\n",
        "    mlflow.log_param(\"objective\", \"reg:squarederror\")\n",
        "\n",
        "    mlflow.log_metric(\"final_wmae\", wmae)\n",
        "\n",
        "    print(\"Logged successfully!\")\n",
        "\n",
        "with mlflow.start_run(run_name = \"XGBoost_0.05_8\"):\n",
        "    mlflow.log_param(\"model\", \"XGBoost\")\n",
        "    mlflow.log_param(\"n_estimators\", 200)\n",
        "    mlflow.log_param(\"learning_rate\", 0.05)\n",
        "    mlflow.log_param(\"max_depth\", 8)\n",
        "    mlflow.log_param(\"subsample\", 0.8)\n",
        "    mlflow.log_param(\"colsample_bytree\", 0.8)\n",
        "    mlflow.log_param(\"tree_method\", \"hist\")\n",
        "    mlflow.log_param(\"objective\", \"reg:squarederror\")\n",
        "\n",
        "    mlflow.log_metric(\"final_wmae\", wmae)\n",
        "\n",
        "    print(\"Logged successfully!\")\n",
        "\n",
        "with mlflow.start_run(run_name = \"XGBoost_0.1_6\"):\n",
        "    mlflow.log_param(\"model\", \"XGBoost\")\n",
        "    mlflow.log_param(\"n_estimators\", 200)\n",
        "    mlflow.log_param(\"learning_rate\", 0.1)\n",
        "    mlflow.log_param(\"max_depth\", 6)\n",
        "    mlflow.log_param(\"subsample\", 0.8)\n",
        "    mlflow.log_param(\"colsample_bytree\", 0.8)\n",
        "    mlflow.log_param(\"tree_method\", \"hist\")\n",
        "    mlflow.log_param(\"objective\", \"reg:squarederror\")\n",
        "\n",
        "    mlflow.log_metric(\"final_wmae\", wmae)\n",
        "\n",
        "    print(\"Logged successfully!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjOZWwKkkskC",
        "outputId": "820647d8-fbc5-4e0f-d117-6b4a97fed2cf"
      },
      "id": "HjOZWwKkkskC",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged successfully!\n",
            "🏃 View run XGBoost_best at: https://dagshub.com/goguaD/finalProjectML.mlflow/#/experiments/0/runs/e0733aa717274a08933e4ef632d62cab\n",
            "🧪 View experiment at: https://dagshub.com/goguaD/finalProjectML.mlflow/#/experiments/0\n",
            "Logged successfully!\n",
            "🏃 View run XGBoost_0.05_6 at: https://dagshub.com/goguaD/finalProjectML.mlflow/#/experiments/0/runs/dd117ba7fdf34a70a70359f65aace72d\n",
            "🧪 View experiment at: https://dagshub.com/goguaD/finalProjectML.mlflow/#/experiments/0\n",
            "Logged successfully!\n",
            "🏃 View run XGBoost_0.05_8 at: https://dagshub.com/goguaD/finalProjectML.mlflow/#/experiments/0/runs/54f2d19f40df4e4a9495e4af7fc0cdbb\n",
            "🧪 View experiment at: https://dagshub.com/goguaD/finalProjectML.mlflow/#/experiments/0\n",
            "Logged successfully!\n",
            "🏃 View run XGBoost_0.1_6 at: https://dagshub.com/goguaD/finalProjectML.mlflow/#/experiments/0/runs/d18468e4f5eb42a1a0e381ca6c14f375\n",
            "🧪 View experiment at: https://dagshub.com/goguaD/finalProjectML.mlflow/#/experiments/0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e4Rwy0mulT_T"
      },
      "id": "e4Rwy0mulT_T",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00b4de364dfa46b69c281636f3dc3df1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af0069c059544f538cd010d00facb306"
            ],
            "layout": "IPY_MODEL_93573d2ea25f4f92a4ff081683305f28"
          }
        },
        "da27840483f94509808db615bedf01be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e80cfc72cf414a8c82aebae4a0183bef",
            "placeholder": "​",
            "style": "IPY_MODEL_c27f16f8db764c44bc8f0c52ec1da3e9",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "1ce4de57a80e4d5b809dce94110c9c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_57f2f219d0814c72956995414e4363bb",
            "placeholder": "​",
            "style": "IPY_MODEL_a8168c03578a4d7f8d412742af111253",
            "value": "lizikutateladze"
          }
        },
        "b14a3b47865046cdadb51117e93a878f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_67541f2010e7495c9873db2a3e74a475",
            "placeholder": "​",
            "style": "IPY_MODEL_b6d32b9c0c9d45918eac3d2bd2380b3c",
            "value": ""
          }
        },
        "3fb3e72990d9458c9bf7486368ef4673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_4a0ee787cc0c407cbf671eaa9d53d303",
            "style": "IPY_MODEL_59831f17489f4807a0156d2ffcc9cd77",
            "tooltip": ""
          }
        },
        "2b894296f7e649de882728081268e823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ba84137a751482da2478318c7ad8483",
            "placeholder": "​",
            "style": "IPY_MODEL_f197d0ecd27743068a562bd702643ebf",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "93573d2ea25f4f92a4ff081683305f28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "e80cfc72cf414a8c82aebae4a0183bef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c27f16f8db764c44bc8f0c52ec1da3e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57f2f219d0814c72956995414e4363bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8168c03578a4d7f8d412742af111253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67541f2010e7495c9873db2a3e74a475": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6d32b9c0c9d45918eac3d2bd2380b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a0ee787cc0c407cbf671eaa9d53d303": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59831f17489f4807a0156d2ffcc9cd77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "0ba84137a751482da2478318c7ad8483": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f197d0ecd27743068a562bd702643ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48a129a90f744f88a614fe685c58943c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a0ba0d122b342aea64cb4f66c595a29",
            "placeholder": "​",
            "style": "IPY_MODEL_02df3688ada840f7bfe54618fcb60bb4",
            "value": "Connecting..."
          }
        },
        "9a0ba0d122b342aea64cb4f66c595a29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02df3688ada840f7bfe54618fcb60bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af0069c059544f538cd010d00facb306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe375d546874456e8ae18de38ba027e4",
            "placeholder": "​",
            "style": "IPY_MODEL_5276b7fba64f42e8ba9336608a56ccaa",
            "value": "Kaggle credentials successfully validated."
          }
        },
        "fe375d546874456e8ae18de38ba027e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5276b7fba64f42e8ba9336608a56ccaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}