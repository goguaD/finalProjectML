# finalProjectML
áƒ”áƒ¡ áƒžáƒ áƒáƒ”áƒ¥áƒ¢áƒ˜ áƒ›áƒ˜áƒ–áƒœáƒáƒ“ áƒ˜áƒ¡áƒáƒ®áƒáƒ•áƒ¡ áƒ¬áƒ˜áƒœáƒáƒ¡áƒ¬áƒáƒ áƒ›áƒ”áƒ¢áƒ§áƒ•áƒ”áƒšáƒ”áƒ‘áƒáƒ¡ áƒ›áƒáƒ¦áƒáƒ–áƒ˜áƒ”áƒ‘áƒ˜áƒ¡ áƒ§áƒáƒ•áƒ”áƒšáƒ™áƒ•áƒ˜áƒ áƒ”áƒ£áƒšáƒ˜ áƒ’áƒáƒ§áƒ˜áƒ“áƒ•áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒáƒªáƒ£áƒšáƒáƒ‘áƒáƒ–áƒ”, â€žWalmart Recruiting - Store Sales Forecastingâ€œ áƒ™áƒáƒ’áƒšáƒ˜áƒ¡ áƒ™áƒáƒœáƒ™áƒ£áƒ áƒ¡áƒ˜áƒ¡ áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ”áƒ‘áƒ–áƒ” áƒ“áƒáƒ§áƒ áƒ“áƒœáƒáƒ‘áƒ˜áƒ—.
áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ£áƒšáƒ˜ áƒáƒšáƒ’áƒáƒ áƒ˜áƒ—áƒ›áƒ”áƒ‘áƒ˜
DLinear, RandomForestRegressor, LightGBM, XGBOOST, N-BEATS, ARIMA, SARIMA

# 1. RANDOMFORESTREGRESSOR

 # áƒ¡áƒáƒ›áƒ£áƒ¨áƒáƒ áƒœáƒáƒ‘áƒ˜áƒ¯áƒ”áƒ‘áƒ˜
áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ”áƒ‘áƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ áƒ“áƒ áƒ’áƒáƒ¬áƒ›áƒ”áƒœáƒ“áƒ

áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ£áƒšáƒ˜áƒ train.csv, test.csv, features.csv, stores.csv

áƒ’áƒáƒ”áƒ áƒ—áƒ˜áƒáƒœáƒ“áƒ áƒ§áƒ•áƒ”áƒšáƒ áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ˜ Store áƒ“áƒ Date áƒ¡áƒ•áƒ”áƒ¢áƒ”áƒ‘áƒ–áƒ”

áƒ—áƒáƒ áƒ˜áƒ¦áƒ˜áƒ“áƒáƒœ áƒ’áƒáƒ›áƒáƒ—áƒ•áƒšáƒ˜áƒšáƒ˜áƒ áƒ“áƒáƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ—áƒ˜ áƒ¡áƒ•áƒ”áƒ¢áƒ”áƒ‘áƒ˜: Year, Month, Week

áƒªáƒáƒ áƒ˜áƒ”áƒšáƒ˜ áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒ”áƒ‘áƒ˜ (Markdown áƒªáƒ•áƒšáƒáƒ“áƒ”áƒ‘áƒ¨áƒ˜) áƒ©áƒáƒœáƒáƒªáƒ•áƒšáƒ“áƒ áƒœáƒ£áƒšáƒ”áƒ‘áƒ˜áƒ—

áƒ™áƒáƒ¢áƒ”áƒ’áƒáƒ áƒ˜áƒ£áƒšáƒ˜ áƒªáƒ•áƒšáƒáƒ“áƒ˜ Type áƒ’áƒáƒ“áƒáƒ™áƒ”áƒ—áƒ“áƒ áƒ áƒ˜áƒªáƒ®áƒ•áƒáƒ‘áƒ áƒ˜áƒ•áƒáƒ“ (LabelEncoder-áƒ˜áƒ—)

áƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’ áƒ•áƒáƒšáƒ˜áƒ“áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ’áƒáƒ§áƒáƒ¤áƒ (Train-Test Split)

áƒ›áƒáƒ“áƒ”áƒšáƒ˜ áƒ’áƒáƒ¬áƒ•áƒ áƒ—áƒœáƒ˜áƒšáƒ˜áƒ áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ”áƒ‘áƒ˜áƒ¡ 80%-áƒ–áƒ”

áƒ¨áƒ”áƒ¤áƒáƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ áƒ©áƒ”áƒœáƒ˜áƒš 20%-áƒ–áƒ”

áƒ¨áƒ”áƒ¤áƒáƒ¡áƒ”áƒ‘áƒ â€” WMAE (Weighted Mean Absolute Error)

áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ˜áƒ¡ áƒ“áƒáƒ—áƒ•áƒšáƒ áƒ˜áƒ—áƒ•áƒáƒšáƒ˜áƒ¡áƒ¬áƒ˜áƒœáƒ”áƒ‘áƒ¡ IsHoliday áƒ¡áƒ•áƒ”áƒ¢áƒ¡: áƒáƒ áƒ“áƒáƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜áƒ¡ áƒ™áƒ•áƒ˜áƒ áƒ”áƒ‘áƒ˜ 5-áƒ¯áƒ”áƒ  áƒ›áƒ”áƒ¢ áƒ¬áƒáƒœáƒáƒ¡ áƒ˜áƒ«áƒšáƒ”áƒ•áƒ

áƒ›áƒ˜áƒ¦áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ¨áƒ”áƒ¤áƒáƒ¡áƒ”áƒ‘áƒ áƒ’áƒ•áƒ˜áƒ©áƒ•áƒ”áƒœáƒ”áƒ‘áƒ¡, áƒ áƒáƒ›áƒ“áƒ”áƒœáƒáƒ“ áƒ™áƒáƒ áƒ’áƒáƒ“ áƒžáƒ áƒáƒ’áƒœáƒáƒ–áƒ˜áƒ áƒ”áƒ‘áƒ¡ áƒ›áƒáƒ“áƒ”áƒšáƒ˜ áƒ áƒ”áƒáƒšáƒ£áƒ  áƒ’áƒáƒ§áƒ˜áƒ“áƒ•áƒ”áƒ‘áƒ¡

# áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ’áƒáƒ“áƒáƒ›áƒ–áƒáƒ“áƒ”áƒ‘áƒ áƒ“áƒ áƒ¤áƒ˜áƒœáƒáƒšáƒ£áƒ áƒ˜ áƒžáƒ áƒáƒ’áƒœáƒáƒ–áƒ˜áƒ áƒ”áƒ‘áƒ

áƒ›áƒáƒ“áƒ”áƒšáƒ˜ áƒ’áƒáƒ“áƒáƒ›áƒ–áƒáƒ“áƒ“áƒ áƒ¡áƒ áƒ£áƒš train áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ–áƒ”

áƒžáƒ áƒáƒ’áƒœáƒáƒ–áƒ”áƒ‘áƒ˜ áƒ’áƒáƒ›áƒáƒ—áƒ•áƒšáƒ˜áƒšáƒ˜áƒ test.csv áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ–áƒ”

# áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜
áƒ•áƒáƒšáƒ˜áƒ“áƒáƒªáƒ˜áƒáƒ–áƒ” áƒ›áƒ˜áƒ¦áƒ”áƒ‘áƒ£áƒšáƒ˜ WMAE áƒ’áƒ•áƒ˜áƒ©áƒ•áƒ”áƒœáƒ”áƒ‘áƒ¡ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¨áƒ”áƒ¡áƒáƒ«áƒšáƒ”áƒ‘áƒšáƒáƒ‘áƒ”áƒ‘áƒ¡ áƒ’áƒáƒ§áƒ˜áƒ“áƒ•áƒ”áƒ‘áƒ˜áƒ¡ áƒžáƒ áƒáƒ’áƒœáƒáƒ–áƒ˜áƒ áƒ”áƒ‘áƒáƒ¨áƒ˜.

Random Forest áƒ’áƒ—áƒáƒ•áƒáƒ–áƒáƒ‘áƒ— áƒ¡áƒ¢áƒáƒ‘áƒ˜áƒšáƒ£áƒ  áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ¡ áƒ áƒáƒ›áƒ“áƒ”áƒœáƒ˜áƒ›áƒ” áƒ¤áƒ£áƒœáƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ˜áƒ—, áƒ§áƒáƒ•áƒ”áƒšáƒ’áƒ•áƒáƒ áƒ˜ áƒ áƒ—áƒ£áƒšáƒ˜ áƒ¢áƒ˜áƒ£áƒœáƒ˜áƒœáƒ’áƒ˜áƒ¡ áƒ’áƒáƒ áƒ”áƒ¨áƒ”.

# 2. ARIMA
### Feature Engineering
- **áƒ“áƒ áƒáƒ˜áƒ¡ áƒ›áƒáƒ®áƒáƒ¡áƒ˜áƒáƒ—áƒ”áƒ‘áƒšáƒ”áƒ‘áƒ˜:** Month, Week, DayOfYear
- **Store-Department áƒ¡áƒ¢áƒáƒ¢áƒ˜áƒ¡áƒ¢áƒ˜áƒ™áƒ:** Sales_Mean, Sales_Std, Sales_Median, Sales_Min, Sales_Max
- **áƒ’áƒáƒ áƒ” áƒ¤áƒáƒ¥áƒ¢áƒáƒ áƒ”áƒ‘áƒ˜:** Temperature, Fuel_Price, CPI, Unemployment
- **áƒáƒ áƒ“áƒáƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒ£áƒšáƒ¢áƒ˜áƒžáƒšáƒ˜áƒ™áƒáƒ¢áƒáƒ áƒ”áƒ‘áƒ˜:** Store-Department áƒ¡áƒžáƒ”áƒªáƒ˜áƒ¤áƒ˜áƒ£áƒšáƒ˜ áƒ“áƒ áƒ’áƒšáƒáƒ‘áƒáƒšáƒ£áƒ áƒ˜

### Data Splitting
- **Training:** 2010-02-05 to 2012-07-27
- **Validation:** 2012-08-03 to 2012-10-26 (13 áƒ™áƒ•áƒ˜áƒ áƒ)
- **Grouping:** Store-Department áƒ™áƒáƒ›áƒ‘áƒ˜áƒœáƒáƒªáƒ˜áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒ˜áƒ®áƒ”áƒ“áƒ•áƒ˜áƒ—

##  ARIMA áƒ›áƒáƒ“áƒ”áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ˜áƒ›áƒžáƒšáƒ”áƒ›áƒ”áƒœáƒ¢áƒáƒªáƒ˜áƒ

### áƒ áƒáƒ¢áƒáƒ› ARIMA?
ARIMA áƒáƒ˜áƒ áƒ©áƒ áƒ áƒáƒ“áƒ’áƒáƒœ:
- **Time Series áƒ‘áƒ£áƒœáƒ”áƒ‘áƒ:** áƒ’áƒáƒ§áƒ˜áƒ“áƒ•áƒ”áƒ‘áƒ˜ áƒ“áƒ áƒáƒ¨áƒ˜ áƒ“áƒáƒ›áƒáƒ™áƒ˜áƒ“áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ
- **Trend áƒ“áƒ Seasonality:** ARIMA áƒ£áƒ›áƒ™áƒšáƒáƒ•áƒ“áƒ”áƒ‘áƒ áƒ¢áƒ”áƒœáƒ“áƒ”áƒœáƒªáƒ˜áƒáƒ¡ áƒ“áƒ áƒ¡áƒ”áƒ–áƒáƒœáƒ£áƒ áƒáƒ‘áƒáƒ¡
- **áƒžáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ˜áƒœáƒ¢áƒ”áƒ áƒžáƒ áƒ”áƒ¢áƒáƒªáƒ˜áƒ:** p,d,q áƒžáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜ áƒ›áƒáƒ áƒ¢áƒ˜áƒ•áƒáƒ“ áƒ’áƒáƒ¡áƒáƒ’áƒ”áƒ‘áƒ˜áƒ

### 1) Simple ARIMA (WMAE: 2213.76)
```python
# áƒ‘áƒáƒ–áƒ£áƒ áƒ˜ ARIMA(1,1,1) áƒ§áƒ•áƒ”áƒšáƒ Store-Dept-áƒ–áƒ”
model = ARIMA(ts, order=(1, 1, 1))
# áƒ’áƒšáƒáƒ‘áƒáƒšáƒ£áƒ áƒ˜ áƒáƒ áƒ“áƒáƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒ£áƒšáƒ¢áƒ˜áƒžáƒšáƒ˜áƒ™áƒáƒ¢áƒáƒ áƒ˜: 1.074
```

**áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒ˜áƒ¡ áƒžáƒ áƒ˜áƒœáƒªáƒ˜áƒžáƒ˜:**
- áƒ›áƒ˜áƒœáƒ˜áƒ›áƒ£áƒ› 10 áƒ“áƒáƒ™áƒ•áƒ˜áƒ áƒ•áƒ”áƒ‘áƒ Store-Dept-áƒ–áƒ”
- Weekly aggregation
- Fallback: Store-Dept áƒ¡áƒáƒ¨áƒ£áƒáƒšáƒ

### 2) Optimized ARIMA (WMAE: 2550.43)
```python
# áƒ›áƒ áƒáƒ•áƒáƒšáƒ˜ ARIMA áƒ™áƒáƒœáƒ¤áƒ˜áƒ’áƒ£áƒ áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ¢áƒ”áƒ¡áƒ¢áƒ˜áƒ áƒ”áƒ‘áƒ
configs = [(1,1,1), (2,1,1), (1,1,2), (0,1,1), (1,0,1), (2,1,2)]
# AIC-áƒ˜áƒ— áƒ¡áƒáƒ£áƒ™áƒ”áƒ—áƒ”áƒ¡áƒ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒáƒ áƒ©áƒ”áƒ•áƒ
```

**áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ”áƒ‘áƒ˜:**
- Store-Dept áƒ¡áƒžáƒ”áƒªáƒ˜áƒ¤áƒ˜áƒ£áƒšáƒ˜ áƒáƒ áƒ“áƒáƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒ£áƒšáƒ¢áƒ˜áƒžáƒšáƒ˜áƒ™áƒáƒ¢áƒáƒ áƒ”áƒ‘áƒ˜
- áƒ¢áƒ”áƒ›áƒžáƒ”áƒ áƒáƒ¢áƒ£áƒ áƒ˜áƒ¡ áƒ™áƒáƒ áƒ”áƒ¥áƒ¢áƒ˜áƒ áƒ”áƒ‘áƒ (>80Â°C: 0.95x, <30Â°C: 1.05x)
- áƒ›áƒ˜áƒœáƒ˜áƒ›áƒ£áƒ› 15 áƒ“áƒáƒ™áƒ•áƒ˜áƒ áƒ•áƒ”áƒ‘áƒ

### 3) Fast Optimized ARIMA (WMAE: 2152.64)
```python
# áƒ¨áƒ”áƒ›áƒªáƒ˜áƒ áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ™áƒáƒœáƒ¤áƒ˜áƒ’áƒ£áƒ áƒáƒªáƒ˜áƒ”áƒ‘áƒ˜ áƒ¡áƒ¬áƒ áƒáƒ¤áƒ˜ áƒáƒžáƒ¢áƒ˜áƒ›áƒ˜áƒ–áƒáƒªáƒ˜áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡
configs = [(1,1,1), (2,1,1), (1,1,2)]
# áƒ¢áƒ”áƒ›áƒžáƒ”áƒ áƒáƒ¢áƒ£áƒ áƒ˜áƒ¡ áƒœáƒáƒ™áƒšáƒ”áƒ‘áƒ˜ áƒ’áƒáƒ•áƒšáƒ”áƒœáƒ (0.98x, 1.02x)
```

**áƒáƒžáƒ¢áƒ˜áƒ›áƒ˜áƒ–áƒáƒªáƒ˜áƒ”áƒ‘áƒ˜:**
- áƒ›áƒ˜áƒœáƒ˜áƒ›áƒ£áƒ› 12 áƒ“áƒáƒ™áƒ•áƒ˜áƒ áƒ•áƒ”áƒ‘áƒ
- 80% recent + 20% overall average fallback
- áƒ¡áƒ¬áƒ áƒáƒ¤áƒ˜ áƒžáƒ áƒáƒªáƒ”áƒ¡áƒ˜áƒœáƒ’áƒ˜

##  WMAE áƒ›áƒ”áƒ¢áƒ áƒ˜áƒ™áƒ

```python
def compute_wmae(y_true, y_pred, is_holiday):
    weights = np.where(is_holiday, 5, 1)  # áƒáƒ áƒ“áƒáƒ“áƒ”áƒ’áƒ”áƒ‘áƒ–áƒ” 5x áƒ¬áƒáƒœáƒ
    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)
```

# 3. N-BEATS 

## ðŸ”§ áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ

### Feature Engineering
- **áƒ“áƒ áƒáƒ˜áƒ¡ áƒ›áƒáƒ®áƒáƒ¡áƒ˜áƒáƒ—áƒ”áƒ‘áƒšáƒ”áƒ‘áƒ˜:** Year, Month, Week, Day, DayOfWeek, DayOfYear, IsWeekend
- **Store áƒ›áƒáƒ®áƒáƒ¡áƒ˜áƒáƒ—áƒ”áƒ‘áƒšáƒ”áƒ‘áƒ˜:** Type (encoded), Size
- **áƒ’áƒáƒ áƒ” áƒ¤áƒáƒ¥áƒ¢áƒáƒ áƒ”áƒ‘áƒ˜:** Temperature, Fuel_Price, MarkDown1-5, CPI, Unemployment
- **áƒáƒ áƒ“áƒáƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒ£áƒšáƒ¢áƒ˜áƒžáƒšáƒ˜áƒ™áƒáƒ¢áƒáƒ áƒ”áƒ‘áƒ˜:** IsHoliday (5x áƒ¬áƒáƒœáƒ WMAE-áƒ¨áƒ˜)

### Data Splitting
- **Training:** 421,570 áƒ©áƒáƒœáƒáƒ¬áƒ”áƒ áƒ˜
- **Sequences:** 261,083 áƒ¡áƒ”áƒ¥áƒ•áƒ”áƒœáƒ¡áƒ˜
- **Sequence Length:** 52 áƒ™áƒ•áƒ˜áƒ áƒ (1 áƒ¬áƒ”áƒšáƒ˜)
- **Forecast Length:** 1 áƒ™áƒ•áƒ˜áƒ áƒ

## N-BEATS áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒáƒ áƒ¥áƒ˜áƒ¢áƒ”áƒ¥áƒ¢áƒ£áƒ áƒ

### áƒ áƒáƒ¢áƒáƒ› N-BEATS?
N-BEATS áƒáƒ˜áƒ áƒ©áƒ áƒ áƒáƒ“áƒ’áƒáƒœ:
- **Interpretable:** áƒ‘áƒáƒ–áƒ˜áƒ¡áƒ£áƒ áƒ˜ áƒ¤áƒ£áƒœáƒ¥áƒªáƒ˜áƒ”áƒ‘áƒ˜áƒ¡ áƒ˜áƒœáƒ¢áƒ”áƒ áƒžáƒ áƒ”áƒ¢áƒáƒªáƒ˜áƒ
- **No Seasonality:** áƒáƒ  áƒ¡áƒáƒ­áƒ˜áƒ áƒáƒ”áƒ‘áƒ¡ áƒ¡áƒ”áƒ–áƒáƒœáƒ£áƒ áƒáƒ‘áƒ˜áƒ¡ áƒžáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ¡
- **Deep Learning:** 
- **Residual Learning:** áƒ‘áƒšáƒáƒ™áƒ”áƒ‘áƒ˜ áƒ§áƒáƒ•áƒ”áƒšáƒ—áƒ•áƒ˜áƒ¡ residuals-áƒ–áƒ” áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒ¡

### áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¡áƒ¢áƒ áƒ£áƒ¥áƒ¢áƒ£áƒ áƒ
```python
class NBeatsModel(nn.Module):
    def __init__(self, input_size=52, forecast_size=1, n_blocks=3, layers=4, layer_size=256):
        # 3 áƒ‘áƒšáƒáƒ™áƒ˜, áƒ—áƒ˜áƒ—áƒáƒ”áƒ£áƒšáƒ˜ 4 áƒ¤áƒ”áƒœáƒ˜áƒ—
        # Generic Basis Function
        # Residual connections
```

**áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒ˜áƒ¡ áƒžáƒ áƒ˜áƒœáƒªáƒ˜áƒžáƒ˜:**
- **Input:** 52 áƒ™áƒ•áƒ˜áƒ áƒ˜áƒ¡ áƒ’áƒáƒ§áƒ˜áƒ“áƒ•áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ”áƒ‘áƒ˜
- **Blocks:** 3 áƒ‘áƒšáƒáƒ™áƒ˜ áƒ—áƒ˜áƒ—áƒáƒ”áƒ£áƒšáƒ˜ áƒ—áƒáƒ•áƒ˜áƒ¡áƒ˜ áƒ‘áƒáƒ–áƒ˜áƒ¡áƒ£áƒ áƒ˜ áƒ¤áƒ£áƒœáƒ¥áƒªáƒ˜áƒ˜áƒ—
- **Residuals:** áƒ§áƒáƒ•áƒ”áƒšáƒ˜ áƒ‘áƒšáƒáƒ™áƒ˜ residuals-áƒ–áƒ” áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒ¡
- **Output:** 1 áƒ™áƒ•áƒ˜áƒ áƒ˜áƒ¡ áƒžáƒ áƒáƒ’áƒœáƒáƒ–áƒ˜

## áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ˜áƒ›áƒžáƒšáƒ”áƒ›áƒ”áƒœáƒ¢áƒáƒªáƒ˜áƒ

### 1. NBeatsBlock
```python
class NBeatsBlock(nn.Module):
    def __init__(self, input_size, theta_size, basis_function, layers, layer_size):
        # 4 áƒ¤áƒ”áƒœáƒ ReLU activation-áƒ˜áƒ—
        # Theta layer áƒ‘áƒáƒ–áƒ˜áƒ¡áƒ£áƒ áƒ˜ áƒ¤áƒ£áƒœáƒ¥áƒªáƒ˜áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡
```

### 2. GenericBasis
```python
class GenericBasis(nn.Module):
    def forward(self, theta):
        # Backcast: input_size áƒ–áƒáƒ›áƒ˜áƒ¡
        # Forecast: forecast_size áƒ–áƒáƒ›áƒ˜áƒ¡
        return backcast, forecast
```

### 3. WalmartDataset
```python
class WalmartDataset(Dataset):
    def __init__(self, data, sequence_length=52, forecast_length=1):
        # Store-Dept grouping
        # Sliding window sequences
        # Holiday weights (5x for holidays)
```

##  áƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒ˜áƒ¡ áƒžáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜

```python
SEQUENCE_LENGTH = 52      # 1 áƒ¬áƒ”áƒšáƒ˜
FORECAST_LENGTH = 1       # 1 áƒ™áƒ•áƒ˜áƒ áƒ
BATCH_SIZE = 32
LEARNING_RATE = 0.001
EPOCHS = 50
```

### áƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒ˜áƒ¡ áƒžáƒ áƒáƒªáƒ”áƒ¡áƒ˜
- **Loss Function:** MSE
- **Optimizer:** Adam
- **Device:** CUDA (GPU)
- **WMAE Tracking:** áƒ§áƒáƒ•áƒ”áƒš epoch-áƒ–áƒ”

## áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜

### áƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒ˜áƒ¡ áƒžáƒ áƒáƒ’áƒ áƒ”áƒ¡áƒ˜
- **Epoch 0:** WMAE: 1914.79
- **Epoch 10:** WMAE: 1444.74
- **Epoch 20:** WMAE: 1353.40
- **Epoch 30:** WMAE: 1299.69
- **Epoch 40:** WMAE: 1254.77
- **Final:** WMAE: 1228.61

### áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ£áƒžáƒ˜áƒ áƒáƒ¢áƒ”áƒ¡áƒáƒ‘áƒ”áƒ‘áƒ˜
- **áƒ¡áƒ¬áƒ áƒáƒ¤áƒ˜ áƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒ˜:** GPU-áƒ–áƒ” áƒáƒžáƒ¢áƒ˜áƒ›áƒ˜áƒ–áƒ”áƒ‘áƒ£áƒšáƒ˜
- **áƒ™áƒáƒ áƒ’áƒ˜ WMAE:** 1228.61 
- **Interpretable:** áƒ‘áƒáƒ–áƒ˜áƒ¡áƒ£áƒ áƒ˜ áƒ¤áƒ£áƒœáƒ¥áƒªáƒ˜áƒ”áƒ‘áƒ˜áƒ¡ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜
- **Scalable:** áƒ›áƒ áƒáƒ•áƒáƒšáƒ˜ Store-Dept áƒ™áƒáƒ›áƒ‘áƒ˜áƒœáƒáƒªáƒ˜áƒ


##  MLflow Tracking

```python
# áƒ”áƒ¥áƒ¡áƒžáƒ”áƒ áƒ˜áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒšáƒáƒ’áƒ˜áƒ áƒ”áƒ‘áƒ
mlflow.log_param("model", "N-BEATS")
mlflow.log_param("sequence_length", 52)
mlflow.log_metric("final_wmae", 1228.61)
```
# 3. DLinear

##  áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ

### Feature Engineering
- **áƒ’áƒáƒ áƒ” áƒ¤áƒáƒ¥áƒ¢áƒáƒ áƒ”áƒ‘áƒ˜:** Temperature, Fuel_Price, CPI, Unemployment
- **áƒ“áƒ áƒáƒ˜áƒ¡ áƒ›áƒáƒ®áƒáƒ¡áƒ˜áƒáƒ—áƒ”áƒ‘áƒšáƒ”áƒ‘áƒ˜:** Week, Year
- **áƒáƒ áƒ“áƒáƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒ£áƒšáƒ¢áƒ˜áƒžáƒšáƒ˜áƒ™áƒáƒ¢áƒáƒ áƒ”áƒ‘áƒ˜:** IsHoliday (5x áƒ¬áƒáƒœáƒ WMAE-áƒ¨áƒ˜)
- **MarkDown áƒ¤áƒáƒ¥áƒ¢áƒáƒ áƒ”áƒ‘áƒ˜:** MarkDown1-5 (áƒ¤áƒáƒ¡áƒ“áƒáƒ™áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜)

### Data Preprocessing
```python
# Log transformation for target variable
y_raw = np.log1p(store_df[target_col].values)

# StandardScaler for features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_raw)

# Forward fill for missing values
df = df.fillna(method='ffill')
```

### Data Splitting
- **Input Length:** 12 áƒ™áƒ•áƒ˜áƒ áƒ
- **Output Length:** 1 áƒ™áƒ•áƒ˜áƒ áƒ
- **Train/Val Split:** 80/20 (shuffle=False)
- **Store Focus:** Store 1 (áƒžáƒ˜áƒšáƒáƒ¢áƒ˜)

##  DLinear áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒáƒ áƒ¥áƒ˜áƒ¢áƒ”áƒ¥áƒ¢áƒ£áƒ áƒ

### áƒ áƒáƒ¢áƒáƒ› LSTM?
LSTM áƒáƒ˜áƒ áƒ©áƒ áƒ áƒáƒ“áƒ’áƒáƒœ:
- **Sequential Data:** áƒ’áƒáƒ§áƒ˜áƒ“áƒ•áƒ”áƒ‘áƒ˜ áƒ“áƒ áƒáƒ¨áƒ˜ áƒ“áƒáƒ›áƒáƒ™áƒ˜áƒ“áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ
- **Long-term Dependencies:** áƒ¨áƒáƒ áƒ”áƒ£áƒšáƒ˜ áƒ™áƒáƒ•áƒ¨áƒ˜áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ›áƒáƒ®áƒ¡áƒáƒ•áƒ áƒ”áƒ‘áƒ
- **Vanishing Gradient:** LSTM áƒ£áƒ›áƒ™áƒšáƒáƒ•áƒ“áƒ”áƒ‘áƒ gradient vanishing áƒžáƒ áƒáƒ‘áƒšáƒ”áƒ›áƒáƒ¡
- **Memory Cells:** áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ•áƒáƒœáƒ˜ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ¨áƒ”áƒœáƒáƒ®áƒ•áƒ

### áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¡áƒ¢áƒ áƒ£áƒ¥áƒ¢áƒ£áƒ áƒ
```python
class LSTMForecaster(nn.Module):
    def __init__(self, n_features, hidden_dim=64, n_layers=2, output_len=1, dropout=0.2):
        self.lstm = nn.LSTM(
            input_size=n_features,
            hidden_size=hidden_dim,
            num_layers=n_layers,
            batch_first=True,
            dropout=dropout
        )
        self.fc = nn.Linear(hidden_dim, output_len)
```

**áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒ˜áƒ¡ áƒžáƒ áƒ˜áƒœáƒªáƒ˜áƒžáƒ˜:**
- **Input:** 12 áƒ™áƒ•áƒ˜áƒ áƒ˜áƒ¡ features (12, n_features)
- **LSTM Layers:** 2 áƒ¤áƒ”áƒœáƒ, 64 hidden units
- **Dropout:** 0.2 overfitting-áƒ˜áƒ¡ áƒ—áƒáƒ•áƒ˜áƒ“áƒáƒœ áƒáƒ¡áƒáƒªáƒ˜áƒšáƒ”áƒ‘áƒšáƒáƒ“
- **Output:** 1 áƒ™áƒ•áƒ˜áƒ áƒ˜áƒ¡ áƒžáƒ áƒáƒ’áƒœáƒáƒ–áƒ˜

## ðŸ“ˆ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ˜áƒ›áƒžáƒšáƒ”áƒ›áƒ”áƒœáƒ¢áƒáƒªáƒ˜áƒ

### Sequence Creation
```python
def make_sequences(X, y, input_len=12, output_len=1):
    # Sliding window approach
    # Input: 12 weeks of features
    # Output: 1 week prediction
```

### áƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒ˜áƒ¡ áƒžáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜
```python
INPUT_LEN = 12          # 12 áƒ™áƒ•áƒ˜áƒ áƒ input
OUTPUT_LEN = 1          # 1 áƒ™áƒ•áƒ˜áƒ áƒ output
BATCH_SIZE = 32
LEARNING_RATE = 5e-4
EPOCHS = 30
```

### áƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒ˜áƒ¡ áƒžáƒ áƒáƒªáƒ”áƒ¡áƒ˜
- **Loss Function:** L1Loss (MAE)
- **Optimizer:** Adam
- **Device:** CUDA (GPU)
- **Validation:** áƒ§áƒáƒ•áƒ”áƒš epoch-áƒ–áƒ”

## ðŸ“Š áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜

### áƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒ˜áƒ¡ áƒžáƒ áƒáƒ’áƒ áƒ”áƒ¡áƒ˜
- **Epoch 1:** Train L1: 3.0910, Val L1: 1.3851
- **Epoch 10:** Train L1: 1.2996, Val L1: 1.3723
- **Epoch 20:** Train L1: 1.2988, Val L1: 1.3683
- **Epoch 30:** Train L1: 1.3002, Val L1: 1.3684

### WMAE áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜
```python
# Store 1-áƒ˜áƒ¡ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜
WMAE (Store 1): 18,232.87

# MLflow tracking-áƒ¨áƒ˜
WMAE: 37,452,165.20 (áƒ¨áƒ”áƒ¡áƒáƒ«áƒšáƒáƒ overfitting)
```

### áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ£áƒžáƒ˜áƒ áƒáƒ¢áƒ”áƒ¡áƒáƒ‘áƒ”áƒ‘áƒ˜
- **Sequential Learning:** áƒ“áƒ áƒáƒ¨áƒ˜ áƒ“áƒáƒ›áƒáƒ™áƒ˜áƒ“áƒ”áƒ‘áƒ£áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒ¥áƒ›áƒœáƒ
- **Feature Integration:** áƒ›áƒ áƒáƒ•áƒáƒšáƒ˜ áƒ¤áƒáƒ¥áƒ¢áƒáƒ áƒ˜áƒ¡ áƒ’áƒáƒ—áƒ•áƒáƒšáƒ˜áƒ¡áƒ¬áƒ˜áƒœáƒ”áƒ‘áƒ
- **Memory Efficient:** LSTM-áƒ˜áƒ¡ áƒ›áƒ”áƒ®áƒ¡áƒ˜áƒ”áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ£áƒœáƒáƒ áƒ˜
- **Scalable:** áƒ¡áƒ®áƒ•áƒ stores-áƒ–áƒ” áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ

## ðŸ” MLflow Tracking

```python
# áƒ”áƒ¥áƒ¡áƒžáƒ”áƒ áƒ˜áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒšáƒáƒ’áƒ˜áƒ áƒ”áƒ‘áƒ
mlflow.log_param("input_len", 12)
mlflow.log_param("output_len", 1)
mlflow.log_param("model_type", "DLinear")
mlflow.log_metric("WMAE", wmae)
```

##  áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜áƒ¡ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜

### áƒžáƒ áƒáƒ‘áƒšáƒ”áƒ›áƒ”áƒ‘áƒ˜
- **Overfitting:** áƒ›áƒáƒ¦áƒáƒšáƒ˜ WMAE MLflow-áƒ¨áƒ˜
- **Store Specific:** áƒ›áƒ®áƒáƒšáƒáƒ“ Store 1-áƒ–áƒ” áƒ¢áƒ”áƒ¡áƒ¢áƒ˜áƒ áƒ”áƒ‘áƒ
- **Feature Scaling:** log transformation áƒ¨áƒ”áƒ˜áƒ«áƒšáƒ”áƒ‘áƒ áƒáƒ  áƒ˜áƒ§áƒáƒ¡ áƒáƒžáƒ¢áƒ˜áƒ›áƒáƒšáƒ£áƒ áƒ˜

### áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ”áƒ‘áƒ˜
- **Multi-store Training:** áƒ§áƒ•áƒ”áƒšáƒ store-áƒ–áƒ” áƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒ˜
- **Hyperparameter Tuning:** áƒ£áƒ™áƒ”áƒ—áƒ”áƒ¡áƒ˜ áƒžáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒáƒ«áƒ˜áƒ”áƒ‘áƒ
- **Ensemble Methods:** áƒ¡áƒ®áƒ•áƒ áƒ›áƒáƒ“áƒ”áƒšáƒ”áƒ‘áƒ—áƒáƒœ áƒ™áƒáƒ›áƒ‘áƒ˜áƒœáƒáƒªáƒ˜áƒ
